1. [CosmicMan: A Text-to-Image Foundation Model for Humans](https://arxiv.org/abs/2404.01294)
2. [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291)
3. [LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization](https://arxiv.org/abs/2404.01282)
4. [Bridging Remote Sensors with Multisensor Geospatial Foundation Models](https://arxiv.org/abs/2404.01260)
5. [Open-Vocabulary Federated Learning with Multimodal Prototyping](https://arxiv.org/abs/2404.01232)
6. [BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2404.01179)
7. [Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation](https://arxiv.org/abs/2404.01127)
8. [Efficient Prompting Methods for Large Language Models: A Survey](https://arxiv.org/abs/2404.01077)
9. [T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation](https://arxiv.org/abs/2404.01065)
10. [Continual Learning for Smart City: A Survey](https://arxiv.org/abs/2404.00983)
11. [Harnessing The Power of Attention For Patch-Based Biomedical Image Classification](https://arxiv.org/abs/2404.00949)
12. [A Comprehensive Review of Knowledge Distillation in Computer Vision](https://arxiv.org/abs/2404.00936)
13. [Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2404.00918)
14. [Slightly Shift New Classes to Remember Old Classes for Video Class-Incremental Learning](https://arxiv.org/abs/2404.00901)
15. [Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance](https://arxiv.org/abs/2404.00860)
16. [Rehearsal-Free Modular and Compositional Continual Learning for Language Models](https://arxiv.org/abs/2404.00790)
17. [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2404.00781)
18. [LLM meets Vision-Language Models for Zero-Shot One-Class Classification](https://arxiv.org/abs/2404.00675)
19. [Deep Instruction Tuning for Segment Anything Model](https://arxiv.org/abs/2404.00650)
20. [SpiralMLP: A Lightweight Vision MLP Architecture](https://arxiv.org/abs/2404.00648)
21. [Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning](https://arxiv.org/abs/2404.00603)
22. [LLMs are Good Action Recognizers](https://arxiv.org/abs/2404.00532)
23. [Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation](https://arxiv.org/abs/2404.00417)
24. [TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias](https://arxiv.org/abs/2404.00384)
25. [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/abs/2404.00376)
26. [DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2404.00380)
27. [CLIP-driven Outliers Synthesis for few-shot OOD detection](https://arxiv.org/abs/2404.00323)
28. [Bayesian Exploration of Pre-trained Models for Low-shot Image Classification](https://arxiv.org/abs/2404.00312)
29. [DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation](https://arxiv.org/abs/2404.00264)
30. [Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2404.00262)
31. [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228)
32. [Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark](https://arxiv.org/abs/2404.00216)
33. [AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation](https://arxiv.org/abs/2404.00122)
34. [Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation](https://arxiv.org/abs/2404.01102)
35. [Are We on the Right Way for Evaluating Large Vision-Language Models?](https://arxiv.org/abs/2403.20330)
36. [MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning](https://arxiv.org/abs/2403.20320)
37. [Convolutional Prompting meets Language Models for Continual Learning](https://arxiv.org/abs/2403.20317)
38. [Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations](https://arxiv.org/abs/2403.20312)
39. [Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference](https://arxiv.org/abs/2403.20306)
40. [Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain](https://arxiv.org/abs/2403.20288)
41. [LayerNorm: A key component in parameter-efficient fine-tuning](https://arxiv.org/abs/2403.20284)
42. [Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://arxiv.org/abs/2403.20271)
43. [MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation](https://arxiv.org/abs/2403.20253)
44. [ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models](https://arxiv.org/abs/2403.20194)
45. [ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models](https://arxiv.org/abs/2403.20158)
46. [ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning](https://arxiv.org/abs/2403.20126)
47. [FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models](https://arxiv.org/abs/2403.20105)
48. [NLP for Counterspeech against Hate: A Survey and How-To Guide](https://arxiv.org/abs/2403.20103)
49. [Selective Attention-based Modulation for Continual Learning](https://arxiv.org/abs/2403.20086)
50. [Negative Label Guided OOD Detection with Pretrained Vision-Language Models](https://arxiv.org/abs/2403.20078)
51. [Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer](https://arxiv.org/abs/2403.19979)
52. [Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data](https://arxiv.org/abs/2403.19950)
53. [FairCLIP: Harnessing Fairness in Vision-Language Learning](https://arxiv.org/abs/2403.19949)
54. [An Information-Theoretic Framework for Out-of-Distribution Generalization](https://arxiv.org/abs/2403.19895)
55. [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)
56. [Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation](https://arxiv.org/abs/2403.19826)
57. [MAPL: Model Agnostic Peer-to-peer Learning](https://arxiv.org/abs/2403.19792)
58. [CLoRA: A Contrastive Approach to Compose Multiple LoRA Models](https://arxiv.org/abs/2403.19776)
59. [GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation](https://arxiv.org/abs/2403.19754)
60. [Analyzing the Roles of Language and Vision in Learning from Limited Data](https://arxiv.org/abs/2403.19669)
61. [UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation](https://arxiv.org/abs/2403.20035)
62. [Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks](https://arxiv.org/abs/2403.19880)
63. [Segment Any 3D Object with Language](https://arxiv.org/abs/2404.02157)
64. [Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration](https://arxiv.org/abs/2404.02154)
65. [Iterated Learning Improves Compositionality in Large Vision-Language Models](https://arxiv.org/abs/2404.02145)
66. [ViTamin: Designing Scalable Vision Models in the Vision-Language Era](https://arxiv.org/abs/2404.02132)
67. [Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners](https://arxiv.org/abs/2404.02117)
68. [Red-Teaming Segment Anything Model](https://arxiv.org/abs/2404.02067)
69. [Improving Bird's Eye View Semantic Segmentation by Task Decomposition](https://arxiv.org/abs/2404.01925)
70. [Class-Incremental Few-Shot Event Detection](https://arxiv.org/abs/2404.01767)
71. [Learning Equi-angular Representations for Online Continual Learning](https://arxiv.org/abs/2404.01628)
72. [Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction](https://arxiv.org/abs/2404.01448)