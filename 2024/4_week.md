1. [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](https://arxiv.org/abs/2401.10862)
2. [Removal and Selection: Improving RGB-Infrared Object Detection via Coarse-to-Fine Fusion](https://arxiv.org/abs/2401.10731)
3. [AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference](https://arxiv.org/abs/2401.10652)
4. [OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy](https://arxiv.org/abs/2401.10559)
5. [Unified View Imputation and Feature Selection Learning for Incomplete Multi-view Data](https://arxiv.org/abs/2401.10549)
6. [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences](https://arxiv.org/abs/2401.10529)
7. [On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss](https://arxiv.org/abs/2401.10526)
8. [Focaler-IoU: More Focused Intersection over Union Loss](https://arxiv.org/abs/2401.10525)
9. [LDReg: Local Dimensionality Regularized Self-Supervised Learning](https://arxiv.org/abs/2401.10474)
10. [Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition](https://arxiv.org/abs/2401.10447)
11. [Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats](https://arxiv.org/abs/2401.10375)
12. [EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model](https://arxiv.org/abs/2401.10278)