1. [Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge](https://arxiv.org/abs/2407.04681v1)
2. [SAM Fewshot Finetuning for Anatomical Segmentation in Medical Images](https://arxiv.org/abs/2407.04651)
3. [Exploration of Class Center for Fine-Grained Visual Classification](https://arxiv.org/abs/2407.04243)
4. [AMD: Automatic Multi-step Distillation of Large-scale Vision Models](https://arxiv.org/abs/2407.04208)
5. [Batch Transformer: Look for Attention in Batch](https://arxiv.org/abs/2407.04218)
6. [Multi-modal Masked Siamese Network Improves Chest X-Ray Representation Learning](https://arxiv.org/abs/2407.04449)
7. [CountGD: Multi-Modal Open-World Counting](https://arxiv.org/abs/2407.04619v1)
8. [Dude: Dual Distribution-Aware Context Prompt Learning For Large Vision-Language Model](https://arxiv.org/abs/2407.04489)
9. [Success or Failure? Analyzing Segmentation Refinement with Few-Shot Segmentation](https://arxiv.org/abs/2407.04519)
10. [JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation](https://arxiv.org/abs/2407.06187)
11. [Layered Diffusion Model for One-Shot High Resolution Text-to-Image Synthesis](https://arxiv.org/abs/2407.06079)
12. [Pseudo-triplet Guided Few-shot Composed Image Retrieval](https://arxiv.org/abs/2407.06001)
13. [Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports](https://arxiv.org/abs/2407.05758v1)
14. [The Solution for Language-Enhanced Image New Category Discovery](https://arxiv.org/abs/2407.04994v1)
15. [SAM-Med3D-MoE: Towards a Non-Forgetting Segment Anything Model via Mixture of Experts for 3D Medical Image Segmentation](https://arxiv.org/abs/2407.04938)
16. [Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models](https://arxiv.org/abs/2407.05342v1)
17. [Sequential Contrastive Audio-Visual Learning](https://arxiv.org/abs/2407.05782v1)
18. [Improving Knowledge Distillation in Transfer Learning with Layer-wise Learning Rates](https://arxiv.org/abs/2407.04871)
19. [Leveraging Task-Specific Knowledge from LLM for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2407.05088v1)
20. [Leveraging Topological Guidance for Improved Knowledge Distillation](https://arxiv.org/abs/2407.05316v1)
21. [Enhancing Label-efficient Medical Image Segmentation with Text-guided Diffusion Models](https://arxiv.org/abs/2407.05323v1)
22. [Exploring Phrase-Level Grounding with Text-to-Image Diffusion Model](https://arxiv.org/abs/2407.05352v1)
23. [A Study of Test-time Contrastive Concepts for Open-world, Open-vocabulary Semantic Segmentation](https://arxiv.org/abs/2407.05061)
24. [Deciphering the Role of Representation Disentanglement: Investigating Compositional Generalization in CLIP Models](https://arxiv.org/abs/2407.05897)
25. [SCSA: Exploring the Synergistic Effects Between Spatial and Channel Attention](https://arxiv.org/abs/2407.05128)
26. [Cross-domain Few-shot In-context Learning for Enhancing Traffic Sign Recognition](https://arxiv.org/abs/2407.05814v1)
27. [HyCIR: Boosting Zero-Shot Composed Image Retrieval with Synthetic Labels](https://arxiv.org/abs/2407.05795)
28. [Rethinking Unsupervised Outlier Detection via Multiple Thresholding](https://arxiv.org/abs/2407.05382v1)
29. [Learning to Adapt Category Consistent Meta-Feature of CLIP for Few-Shot Classification](https://arxiv.org/abs/2407.05647)
30. [HPFF: Hierarchical Locally Supervised Learning with Patch Feature Fusion](https://arxiv.org/abs/2407.05638v1)
31. [Momentum Auxiliary Network for Supervised Local Learning](https://arxiv.org/abs/2407.05623)
32. [An Experimental Comparison of Transfer Learning against Self-supervised Learning](https://arxiv.org/abs/2407.05592v1)
33. [FALIP: Visual Prompt as Foveal Attention Boosts CLIP Zero-Shot Performance](https://arxiv.org/abs/2407.05578v1)
34. [Ada-adapter:Fast Few-shot Style Personlization of Diffusion Model with Pre-trained Image Encoder](https://arxiv.org/abs/2407.05552v1)
35. [Cross Prompting Consistency with Segment Anything Model for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2407.05416v1)
36. [FM-OSD: Foundation Model-Enabled One-Shot Detection of Anatomical Landmarks](https://arxiv.org/abs/2407.05412v1)
37. [Image-Conditional Diffusion Transformer for Underwater Image Enhancement](https://arxiv.org/abs/2407.05389)
38. [Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2407.06136)
39. [OpenCIL: Benchmarking Out-of-Distribution Detection in Class-Incremental Learning](https://arxiv.org/abs/2407.06045)
40. [Contrastive Learning of Preferences with a Contextual InfoNCE Loss](https://arxiv.org/abs/2407.05898)
41. [Minutes to Seconds: Speeded-up DDPM-based Image Inpainting with Coarse-to-Fine Sampling](https://arxiv.org/abs/2407.05875)
42. [Anatomy-guided Pathology Segmentation](https://arxiv.org/abs/2407.05844)
43. [Evaluating the Fairness of Neural Collapse in Medical Image Classification](https://arxiv.org/abs/2407.05843)
44. [Chat-Edit-3D: Interactive 3D Scene Editing via Text Prompts](https://arxiv.org/abs/2407.06842)
45. [CycleSAM: One-Shot Surgical Scene Segmentation using Cycle-Consistent Feature Matching to Prompt SAM](https://arxiv.org/abs/2407.06795)
46. [Ensembled Cold-Diffusion Restorations for Unsupervised Anomaly Detection](https://arxiv.org/abs/2407.06635)
47. [Decomposition Betters Tracking Everything Everywhere](https://arxiv.org/abs/2407.06531)
48. [Reprogramming Distillation for Medical Foundation Models](https://arxiv.org/abs/2407.06504)
49. [A Single Transformer for Scalable Vision-Language Modeling](https://arxiv.org/abs/2407.06438)
50. [Non-Robust Features are Not Always Useful in One-Class Classification](https://arxiv.org/abs/2407.06372)
51. [MagMax: Leveraging Model Merging for Seamless Continual Learning](https://arxiv.org/abs/2407.06322)
52. [FairDiff: Fair Segmentation with Point-Image Diffusion](https://arxiv.org/abs/2407.06250)
53. [A Survey on Mixture of Experts](https://arxiv.org/abs/2407.06204)
54. [A Clinical Benchmark of Public Self-Supervised Pathology Foundation Models](https://arxiv.org/abs/2407.06508)
55. [Transformer Alignment in Large Language Models](https://arxiv.org/abs/2407.07810)
56. [ROSA: Random Subspace Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2407.07802)
57. [How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning](https://arxiv.org/abs/2407.07668)
58. [Tuning Vision-Language Models with Candidate Labels by Prompt Alignment](https://arxiv.org/abs/2407.07638)
59. [MARS: Mixture of Auto-Regressive Models for Fine-grained Text-to-image Synthesis](https://arxiv.org/abs/2407.07614)
60. [IRSAM: Advancing Segment Anything Model for Infrared Small Target Detection](https://arxiv.org/abs/2407.07520)
61. [Zero-Shot Class Unlearning in CLIP with Synthetic Samples](https://arxiv.org/abs/2407.07485)
62. [Rethinking Few-shot Class-incremental Learning: Learning from Yourself](https://arxiv.org/abs/2407.07468)
63. [Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models](https://arxiv.org/abs/2407.07263)
64. [Neuromimetic metaplasticity for adaptive continual learning](https://arxiv.org/abs/2407.07133)
65. [AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning](https://arxiv.org/abs/2407.07094)
66. [ProtoSAM -- One Shot Medical Image Segmentation With Foundational Models](https://arxiv.org/abs/2407.07042)
67. [Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models](https://arxiv.org/abs/2407.07035)
68. [Parameter-Efficient and Memory-Efficient Tuning for Vision Transformer: A Disentangled Approach](https://arxiv.org/abs/2407.06964)
69. [TACLE: Task and Class-aware Exemplar-free Semi-supervised Class Incremental Learning](https://arxiv.org/abs/2407.08041)
70. [Vulnerability Detection in Smart Contracts: A Comprehensive Survey](https://arxiv.org/abs/2407.07922)
71. [BiasPruner: Debiased Continual Learning for Medical Image Classification](https://arxiv.org/abs/2407.08609)
72. [SliceMamba for Medical Image Segmentation](https://arxiv.org/abs/2407.08481)
73. [Mitigating Catastrophic Forgetting in Language Transfer via Model Merging](https://arxiv.org/abs/2407.08699)
74. [Emergent Visual-Semantic Hierarchies in Image-Text Representations](https://arxiv.org/abs/2407.08521)
75. [Semi-Supervised Object Detection: A Survey on Progress from CNN to Transformer](https://arxiv.org/abs/2407.08460)
76. [CLEO: Continual Learning of Evolving Ontologies](https://arxiv.org/abs/2407.08411)
77. [On the attribution of confidence to large language models](https://arxiv.org/abs/2407.08388)
78. [Long-range Turbulence Mitigation: A Large-scale Dataset and A Coarse-to-fine Framework](https://arxiv.org/abs/2407.08377)
79. [Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation](https://arxiv.org/abs/2407.08268)
80. [Spiking Tucker Fusion Transformer for Audio-Visual Zero-Shot Learning](https://arxiv.org/abs/2407.08130)
81. [FYI: Flip Your Images for Dataset Distillation](https://arxiv.org/abs/2407.08113)
82. [Urban Waterlogging Detection: A Challenging Benchmark and Large-Small Model Co-Adapter](https://arxiv.org/abs/2407.08109)