1. [Observational Scaling Laws and the Predictability of Language Model Performance](https://arxiv.org/abs/2405.10938)
2. [Blackbox Adaptation for Medical Image Segmentation](https://arxiv.org/abs/2405.10913)
3. [ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios](https://arxiv.org/abs/2405.10808)
4. [Efficient Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2405.10739)
5. [Challenging the Human-in-the-loop in Algorithmic Decision-making](https://arxiv.org/abs/2405.10706)
6. [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](https://arxiv.org/abs/2405.10637)
7. [Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges](https://arxiv.org/abs/2405.10630)
8. [CM-UNet: Hybrid CNN-Mamba UNet for Remote Sensing Image Semantic Segmentation](https://arxiv.org/abs/2405.10530)
9. [Networking Systems for Video Anomaly Detection: A Tutorial and Survey](https://arxiv.org/abs/2405.10347)
10. [Adapting Large Multimodal Models to Distribution Shifts: The Role of In-Context Learning](https://arxiv.org/abs/2405.12217)
11. [DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM](https://arxiv.org/abs/2405.12139)
12. [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.12130)
13. [Position-Guided Prompt Learning for Anomaly Detection in Chest X-Rays](https://arxiv.org/abs/2405.11976)
14. [Data Augmentation for Text-based Person Retrieval Using Large Language Models](https://arxiv.org/abs/2405.11971)
15. [MM-Retinal: Knowledge-Enhanced Foundational Pretraining with Fundus Image-Text Expertise](https://arxiv.org/abs/2405.11793)
16. [Erasing the Bias: Fine-Tuning Foundation Models for Semi-Supervised Learning](https://arxiv.org/abs/2405.11756)
17. [Universal Organizer of SAM for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2405.11742)
18. [Reproducibility Study of CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification](https://arxiv.org/abs/2405.11574)
19. [DEMO: A Statistical Perspective for Efficient Image-Text Matching](https://arxiv.org/abs/2405.11496)
20. [BOSC: A Backdoor-based Framework for Open Set Synthetic Image Attribution](https://arxiv.org/abs/2405.11491)
21. [NubbleDrop: A Simple Way to Improve Matching Strategy for Prompted One-Shot Segmentation](https://arxiv.org/abs/2405.11476)
22. [Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion](https://arxiv.org/abs/2405.11464)
23. [MAML-en-LLM: Model Agnostic Meta-Training of LLMs for Improved In-Context Learning](https://arxiv.org/abs/2405.11446)
24. [A Unified Approach Towards Active Learning and Out-of-Distribution Detection](https://arxiv.org/abs/2405.11337)
25. [MediCLIP: Adapting CLIP for Few-shot Medical Image Anomaly Detection](https://arxiv.org/abs/2405.11315)
26. [Enhancing Fine-Grained Image Classifications via Cascaded Vision Language Models](https://arxiv.org/abs/2405.11301)
27. [Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts](https://arxiv.org/abs/2405.11273)
28. [Bayesian Learning-driven Prototypical Contrastive Loss for Class-Incremental Learning](https://arxiv.org/abs/2405.11067)
29. [Multimodal CLIP Inference for Meta-Few-Shot Image Classification](https://arxiv.org/abs/2405.10954)
30. [Diffusion Model Driven Test-Time Image Adaptation for Robust Skin Lesion Classification](https://arxiv.org/abs/2405.11289)
31. [Reducing Transformer Key-Value Cache Size with Cross-Layer Attention](https://arxiv.org/abs/2405.12981)
32. [Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control](https://arxiv.org/abs/2405.12970)
33. [An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation](https://arxiv.org/abs/2405.12914)
34. [Large Language Models Meet NLP: A Survey](https://arxiv.org/abs/2405.12819)
35. [C3L: Content Correlated Vision-Language Instruction Tuning Data Generation via Contrastive Learning](https://arxiv.org/abs/2405.12752)
36. [Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation](https://arxiv.org/abs/2405.12328)
37. [Mamba in Speech: Towards an Alternative to Self-Attention](https://arxiv.org/abs/2405.12609)
38. [Spatial-aware Attention Generative Adversarial Network for Semi-supervised Anomaly Detection in Medical Image](https://arxiv.org/abs/2405.12872)
39. [Hierarchical SegNet with Channel and Context Attention for Accurate Lung Segmentation in Chest X-ray Images](https://arxiv.org/abs/2405.12318)
40. [Bitune: Bidirectional Instruction-Tuning](https://arxiv.org/abs/2405.14862)
41. [Not All Language Model Features Are Linear](https://arxiv.org/abs/2405.14860)
42. [Mamba-R: Vision Mamba ALSO Needs Registers](https://arxiv.org/abs/2405.14858)
43. [Good Seed Makes a Good Crop: Discovering Secret Seeds in Text-to-Image Diffusion Models](https://arxiv.org/abs/2405.14828)
44. [Masked Image Modelling for retinal OCT understanding](https://arxiv.org/abs/2405.14788)
45. [CLIPScope: Enhancing Zero-Shot OOD Detection with Bayesian Scoring](https://arxiv.org/abs/2405.14737)
46. [Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models](https://arxiv.org/abs/2405.14715)
47. [Learning Multi-dimensional Human Preference for Text-to-Image Generation](https://arxiv.org/abs/2405.14705)
48. [Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference](https://arxiv.org/abs/2405.14700)
49. [Towards Imperceptible Backdoor Attack in Self-supervised Learning](https://arxiv.org/abs/2405.14672)
50. [Reinforcement Learning for Fine-tuning Text-to-speech Diffusion Models](https://arxiv.org/abs/2405.14632)
51. [U-TELL: Unsupervised Task Expert Lifelong Learning](https://arxiv.org/abs/2405.14623)
52. [Concept Visualization: Explaining the CLIP Multi-modal Embedding Using WordNet](https://arxiv.org/abs/2405.14563)
53. [Multistable Shape from Shading Emerges from Patch Diffusion](https://arxiv.org/abs/2405.14530)
54. [AnomalyDINO: Boosting Patch-based Few-shot Anomaly Detection with DINOv2](https://arxiv.org/abs/2405.14529)
55. [Identity Inference from CLIP Models using Only Textual Data](https://arxiv.org/abs/2405.14517)
56. [DuEDL: Dual-Branch Evidential Deep Learning for Scribble-Supervised Medical Image Segmentation](https://arxiv.org/abs/2405.14444)
57. [Adaptive Rentention & Correction for Continual Learning](https://arxiv.org/abs/2405.14318)
58. [Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models](https://arxiv.org/abs/2405.14297)
59. [Tuning-free Universally-Supervised Semantic Segmentation](https://arxiv.org/abs/2405.14294)
60. [Fine-grained Image-to-LiDAR Contrastive Distillation with Visual Foundation Models](https://arxiv.org/abs/2405.14271)
61. [LG-VQ: Language-Guided Codebook Learning](https://arxiv.org/abs/2405.14206)
62. [Multi-view Remote Sensing Image Segmentation With SAM priors](https://arxiv.org/abs/2405.14171)
63. [Leveraging Semantic Segmentation Masks with Embeddings for Fine-Grained Form Classification](https://arxiv.org/abs/2405.14162)
64. [RET-CLIP: A Retinal Image Foundation Model Pre-trained with Clinical Diagnostic Reports](https://arxiv.org/abs/2405.14137)
65. [Automated Loss function Search for Class-imbalanced Node Classification](https://arxiv.org/abs/2405.14133)
66. [Mixture of Experts Meets Prompt-Based Continual Learning](https://arxiv.org/abs/2405.14124)
67. [Refining Skewed Perceptions in Vision-Language Models through Visual Representations](https://arxiv.org/abs/2405.14030)
68. [Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning](https://arxiv.org/abs/2405.13978)
69. [MAGIC: Map-Guided Few-Shot Audio-Visual Acoustics Modeling](https://arxiv.org/abs/2405.13860)
70. [Robust Disaster Assessment from Aerial Imagery Using Text-to-Image Synthetic Data](https://arxiv.org/abs/2405.13779)
71. [What Makes Good Few-shot Examples for Vision-Language Models?](https://arxiv.org/abs/2405.13532)
72. [A Label Propagation Strategy for CutMix in Multi-Label Remote Sensing Image Classification](https://arxiv.org/abs/2405.13451)
73. [Unsupervised Pre-training with Language-Vision Prompts for Low-Data Instance Segmentation](https://arxiv.org/abs/2405.13388)
74. [Gradient Projection For Parameter-Efficient Continual Learning](https://arxiv.org/abs/2405.13383)
75. [MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models](https://arxiv.org/abs/2405.13053)
76. [CoLay: Controllable Layout Generation through Multi-conditional Latent Diffusion](https://arxiv.org/abs/2405.13045)
77. [Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image Generation](https://arxiv.org/abs/2405.14802)
78. [A label-free and data-free training strategy for vasculature segmentation in serial sectioning OCT data](https://arxiv.org/abs/2405.13757)