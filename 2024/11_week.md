1. [DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/abs/2403.05525)
2. [Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation](https://arxiv.org/abs/2403.05433)
3. [SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection](https://arxiv.org/abs/2403.05416)
4. [VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model](https://arxiv.org/abs/2403.05346)
5. [Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance](https://arxiv.org/abs/2403.05231)
6. [Continual Learning and Catastrophic Forgetting](https://arxiv.org/abs/2403.05175)
7. [CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model](https://arxiv.org/abs/2403.05124)
8. [FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2403.05408)
9. [LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation](https://arxiv.org/abs/2403.05246)
10. [CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion](https://arxiv.org/abs/2403.05121)
11. [BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion](https://arxiv.org/abs/2403.06976)
12. [SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data](https://arxiv.org/abs/2403.06952)
13. [MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning](https://arxiv.org/abs/2403.06914)
14. [Semantic Residual Prompts for Continual Learning](https://arxiv.org/abs/2403.06870)
15. [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://arxiv.org/abs/2403.06764)
16. [Probabilistic Contrastive Learning for Long-Tailed Visual Recognition](https://arxiv.org/abs/2403.06726)
17. [Trustworthy Partial Label Learning with Out-of-distribution Detection](https://arxiv.org/abs/2403.06681)
18. [CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin](https://arxiv.org/abs/2403.06670)
19. [Query-guided Prototype Evolution Network for Few-Shot Segmentation](https://arxiv.org/abs/2403.06488)
20. [DivCon: Divide and Conquer for Progressive Text-to-Image Generation](https://arxiv.org/abs/2403.06400)
21. [On the Diminishing Returns of Width for Continual Learning](https://arxiv.org/abs/2403.06398)
22. [Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models](https://arxiv.org/abs/2403.06381)
23. [A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets](https://arxiv.org/abs/2403.06295)
24. [Probing Image Compression For Class-Incremental Learning](https://arxiv.org/abs/2403.06288)
25. [Poly Kernel Inception Network for Remote Sensing Detection](https://arxiv.org/abs/2403.06258)
26. [Decoupled Contrastive Learning for Long-Tailed Recognition](https://arxiv.org/abs/2403.06151)
27. [RESTORE: Towards Feature Shift for Vision-Language Prompt Learning](https://arxiv.org/abs/2403.06136)
28. [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135)
29. [In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model](https://arxiv.org/abs/2403.06126)
30. [Universal Debiased Editing for Fair Medical Image Classification](https://arxiv.org/abs/2403.06104)
31. [VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models](https://arxiv.org/abs/2403.06098)
32. [Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?](https://arxiv.org/abs/2403.06092)
33. [Frequency Attention for Knowledge Distillation](https://arxiv.org/abs/2403.05894)
34. [Last Iterate Convergence of Incremental Methods and Applications in Continual Learning](https://arxiv.org/abs/2403.06873)
35. [Shortcut Learning in Medical Image Segmentation](https://arxiv.org/abs/2403.06748)
36. [Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation](https://arxiv.org/abs/2403.05912)
37. [Rethinking Generative Large Language Model Evaluation for Semantic Comprehension](https://arxiv.org/abs/2403.07872)
38. [MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric](https://arxiv.org/abs/2403.07839)
39. [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816)
40. [Multi-modal Auto-regressive Modeling via Visual Words](https://arxiv.org/abs/2403.07720)
41. [FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine Tuning in High-resolution Medical Image Classification](https://arxiv.org/abs/2403.07576)
42. [LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced Personality Detection Model](https://arxiv.org/abs/2403.07581)
43. [RSBuilding: Towards General Remote Sensing Image Building Extraction and Change Detection with Foundation Model](https://arxiv.org/abs/2403.07564)
44. [MoAI: Mixture of All Intelligence for Large Language and Vision Models](https://arxiv.org/abs/2403.07508)
45. [Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation](https://arxiv.org/abs/2403.07500)
46. [Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A Brain-Inspired Method for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2403.07440)
47. [Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized Visual Class Discovery](https://arxiv.org/abs/2403.07369)
48. [Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning](https://arxiv.org/abs/2403.07356)
49. [KEBench: A Benchmark on Knowledge Editing for Large Vision-Language Models](https://arxiv.org/abs/2403.07350)
50. [A Bayesian Approach to OOD Robustness in Image Classification](https://arxiv.org/abs/2403.07277)
51. [You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval](https://arxiv.org/abs/2403.07222)
52. [UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation](https://arxiv.org/abs/2403.07187)
53. [AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation](https://arxiv.org/abs/2403.07030)
54. [Adaptive Hyperparameter Optimization for Continual Learning Scenarios](https://arxiv.org/abs/2403.07015)
55. [When Eye-Tracking Meets Machine Learning: A Systematic Review on Applications in Medical Image Analysis](https://arxiv.org/abs/2403.07834)
56. [Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ Segmentation](https://arxiv.org/abs/2403.07303)
57. [Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA Images](https://arxiv.org/abs/2403.07116)
58. [Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models](https://arxiv.org/abs/2403.07066)