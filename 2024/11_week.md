1. [DeepSeek-VL: Towards Real-World Vision-Language Understanding](https://arxiv.org/abs/2403.05525)
2. [Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation](https://arxiv.org/abs/2403.05433)
3. [SIRST-5K: Exploring Massive Negatives Synthesis with Self-supervised Learning for Robust Infrared Small Target Detection](https://arxiv.org/abs/2403.05416)
4. [VLM-PL: Advanced Pseudo Labeling approach Class Incremental Object Detection with Vision-Language Model](https://arxiv.org/abs/2403.05346)
5. [Tracking Meets LoRA: Faster Training, Larger Model, Stronger Performance](https://arxiv.org/abs/2403.05231)
6. [Continual Learning and Catastrophic Forgetting](https://arxiv.org/abs/2403.05175)
7. [CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model](https://arxiv.org/abs/2403.05124)
8. [FedFMS: Exploring Federated Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2403.05408)
9. [LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image Segmentation](https://arxiv.org/abs/2403.05246)
10. [CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion](https://arxiv.org/abs/2403.05121)
11. [BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion](https://arxiv.org/abs/2403.06976)
12. [SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data](https://arxiv.org/abs/2403.06952)
13. [MEND: Meta dEmonstratioN Distillation for Efficient and Effective In-Context Learning](https://arxiv.org/abs/2403.06914)
14. [Semantic Residual Prompts for Continual Learning](https://arxiv.org/abs/2403.06870)
15. [An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models](https://arxiv.org/abs/2403.06764)
16. [Probabilistic Contrastive Learning for Long-Tailed Visual Recognition](https://arxiv.org/abs/2403.06726)
17. [Trustworthy Partial Label Learning with Out-of-distribution Detection](https://arxiv.org/abs/2403.06681)
18. [CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar Class-Incremental Learnin](https://arxiv.org/abs/2403.06670)
19. [Query-guided Prototype Evolution Network for Few-Shot Segmentation](https://arxiv.org/abs/2403.06488)
20. [DivCon: Divide and Conquer for Progressive Text-to-Image Generation](https://arxiv.org/abs/2403.06400)
21. [On the Diminishing Returns of Width for Continual Learning](https://arxiv.org/abs/2403.06398)
22. [Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention Regulation in Diffusion Models](https://arxiv.org/abs/2403.06381)
23. [A streamlined Approach to Multimodal Few-Shot Class Incremental Learning for Fine-Grained Datasets](https://arxiv.org/abs/2403.06295)
24. [Probing Image Compression For Class-Incremental Learning](https://arxiv.org/abs/2403.06288)
25. [Poly Kernel Inception Network for Remote Sensing Detection](https://arxiv.org/abs/2403.06258)
26. [Decoupled Contrastive Learning for Long-Tailed Recognition](https://arxiv.org/abs/2403.06151)
27. [RESTORE: Towards Feature Shift for Vision-Language Prompt Learning](https://arxiv.org/abs/2403.06136)
28. [MACE: Mass Concept Erasure in Diffusion Models](https://arxiv.org/abs/2403.06135)
29. [In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model](https://arxiv.org/abs/2403.06126)
30. [Universal Debiased Editing for Fair Medical Image Classification](https://arxiv.org/abs/2403.06104)
31. [VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models](https://arxiv.org/abs/2403.06098)
32. [Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?](https://arxiv.org/abs/2403.06092)
33. [Frequency Attention for Knowledge Distillation](https://arxiv.org/abs/2403.05894)
34. [Last Iterate Convergence of Incremental Methods and Applications in Continual Learning](https://arxiv.org/abs/2403.06873)
35. [Shortcut Learning in Medical Image Segmentation](https://arxiv.org/abs/2403.06748)
36. [Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation](https://arxiv.org/abs/2403.05912)