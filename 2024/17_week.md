1. [Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs](https://arxiv.org/abs/2404.13033)
2. [LaPA: Latent Prompt Assist Model For Medical Visual Question Answering](https://arxiv.org/abs/2404.13039)
3. [MoVA: Adapting Mixture of Vision Experts to Multimodal Context](https://arxiv.org/abs/2404.13046)
4. [Next Generation Loss Function for Image Classification](https://arxiv.org/abs/2404.12948)
5. [Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images](https://arxiv.org/abs/2404.12922)
6. [Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images](https://arxiv.org/abs/2404.12908)
7. [How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?](https://arxiv.org/abs/2404.12866)
8. [ECOR: Explainable CLIP for Object Recognition](https://arxiv.org/abs/2404.12839)
9. [TextSquare: Scaling up Text-Centric Visual Instruction Tuning](https://arxiv.org/abs/2404.12803)
10. [Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for Vision-Language Models](https://arxiv.org/abs/2404.12588)
11. [GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models](https://arxiv.org/abs/2404.12541)
12. [Adaptive Memory Replay for Continual Learning](https://arxiv.org/abs/2404.12526)