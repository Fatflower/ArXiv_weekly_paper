1. [DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model](https://arxiv.org/abs/2408.17433)
2. [Open-vocabulary Temporal Action Localization using VLMs](https://arxiv.org/abs/2408.17422)
3. [LSMS: Language-guided Scale-aware MedSegmentor for Medical Image Referring Segmentation](https://arxiv.org/abs/2408.17347)
4. [Evaluating Reliability in Medical DNNs: A Critical Analysis of Feature and Confidence-Based OOD Detection](https://arxiv.org/abs/2408.17337)
5. [Flexible and Effective Mixing of Large Language Models into a Mixture of Domain Experts](https://arxiv.org/abs/2408.17280)
6. [Abstracted Gaussian Prototypes for One-Shot Concept Learning](https://arxiv.org/abs/2408.17251)
7. [The Many Faces of Optimal Weak-to-Strong Learning](https://arxiv.org/abs/2408.17148)
8. [Novel-WD: Exploring acquisition of Novel World Knowledge in LLMs Using Prefix-Tuning](https://arxiv.org/abs/2408.17070)
9. [Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer](https://arxiv.org/abs/2408.17062)
10. [Text-to-Image Generation Via Energy-Based CLIP](https://arxiv.org/abs/2408.17046)
11. [CP-VoteNet: Contrastive Prototypical VoteNet for Few-Shot Point Cloud Object Detection](https://arxiv.org/abs/2408.17036)
12. [Improving Time Series Classification with Representation Soft Label Smoothing](https://arxiv.org/abs/2408.17010)
13. [An Empirical Study of Scaling Laws for Transfer](https://arxiv.org/abs/2408.16947)
14. [FlowRetrieval: Flow-Guided Data Retrieval for Few-Shot Imitation Learning](https://arxiv.org/abs/2408.16944)
15. [Theoretical Insights into Overparameterized Models in Multi-Task and Replay-Based Continual Learning](https://arxiv.org/abs/2408.16939)
16. [VLM-KD: Knowledge Distillation from VLM for Long-Tail Visual Recognition](https://arxiv.org/abs/2408.16930)
17. [FineFACE: Fair Facial Attribute Classification Leveraging Fine-grained Features](https://arxiv.org/abs/2408.16881)
18. [STEREO: Towards Adversarially Robust Concept Erasing from Text-to-Image Generation Models](https://arxiv.org/abs/2408.16807)
19. [Generative AI in Ship Design](https://arxiv.org/abs/2408.16798)
20. [An Effective Information Theoretic Framework for Channel Pruning](https://arxiv.org/abs/2408.16772)
21. [LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation](https://arxiv.org/abs/2408.16886)
22. [Accurate Forgetting for All-in-One Image Restoration Model](https://arxiv.org/abs/2409.00685)
23. [YOLOO: You Only Learn from Others Once](https://arxiv.org/abs/2409.00618)
24. [Sparse Mamba: Reinforcing Controllability In Structural State Space Models](https://arxiv.org/abs/2409.00563)
25. [FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model](https://arxiv.org/abs/2409.00556)
26. [Data Augmentation for Image Classification using Generative AI](https://arxiv.org/abs/2409.00547)
27. [How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?](https://arxiv.org/abs/2409.00543)
28. [Incremental Open-set Domain Adaptation](https://arxiv.org/abs/2409.00530)
29. [A Survey for Large Language Models in Biomedicine](https://arxiv.org/abs/2409.00133)
30. [Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models](https://arxiv.org/abs/2409.00084)
31. [HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts](https://arxiv.org/abs/2409.02919)
32. [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://arxiv.org/abs/2409.02889)
33. [Benchmarking Spurious Bias in Few-Shot Image Classifiers](https://arxiv.org/abs/2409.02882)
34. [Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning](https://arxiv.org/abs/2409.02850)
35. [Towards a Unified View of Preference Learning for Large Language Models: A Survey](https://arxiv.org/abs/2409.02795)
36. [A Comparative Study of Pre-training and Self-training](https://arxiv.org/abs/2409.02751)
37. [MedUnA: Language guided Unsupervised Adaptation of Vision-Language Models for Medical Image Classification](https://arxiv.org/abs/2409.02729)
38. [Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs](https://arxiv.org/abs/2409.02686)
39. [LLM-Assisted Visual Analytics: Opportunities and Challenges](https://arxiv.org/abs/2409.02691)
40. [Few-shot Multi-Task Learning of Linear Invariant Features with Meta Subspace Pursuit](https://arxiv.org/abs/2409.02708)
41. [Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection](https://arxiv.org/abs/2409.02664)
42. [A Medical Multimodal Large Language Model for Pediatric Pneumonia](https://arxiv.org/abs/2409.02608)
43. [Evaluation Study on SAM 2 for Class-agnostic Instance-level Segmentation](https://arxiv.org/abs/2409.02567)
44. [Vision-Language Navigation with Continual Learning](https://arxiv.org/abs/2409.02561)
45. [SG-MIM: Structured Knowledge Guided Efficient Pre-training for Dense Prediction](https://arxiv.org/abs/2409.02513)
46. [Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis](https://arxiv.org/abs/2409.02429)
47. [Neural Dynamics Model of Visual Decision-Making: Learning from Human Experts](https://arxiv.org/abs/2409.02390)
48. [How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?](https://arxiv.org/abs/2409.02253)
49. [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060)
50. [PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for Efficient Point Cloud Classification](https://arxiv.org/abs/2409.02007)
51. [Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey](https://arxiv.org/abs/2409.01980)
52. [Efficient LLM Context Distillation](https://arxiv.org/abs/2409.01930)
53. [Towards Generative Class Prompt Learning for Few-shot Visual Recognition](https://arxiv.org/abs/2409.01835)
54. [When Does Visual Prompting Outperform Linear Probing for Vision-Language Models? A Likelihood Perspective](https://arxiv.org/abs/2409.01821)
55. [Shuffle Mamba: State Space Models with Random Shuffle for Multi-Modal Image Fusion](https://arxiv.org/abs/2409.01728)
56. [LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation](https://arxiv.org/abs/2409.01572)
57. [CT-SDM: A Sampling Diffusion Model for Sparse-View CT Reconstruction across All Sampling Rates](https://arxiv.org/abs/2409.01571)
58. [Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning](https://arxiv.org/abs/2409.01483)
59. [Towards General Industrial Intelligence: A Survey on IIoT-Enhanced Continual Large Models](https://arxiv.org/abs/2409.01207)
60. [Logit Scaling for Out-of-Distribution Detection](https://arxiv.org/abs/2409.01175)
61. [Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning](https://arxiv.org/abs/2409.01128)
62. [SOOD-ImageNet: a Large-Scale Dataset for Semantic Out-Of-Distribution Image Classification and Semantic Segmentation](https://arxiv.org/abs/2409.01109)
63. [Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning](https://arxiv.org/abs/2409.01035)
64. [MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM](https://arxiv.org/abs/2409.00924)
65. [Modeling Text-Label Alignment for Hierarchical Text Classification](https://arxiv.org/abs/2409.00788)
66. [Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene Classification](https://arxiv.org/abs/2409.00698)
67. [Curriculum Prompting Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2409.00695)
68. [Foundation Model or Finetune? Evaluation of few-shot semantic segmentation for river pollution](https://arxiv.org/abs/2409.03754)
69. [Attention Heads of Large Language Models: A Survey](https://arxiv.org/abs/2409.03752)
70. [The representation landscape of few-shot learning and fine-tuning in large language models](https://arxiv.org/abs/2409.03662)
71. [Text-Guided Mixup Towards Long-Tailed Image Categorization](https://arxiv.org/abs/2409.03583)
72. [TG-LMM: Enhancing Medical Image Segmentation Accuracy through Text-Guided Large Multi-Modal Model](https://arxiv.org/abs/2409.03412)
73. [Labeled-to-Unlabeled Distribution Alignment for Partially-Supervised Multi-Organ Medical Image Segmentation](https://arxiv.org/abs/2409.03228)
74. [PEPL: Precision-Enhanced Pseudo-Labeling for Fine-Grained Image Classification in Semi-Supervised Learning](https://arxiv.org/abs/2409.03192)
75. [Better Verified Explanations with Applications to Incorrectness and Out-of-Distribution Detection](https://arxiv.org/abs/2409.03060)
76. [Can Your Generative Model Detect Out-of-Distribution Covariate Shift?](https://arxiv.org/abs/2409.03043)
77. [No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning](https://arxiv.org/abs/2409.03025)
78. [Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2409.02958)
79. [MSTT-199: MRI Dataset for Musculoskeletal Soft Tissue Tumor Segmentation](https://arxiv.org/abs/2409.03110)