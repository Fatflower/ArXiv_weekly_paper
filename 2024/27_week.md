1. [PECTP: Parameter-Efficient Cross-Task Prompts for Incremental Vision Transformer](https://arxiv.org/abs/2407.03813)
2. [Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners](https://arxiv.org/abs/2407.04003)
3. [Learning Non-Linear Invariants for Unsupervised Out-of-Distribution Detection](https://arxiv.org/abs/2407.04022)
4. [Elevating All Zero-Shot Sketch-Based Image Retrieval Through Multimodal Prompt Learning](https://arxiv.org/abs/2407.04207)
5. [Prompt Refinement with Image Pivot for Text-to-Image Generation](https://arxiv.org/abs/2407.00247)
6. [From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models](https://arxiv.org/abs/2407.00263)
7. [Unveiling Glitches: A Deep Dive into Image Encoding Bugs within CLIP](https://arxiv.org/abs/2407.00592)
8. [The impact of model size on catastrophic forgetting in Online Continual Learning](https://arxiv.org/abs/2407.00176)
9. [TEAL: New Selection Strategy for Small Buffers in Experience Replay Class Incremental Learning](https://arxiv.org/abs/2407.00673)
10. [Diffusion Models and Representation Learning: A Survey](https://arxiv.org/abs/2407.00783)
11. [Large Language Model Enhanced Knowledge Representation Learning: A Survey](https://arxiv.org/abs/2407.00936)
12. [FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models](https://arxiv.org/abs/2407.00983)
13. [Embedded Prompt Tuning: Towards Enhanced Calibration of Pretrained Models for Medical Images](https://arxiv.org/abs/2407.01003)
14. [DynaThink: Fast or Slow? A Dynamic Decision-Making Framework for Large Language Models](https://arxiv.org/abs/2407.01009)
15. [Unaligning Everything: Or Aligning Any Text to Any Image in Multimodal Models](https://arxiv.org/abs/2407.01157)
16. [Efficient Cutting Tool Wear Segmentation Based on Segment Anything Model](https://arxiv.org/abs/2407.01211)
17. [ToCoAD: Two-Stage Contrastive Learning for Industrial Anomaly Detection](https://arxiv.org/abs/2407.01312)
18. [Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://arxiv.org/abs/2407.01320)
19. [GalLoP: Learning Global and Local Prompts for Vision-Language Models](https://arxiv.org/abs/2407.01400)
20. [Semantic Compositions Enhance Vision-Language Contrastive Learning](https://arxiv.org/abs/2407.01408)
21. [To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models](https://arxiv.org/abs/2407.01920)
22. [Self-Cooperation Knowledge Distillation for Novel Class Discovery](https://arxiv.org/abs/2407.01930)
23. [SAVE: Segment Audio-Visual Easy way using Segment Anything Model](https://arxiv.org/abs/2407.02004)
24. [Multi-Grained Contrast for Data-Efficient Unsupervised Representation Learning](https://arxiv.org/abs/2407.02014)
25. [Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts](https://arxiv.org/abs/2407.02075)
26. [Hybrid Feature Collaborative Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2407.02123)
27. [Conceptual Codebook Learning for Vision-Language Models](https://arxiv.org/abs/2407.02350)
28. [Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything](https://arxiv.org/abs/2407.02534)
29. [MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language Models](https://arxiv.org/abs/2407.02775)
30. [YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision](https://arxiv.org/abs/2407.02988)
31. [Enhancing Translation Accuracy of Large Language Models through Continual Pre-Training on Parallel Data](https://arxiv.org/abs/2407.03145)
32. [SOWA: Adapting Hierarchical Frozen Window Self-Attention to Visual-Language Models for Better Anomaly Detection](https://arxiv.org/abs/2407.03634)