1. [Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts](https://arxiv.org/abs/2402.15505)
2. [A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends](https://arxiv.org/abs/2402.15490)
3. [Debiasing Machine Learning Models by Using Weakly Supervised Learning](https://arxiv.org/abs/2402.15477)
4. [Active Few-Shot Fine-Tuning](https://arxiv.org/abs/2402.15441)
5. [Hierarchical Invariance for Robust and Interpretable Vision Tasks at Larger Scales](https://arxiv.org/abs/2402.15430)
6. [Does Combining Parameter-efficient Modules Improve Few-shot Transfer Accuracy?](https://arxiv.org/abs/2402.15414)
7. [Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding](https://arxiv.org/abs/2402.15300)
8. [Label-efficient Multi-organ Segmentation Method with Diffusion Model](https://arxiv.org/abs/2402.15216)
9. [Machine Unlearning of Pre-trained Large Language Models](https://arxiv.org/abs/2402.15159)
10. [PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2402.15082)
11. [CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models](https://arxiv.org/abs/2402.15021)
12. [Consistency-Guided Temperature Scaling Using Style and Content Information for Out-of-Domain Calibration](https://arxiv.org/abs/2402.15019)
13. [Practical Insights into Knowledge Distillation for Pre-Trained Models](https://arxiv.org/abs/2402.14922)
14. [HumanEval on Latest GPT Models -- 2024](https://arxiv.org/abs/2402.14852)
15. [Smoothness Adaptive Hypothesis Transfer Learning](https://arxiv.org/abs/2402.14966)
16. [Multi-LoRA Composition for Image Generation](https://arxiv.org/abs/2402.16843)
17. [GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning](https://arxiv.org/abs/2402.16829)
18. [Training Neural Networks from Scratch with Parallel Low-Rank Adapters](https://arxiv.org/abs/2402.16828)
19. [A Survey on Data Selection for Language Models](https://arxiv.org/abs/2402.16827)
20. [Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models](https://arxiv.org/abs/2402.16696)
21. [ConSept: Continual Semantic Segmentation via Adapter-based Vision Transformer](https://arxiv.org/abs/2402.16674)
22. [Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing](https://arxiv.org/abs/2402.16627)
23. [LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification](https://arxiv.org/abs/2402.16515)
24. [Placing Objects in Context via Inpainting for Out-of-distribution Segmentation](https://arxiv.org/abs/2402.16392)
25. [Graph Learning under Distribution Shifts: A Comprehensive Survey on Domain Adaptation, Out-of-distribution, and Continual Learning](https://arxiv.org/abs/2402.16374)
26. [An Integrated Data Processing Framework for Pretraining Foundation Models](https://arxiv.org/abs/2402.16358)
27. [BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning of SAM](https://arxiv.org/abs/2402.16338)
28. [Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models](https://arxiv.org/abs/2402.16315)
29. [Few-Shot Learning for Annotation-Efficient Nucleus Instance Segmentation](https://arxiv.org/abs/2402.16280)
30. [Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space](https://arxiv.org/abs/2402.16267)
31. [One-stage Prompt-based Continual Learning](https://arxiv.org/abs/2402.16189)
32. [Task Specific Pretraining with Noisy Labels for Remote sensing Image Segmentation](https://arxiv.org/abs/2402.16164)
33. [PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization](https://arxiv.org/abs/2402.16141)
34. [Semi-supervised Open-World Object Detection](https://arxiv.org/abs/2402.16013)
35. [CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge](https://arxiv.org/abs/2402.15726)
36. [Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors](https://arxiv.org/abs/2402.15713)
37. [Investigating the Robustness of Vision Transformers against Label Noise in Medical Image Classification](https://arxiv.org/abs/2402.16734)
38. [Integrating Preprocessing Methods and Convolutional Neural Networks for Effective Tumor Detection in Medical Imaging](https://arxiv.org/abs/2402.16221)
39. [DiCoM -- Diverse Concept Modeling towards Enhancing Generalizability in Chest X-Ray Studies](https://arxiv.org/abs/2402.15534)
40. [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
41. [VRP-SAM: SAM with Visual Reference Prompt](https://arxiv.org/abs/2402.17726)
42. [Securing Reliability: A Brief Overview on Enhancing In-Context Learning for Foundation Models](https://arxiv.org/abs/2402.17671)
43. [Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2402.17614)
44. [Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label](https://arxiv.org/abs/2402.17555)
45. [Label-Noise Robust Diffusion Models](https://arxiv.org/abs/2402.17517)
46. [Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning](https://arxiv.org/abs/2402.17510)
47. [FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation](https://arxiv.org/abs/2402.17502)
48. [PANDAS: Prototype-based Novel Class Discovery and Detection](https://arxiv.org/abs/2402.17420)
49. [LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning](https://arxiv.org/abs/2402.17406)
50. [RECOST: External Knowledge Guided Data-efficient Instruction Tuning](https://arxiv.org/abs/2402.17355)
51. [SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection](https://arxiv.org/abs/2402.17323)
52. [ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks](https://arxiv.org/abs/2402.17298)
53. [Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263)
54. [When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method](https://arxiv.org/abs/2402.17193)
55. [Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models](https://arxiv.org/abs/2402.17177)
56. [Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation](https://arxiv.org/abs/2402.16933)
57. [LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language](https://arxiv.org/abs/2402.16929)
58. [PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA](https://arxiv.org/abs/2402.16902)
59. [MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation](https://arxiv.org/abs/2402.17725)