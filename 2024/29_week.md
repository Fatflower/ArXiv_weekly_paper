1. [Adaptive Prediction Ensemble: Improving Out-of-Distribution Generalization of Motion Forecasting](https://arxiv.org/abs/2407.09475)
2. [6G: The Intelligent Network of Everything -- A Comprehensive Vision, Survey, and Tutorial](https://arxiv.org/abs/2407.09398)
3. [GAVEL: Generating Games Via Evolution and Language Models](https://arxiv.org/abs/2407.09388)
4. [Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text](https://arxiv.org/abs/2407.09364)
5. [iNeMo: Incremental Neural Mesh Models for Robust Class-Incremental Learning](https://arxiv.org/abs/2407.09271)
6. [On the Role of Discrete Tokenization in Visual Representation Learning](https://arxiv.org/abs/2407.09087)
7. [SlideGCD: Slide-based Graph Collaborative Training with Knowledge Distillation for Whole Slide Image Classification](https://arxiv.org/abs/2407.08968)
8. [LAPT: Label-driven Automated Prompt Tuning for OOD Detection with Vision-Language Models](https://arxiv.org/abs/2407.08966)
9. [Lite-SAM Is Actually What You Need for Segment Everything](https://arxiv.org/abs/2407.08965)
10. [FD-SOS: Vision-Language Open-Set Detectors for Bone Fenestration and Dehiscence Detection from Intraoral Images](https://arxiv.org/abs/2407.09088)
11. [Learning Confidence Bounds for Classification with Imbalanced Data](https://arxiv.org/abs/2407.11878)
12. [Mitigating Background Shift in Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2407.11859)
13. [Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models](https://arxiv.org/abs/2407.11717)
14. [DiNO-Diffusion. Scaling Medical Diffusion via Self-Supervised Pre-Training](https://arxiv.org/abs/2407.11594)
15. [Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification](https://arxiv.org/abs/2407.11573)
16. [Reasoning with Large Language Models, a Survey](https://arxiv.org/abs/2407.11511)
17. [EndoFinder: Online Image Retrieval for Explainable Colorectal Polyp Diagnosis](https://arxiv.org/abs/2407.11401)
18. [Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts](https://arxiv.org/abs/2407.11382)
19. [The Devil is in the Statistics: Mitigating and Exploiting Statistics Difference for Generalizable Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2407.11356)
20. [CLAMS: A System for Zero-Shot Model Selection for Clustering](https://arxiv.org/abs/2407.11286)
21. [Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion](https://arxiv.org/abs/2407.11211)
22. [A Survey on LoRA of Large Language Models](https://arxiv.org/abs/2407.11046)
23. [How Good Is It? Evaluating the Efficacy of Common versus Domain-Specific Prompts on Foundational Large Language Models](https://arxiv.org/abs/2407.11006)
24. [Generative AI Systems: A Systems-based Perspective on Generative AI](https://arxiv.org/abs/2407.11001)
25. [TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot](https://arxiv.org/abs/2407.10999)
26. [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)
27. [Towards Enhanced Classification of Abnormal Lung sound in Multi-breath: A Light Weight Multi-label and Multi-head Attention Classification Method](https://arxiv.org/abs/2407.10828)
28. [Pathology-knowledge Enhanced Multi-instance Prompt Learning for Few-shot Whole Slide Image Classification](https://arxiv.org/abs/2407.10814)
29. [SEED: A Simple and Effective 3D DETR in Point Clouds](https://arxiv.org/abs/2407.10749)
30. [Anticipating Future Object Compositions without Forgetting](https://arxiv.org/abs/2407.10723)
31. [Qwen2 Technical Report](https://arxiv.org/abs/2407.10671)
32. [OVLW-DETR: Open-Vocabulary Light-Weighted Detection Transformer](https://arxiv.org/abs/2407.10655)
33. [Don't Throw Away Data: Better Sequence Knowledge Distillation](https://arxiv.org/abs/2407.10456)
34. [A Multi-Stage Framework for 3D Individual Tooth Segmentation in Dental CBCT](https://arxiv.org/abs/2407.10433)
35. [Accessing Vision Foundation Models at ImageNet-level Costs](https://arxiv.org/abs/2407.10366)
36. [Beyond Prompt Learning: Continual Adapter for Efficient Rehearsal-Free Continual Learning](https://arxiv.org/abs/2407.10281)
37. [Visual Prompt Selection for In-Context Learning Segmentation](https://arxiv.org/abs/2407.10233)
38. [WPS-SAM: Towards Weakly-Supervised Part Segmentation with Foundation Models](https://arxiv.org/abs/2407.10131)
39. [What Appears Appealing May Not be Significant! -- A Clinical Perspective of Diffusion Models](https://arxiv.org/abs/2407.10029)
40. [Investigating Low-Rank Training in Transformer Language Models: Efficiency and Scaling Analysis](https://arxiv.org/abs/2407.09835)
41. [Explanation is All You Need in Distillation: Mitigating Bias and Shortcut Learning](https://arxiv.org/abs/2407.09788)
42. [Diagnosing and Re-learning for Balanced Multimodal Learning](https://arxiv.org/abs/2407.09705)
43. [Segmentation of Prostate Tumour Volumes from PET Images is a Different Ball Game](https://arxiv.org/abs/2407.10537)
44. [SACNet: A Spatially Adaptive Convolution Network for 2D Multi-organ Medical Segmentation](https://arxiv.org/abs/2407.10157)
45. [DiffRect: Latent Diffusion Label Rectification for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2407.09918)
46. [Learning Multi-view Anomaly Detection](https://arxiv.org/abs/2407.11935)
47. [Patch-Level Training for Large Language Models](https://arxiv.org/abs/2407.12665)
48. [Evaluating Linguistic Capabilities of Multimodal LLMs in the Lens of Few-Shot Learning](https://arxiv.org/abs/2407.12498)
49. [Dual-level Adaptive Self-Labeling for Novel Class Discovery in Point Cloud Segmentation](https://arxiv.org/abs/2407.12489)
50. [ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference](https://arxiv.org/abs/2407.12442)
51. [FETCH: A Memory-Efficient Replay Approach for Continual Learning in Image Classification](https://arxiv.org/abs/2407.12375)
52. [ER-FSL: Experience Replay with Feature Subspace Learning for Online Continual Learning](https://arxiv.org/abs/2407.12279)
53. [A Closer Look at Benchmarking Self-Supervised Pre-training with Image Classification](https://arxiv.org/abs/2407.12210)
54. [CroMo-Mixup: Augmenting Cross-Model Representations for Continual Self-Supervised Learning](https://arxiv.org/abs/2407.12188)
55. [Evaluation of Bias Towards Medical Professionals in Large Language Models](https://arxiv.org/abs/2407.12031)
56. [LiteGPT: Large Vision-Language Model for Joint Chest X-ray Localization and Classification Task](https://arxiv.org/abs/2407.12064)
57. [Across-subject ensemble-learning alleviates the need for large samples for fMRI decoding](https://arxiv.org/abs/2407.12056)
58. [Addressing Imbalance for Class Incremental Learning in Medical Image Classification](https://arxiv.org/abs/2407.13768)
59. [BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models](https://arxiv.org/abs/2407.13442)
60. [Learning from the Web: Language Drives Weakly-Supervised Incremental Learning for Semantic Segmentation](https://arxiv.org/abs/2407.13363)