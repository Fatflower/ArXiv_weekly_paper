1. [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/abs/2408.05211)
2. [TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning](https://arxiv.org/abs/2408.05200)
3. [Cautious Calibration in Binary Classification](https://arxiv.org/abs/2408.05120)
4. [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)
5. [UNIC: Universal Classification Models via Multi-teacher Distillation](https://arxiv.org/abs/2408.05088)
6. [In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2408.04961)
7. [Avoid Wasted Annotation Costs in Open-set Active Learning with Pre-trained Vision-Language Model](https://arxiv.org/abs/2408.04917)
8. [GuidedNet: Semi-Supervised Multi-Organ Segmentation via Labeled Data Guide Unlabeled Data](https://arxiv.org/abs/2408.04914)
9. [ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation](https://arxiv.org/abs/2408.04883)
10. [On the Element-Wise Representation and Reasoning in Zero-Shot Image Recognition: A Systematic Survey](https://arxiv.org/abs/2408.04879)
11. [Your Classifier Can Be Secretly a Likelihood-Based OOD Detector](https://arxiv.org/abs/2408.04851)
12. [Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)](https://arxiv.org/abs/2408.04664)
13. [Beyond the Eye: A Relational Model for Early Dementia Detection Using Retinal OCTA Images](https://arxiv.org/abs/2408.05117)
14. [See It All: Contextualized Late Aggregation for 3D Dense Captioning](https://arxiv.org/abs/2408.07648)
15. [Attention-Guided Perturbation for Unsupervised Image Anomaly Detection](https://arxiv.org/abs/2408.07490)
16. [BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt Learning](https://arxiv.org/abs/2408.07440)
17. [Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation](https://arxiv.org/abs/2408.07343)
18. [UniFed: A Universal Federation of a Mixture of Highly Heterogeneous Medical Image Classification Tasks](https://arxiv.org/abs/2408.07075)
19. [Imagen 3](https://arxiv.org/abs/2408.07009)
20. [BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning](https://arxiv.org/abs/2408.06890)
21. [LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models](https://arxiv.org/abs/2408.06854)
22. [Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning](https://arxiv.org/abs/2408.06798)
23. [Layerwise Recurrent Router for Mixture-of-Experts](https://arxiv.org/abs/2408.06793)
24. [ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2408.06747)
25. [Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to Tail](https://arxiv.org/abs/2408.06742)
26. [DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion](https://arxiv.org/abs/2408.06740)
27. [Towards Cross-Domain Single Blood Cell Image Classification via Large-Scale LoRA-based Segment Anything Model](https://arxiv.org/abs/2408.06716)
28. [Masked Image Modeling: A Survey](https://arxiv.org/abs/2408.06687)
29. [IFShip: A Large Vision-Language Model for Interpretable Fine-grained Ship Classification via Domain Knowledge-Enhanced Instruction Tuning](https://arxiv.org/abs/2408.06631)
30. [CROME: Cross-Modal Adapters for Efficient Multimodal LLM](https://arxiv.org/abs/2408.06610)
31. [S-SAM: SVD-based Fine-Tuning of Segment Anything Model for Medical Image Segmentation](https://arxiv.org/abs/2408.06447)
32. [Attention Based Feature Fusion Network for Monkeypox Skin Lesion Detection](https://arxiv.org/abs/2408.06640)
33. [How good nnU-Net for Segmenting Cardiac MRI: A Comprehensive Evaluation](https://arxiv.org/abs/2408.06358)
34. [VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents](https://arxiv.org/abs/2408.06327)
35. [From SAM to SAM 2: Exploring Improvements in Meta's Segment Anything Model](https://arxiv.org/abs/2408.06305)
36. [FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data](https://arxiv.org/abs/2408.06273)
37. [Context-aware Visual Storytelling with Visual Prefix Tuning and Contrastive Learning](https://arxiv.org/abs/2408.06259)
38. [Correlation Weighted Prototype-based Self-Supervised One-Shot Segmentation of Medical Images](https://arxiv.org/abs/2408.06235)
39. [Med42-v2: A Suite of Clinical LLMs](https://arxiv.org/abs/2408.06142)
40. [HySparK: Hybrid Sparse Masking for Large Scale Medical Image Pre-Training](https://arxiv.org/abs/2408.05815)
41. [Efficient Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2408.05775)
42. [A Novel Momentum-Based Deep Learning Techniques for Medical Image Classification and Segmentation](https://arxiv.org/abs/2408.05692)
43. [PS-TTL: Prototype-based Soft-labels and Test-Time Learning for Few-shot Object Detection](https://arxiv.org/abs/2408.05674)
44. [SAM-FNet: SAM-Guided Fusion Network for Laryngo-Pharyngeal Tumor Detection](https://arxiv.org/abs/2408.05426)
45. [Zero-shot 3D Segmentation of Abdominal Organs in CT Scans Using Segment Anything Model 2: Adapting Video Tracking Capabilities for 3D Medical Imaging](https://arxiv.org/abs/2408.06170)
46. [Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer Detection](https://arxiv.org/abs/2408.05892)
47. [Category-Prompt Refined Feature Learning for Long-Tailed Multi-Label Image Classification](https://arxiv.org/abs/2408.08125)
48. [DIVE: Towards Descriptive and Diverse Visual Commonsense Generation](https://arxiv.org/abs/2408.08021)
49. [Surgical SAM 2: Real-time Segment Anything in Surgical Video by Efficient Frame Pruning](https://arxiv.org/abs/2408.07931)
50. [MedTsLLM: Leveraging LLMs for Multimodal Medical Time Series Analysis](https://arxiv.org/abs/2408.07773)
51. [SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training](https://arxiv.org/abs/2408.08295)
52. [BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts](https://arxiv.org/abs/2408.08274)