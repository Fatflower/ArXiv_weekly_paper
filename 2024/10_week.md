1. [UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control](https://arxiv.org/abs/2403.02332)
2. [Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training](https://arxiv.org/abs/2403.02325)
3. [Non-autoregressive Sequence-to-Sequence Vision-Language Models](https://arxiv.org/abs/2403.02249)
4. [MiM-ISTD: Mamba-in-Mamba for Efficient Infrared Small Target Detection](https://arxiv.org/abs/2403.02148)
5. [Towards Implicit Prompt For Text-To-Image Models](https://arxiv.org/abs/2403.02118)
6. [VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT](https://arxiv.org/abs/2403.02076)
7. [Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation](https://arxiv.org/abs/2403.02074)
8. [Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey](https://arxiv.org/abs/2403.01909)
9. [A Survey on Evaluation of Out-of-Distribution Generalization](https://arxiv.org/abs/2403.01874)
10. [One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models](https://arxiv.org/abs/2403.01849)
11. [AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2403.01818)
12. [Transformers for Supervised Online Continual Learning](https://arxiv.org/abs/2403.01554)
13. [Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey](https://arxiv.org/abs/2403.01528)
14. [CCC: Color Classified Colorization](https://arxiv.org/abs/2403.01476)
15. [What Is Missing in Multilingual Visual Reasoning and How to Fix It](https://arxiv.org/abs/2403.01404)
16. [Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective](https://arxiv.org/abs/2403.01373)
17. [LM4OPT: Unveiling the Potential of Large Language Models in Formulating Mathematical Optimization Problems](https://arxiv.org/abs/2403.01342)
18. [AcME-AD: Accelerated Model Explanations for Anomaly Detection](https://arxiv.org/abs/2403.01245)
19. [Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning](https://arxiv.org/abs/2403.01209)
20. [STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2403.01165)
21. [Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models](https://arxiv.org/abs/2403.01101)
22. [Beyond Night Visibility: Adaptive Multi-Scale Fusion of Infrared and Visible Images](https://arxiv.org/abs/2403.01083)
23. [SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/abs/2403.03170)
24. [MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer](https://arxiv.org/abs/2403.02991)
25. [Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception](https://arxiv.org/abs/2403.02969)
26. [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/abs/2403.02910)
27. [PromptKD: Unsupervised Prompt Distillation for Vision-Language Models](https://arxiv.org/abs/2403.02781)
28. [Interactive Continual Learning: Fast and Slow Thinking](https://arxiv.org/abs/2403.02628)
29. [What do we learn from inverting CLIP models?](https://arxiv.org/abs/2403.02580)
30. [Vision-Language Models for Medical Report Generation and Visual Question Answering: A Review](https://arxiv.org/abs/2403.02469)
31. [Bias in Generative AI](https://arxiv.org/abs/2403.02726)
32. [GUIDE: Guidance-based Incremental Learning with Diffusion Models](https://arxiv.org/abs/2403.03938)
33. [Self and Mixed Supervision to Improve Training Labels for Multi-Class Medical Image Segmentation](https://arxiv.org/abs/2403.03882)
34. [Designing Informative Metrics for Few-Shot Example Selection](https://arxiv.org/abs/2403.03861)
35. [X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot Learning Simultaneously in Classification](https://arxiv.org/abs/2403.03863)
36. [ShortGPT: Layers in Large Language Models are More Redundant Than You Expect](https://arxiv.org/abs/2403.03853)
37. [Popeye: A Unified Visual-Language Model for Multi-Source Ship Detection from Remote Sensing Imagery](https://arxiv.org/abs/2403.03790)
38. [Towards Safe and Aligned Large Language Models for Medicine](https://arxiv.org/abs/2403.03744)
39. [The Visual Debugger: Past, Present, and Future](https://arxiv.org/abs/2403.03683)
40. [DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2403.03542)
41. [Dcl-Net: Dual Contrastive Learning Network for Semi-Supervised Multi-Organ Segmentation](https://arxiv.org/abs/2403.03512)
42. [A Teacher-Free Graph Knowledge Distillation Framework with Dual Self-Distillation](https://arxiv.org/abs/2403.03483)
43. [Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing](https://arxiv.org/abs/2403.03431)
44. [Causality-based Cross-Modal Representation Learning for Vision-and-Language Navigation](https://arxiv.org/abs/2403.03405)
45. [Adaptive Discovering and Merging for Incremental Novel Class Discovery](https://arxiv.org/abs/2403.03382)
46. [Enhancing Vision-Language Pre-training with Rich Supervisions](https://arxiv.org/abs/2403.03346)
47. [DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation](https://arxiv.org/abs/2403.03273)
48. [MedMamba: Vision Mamba for Medical Image Classification](https://arxiv.org/abs/2403.03849)
49. [MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training with Masked Autoencoder](https://arxiv.org/abs/2403.04626)
50. [Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification](https://arxiv.org/abs/2403.04024)
51. [iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries](https://arxiv.org/abs/2403.04760)
52. [KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts](https://arxiv.org/abs/2403.04758)
53. [SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM](https://arxiv.org/abs/2403.04735)
54. [PixArt-Î£: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation](https://arxiv.org/abs/2403.04692)
55. [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652)
56. [Context-Based Multimodal Fusion](https://arxiv.org/abs/2403.04650)
57. [CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?](https://arxiv.org/abs/2403.04547)
58. [T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers](https://arxiv.org/abs/2403.04523)
59. [Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2403.04492)
60. [Do Large Language Model Understand Multi-Intent Spoken Language ?](https://arxiv.org/abs/2403.04481)
61. [CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction Tuning](https://arxiv.org/abs/2403.04343)
62. [Discriminative Probing and Tuning for Text-to-Image Generation](https://arxiv.org/abs/2403.04321)
63. [LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking](https://arxiv.org/abs/2403.04303)
64. [Controllable Generation with Text-to-Image Diffusion Models: A Survey](https://arxiv.org/abs/2403.04279)
65. [Large Language Models are In-Context Molecule Learners](https://arxiv.org/abs/2403.04197)
66. [SAM-PD: How Far Can SAM Take Us in Tracking and Segmenting Anything in Videos by Prompt Denoising](https://arxiv.org/abs/2403.04194)
67. [ProMISe: Promptable Medical Image Segmentation using SAM](https://arxiv.org/abs/2403.04164)
68. [DA-Net: A Disentangled and Adaptive Network for Multi-Source Cross-Lingual Transfer Learning](https://arxiv.org/abs/2403.04158)
69. [Dual-path Frequency Discriminators for Few-shot Anomaly Detection](https://arxiv.org/abs/2403.04151)
70. [Contrastive Augmented Graph2Graph Memory Interaction for Few Shot Continual Learning](https://arxiv.org/abs/2403.04140)
71. [An Explainable AI Framework for Artificial Intelligence of Medical Things](https://arxiv.org/abs/2403.04130)
72. [Scalable and Robust Transformer Decoders for Interpretable Image Classification with Foundation Models](https://arxiv.org/abs/2403.04125)
73. [LoDisc: Learning Global-Local Discriminative Features for Self-Supervised Fine-Grained Visual Recognition](https://arxiv.org/abs/2403.04066)