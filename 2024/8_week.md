1. [PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter](https://arxiv.org/abs/2402.10896)
2. [Multi-modal preference alignment remedies regression of visual instruction tuning on language model](https://arxiv.org/abs/2402.10884)
3. [Universal Prompt Optimizer for Safe Text-to-Image Generation](https://arxiv.org/abs/2402.10882)
4. [OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models](https://arxiv.org/abs/2402.10670)
5. [Compact and De-biased Negative Instance Embedding for Multi-Instance Learning on Whole-Slide Image Classification](https://arxiv.org/abs/2402.10595)
6. [Using Left and Right Brains Together: Towards Vision and Language Planning](https://arxiv.org/abs/2402.10534)
7. [Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.10887)
8. [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models](https://arxiv.org/abs/2402.12336)
9. [DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/abs/2402.12289)
10. [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264)
11. [All Language Models Large and Small](https://arxiv.org/abs/2402.12061)
12. [One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation](https://arxiv.org/abs/2402.11909)
13. [SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.11896)
14. [Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before](https://arxiv.org/abs/2402.11816)
15. [Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning](https://arxiv.org/abs/2402.11690)
16. [ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model](https://arxiv.org/abs/2402.11684)
17. [PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM](https://arxiv.org/abs/2402.11585)
18. [Visual In-Context Learning for Large Vision-Language Models](https://arxiv.org/abs/2402.11574)
19. [Visual Concept-driven Image Generation with Text-to-Image Diffusion Model](https://arxiv.org/abs/2402.11487)
20. [Aligning Modalities in Vision Large Language Models via Preference Fine-tuning](https://arxiv.org/abs/2402.11411)
21. [MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal](https://arxiv.org/abs/2402.11297)
22. [OneBit: Towards Extremely Low-bit Large Language Models](https://arxiv.org/abs/2402.11295)
23. [MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning](https://arxiv.org/abs/2402.11260)
24. [HEAL: Brain-inspired Hyperdimensional Efficient Active Learning](https://arxiv.org/abs/2402.11223)
25. [KnowTuning: Knowledge-aware Fine-tuning for Large Language Models](https://arxiv.org/abs/2402.11176)
26. [Knowledge Distillation Based on Transformed Teacher Matching](https://arxiv.org/abs/2402.11148)
27. [Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model](https://arxiv.org/abs/2402.10965)
28. [Text2Data: Low-Resource Data Generation with Textual Control](https://arxiv.org/abs/2402.10941)
29. [A Spatiotemporal Illumination Model for 3D Image Fusion in Optical Coherence Tomography](https://arxiv.org/abs/2402.12114)
30. [BiMediX: Bilingual Medical Mixture of Experts LLM](https://arxiv.org/abs/2402.13253)
31. [A Touch, Vision, and Language Dataset for Multimodal Alignment](https://arxiv.org/abs/2402.13232)
32. [A Survey on Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2402.13116)
33. [Slot-VLM: SlowFast Slots for Video-Language Modeling](https://arxiv.org/abs/2402.13088)
34. [UniCell: Universal Cell Nucleus Classification via Prompt Learning](https://arxiv.org/abs/2402.12938)
35. [Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation](https://arxiv.org/abs/2402.12862)
36. [Instruction-tuned Language Models are Better Knowledge Learners](https://arxiv.org/abs/2402.12847)
37. [Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?](https://arxiv.org/abs/2402.12819)
38. [A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis](https://arxiv.org/abs/2402.12760)
39. [Me LLaMA: Foundation Large Language Models for Medical Applications](https://arxiv.org/abs/2402.12749)
40. [CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning](https://arxiv.org/abs/2402.12736)
41. [HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts](https://arxiv.org/abs/2402.12656)
42. [Efficient Parameter Mining and Freezing for Continual Object Detection](https://arxiv.org/abs/2402.12624)
43. [Towards Cross-Domain Continual Learning](https://arxiv.org/abs/2402.12490)
44. [The (R)Evolution of Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2402.12451)
45. [Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference](https://arxiv.org/abs/2402.12735)