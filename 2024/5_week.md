1. [From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities](https://arxiv.org/abs/2401.15071)
2. [Masked Pre-trained Model Enables Universal Zero-shot Denoiser](https://arxiv.org/abs/2401.14966)
3. [Memory-Inspired Temporal Prompt Interaction for Text-Image Classification](https://arxiv.org/abs/2401.14856)
4. [TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts](https://arxiv.org/abs/2401.14828)
5. [PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2401.14807)
6. [SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation](https://arxiv.org/abs/2401.14686)
7. [Revisiting Active Learning in the Era of Vision Foundation Models](https://arxiv.org/abs/2401.14555)
8. [K-QA: A Real-World Medical Q&A Benchmark](https://arxiv.org/abs/2401.14493)
9. [Strategic Usage in a Multi-Learner Setting](https://arxiv.org/abs/2401.16422)
10. [InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model](https://arxiv.org/abs/2401.16420)
11. [Scaling Sparse Fine-Tuning to Large Language Models](https://arxiv.org/abs/2401.16405)
12. [Continual Learning with Pre-Trained Models: A Survey](https://arxiv.org/abs/2401.16386)
13. [Spatial-Aware Latent Initialization for Controllable Image Generation](https://arxiv.org/abs/2401.16157)
14. [Sample Weight Estimation Using Meta-Updates for Online Continual Learning](https://arxiv.org/abs/2401.15973)
15. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2401.15947)
16. [Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization](https://arxiv.org/abs/2401.15914)
17. [Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing](https://arxiv.org/abs/2401.15855)
18. [Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes](https://arxiv.org/abs/2401.15834)
19. [Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation](https://arxiv.org/abs/2401.15688)
20. [Data-Free Generalized Zero-Shot Learning](https://arxiv.org/abs/2401.15657)
21. [IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models](https://arxiv.org/abs/2401.15559)
22. [Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks](https://arxiv.org/abs/2401.15275)
23. [HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy](https://arxiv.org/abs/2401.15207)
24. [Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning](https://arxiv.org/abs/2401.15111)
25. [YOLO-World: Real-Time Open-Vocabulary Object Detection](https://arxiv.org/abs/2401.17270)
26. [ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models](https://arxiv.org/abs/2401.17267)
27. [Weaver: Foundation Models for Creative Writing](https://arxiv.org/abs/2401.17268)
28. [Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning](https://arxiv.org/abs/2401.17186)
29. [ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained Visual Categorization](https://arxiv.org/abs/2401.17050)
30. [EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain](https://arxiv.org/abs/2401.16822)
31. [Pick-and-Draw: Training-free Semantic Guidance for Text-to-Image Personalization](https://arxiv.org/abs/2401.16762)
32. [MESA: Matching Everything by Segmenting Anything](https://arxiv.org/abs/2401.16741)
33. [Depth Anything in Medical Images: A Comparative Study](https://arxiv.org/abs/2401.16600)
34. [Binding Touch to Everything: Learning Unified Multimodal Tactile Representations](https://arxiv.org/abs/2401.18084)
35. [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://arxiv.org/abs/2401.18070)
36. [Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models](https://arxiv.org/abs/2401.18034)
37. [Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation](https://arxiv.org/abs/2401.17904)
38. [Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://arxiv.org/abs/2401.17868)
39. [Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2401.17828)
40. [Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation](https://arxiv.org/abs/2401.17664)
41. [All Beings Are Equal in Open Set Recognition](https://arxiv.org/abs/2401.17654)
42. [Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning](https://arxiv.org/abs/2401.17602)
43. [Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data](https://arxiv.org/abs/2401.17600)
44. [Local Feature Matching Using Deep Learning: A Survey](https://arxiv.org/abs/2401.17592)
45. [Data-Effective Learning: A Comprehensive Medical Benchmark](https://arxiv.org/abs/2401.17542)
46. [Customizing Language Model Responses with Contrastive In-Context Learning](https://arxiv.org/abs/2401.17390)