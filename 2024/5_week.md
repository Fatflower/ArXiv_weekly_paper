1. [From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities](https://arxiv.org/abs/2401.15071)
2. [Masked Pre-trained Model Enables Universal Zero-shot Denoiser](https://arxiv.org/abs/2401.14966)
3. [Memory-Inspired Temporal Prompt Interaction for Text-Image Classification](https://arxiv.org/abs/2401.14856)
4. [TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts](https://arxiv.org/abs/2401.14828)
5. [PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2401.14807)
6. [SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation](https://arxiv.org/abs/2401.14686)
7. [Revisiting Active Learning in the Era of Vision Foundation Models](https://arxiv.org/abs/2401.14555)
8. [K-QA: A Real-World Medical Q&A Benchmark](https://arxiv.org/abs/2401.14493)
9. [Strategic Usage in a Multi-Learner Setting](https://arxiv.org/abs/2401.16422)
10. [InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model](https://arxiv.org/abs/2401.16420)
11. [Scaling Sparse Fine-Tuning to Large Language Models](https://arxiv.org/abs/2401.16405)
12. [Continual Learning with Pre-Trained Models: A Survey](https://arxiv.org/abs/2401.16386)
13. [Spatial-Aware Latent Initialization for Controllable Image Generation](https://arxiv.org/abs/2401.16157)
14. [Sample Weight Estimation Using Meta-Updates for Online Continual Learning](https://arxiv.org/abs/2401.15973)
15. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2401.15947)
16. [Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization](https://arxiv.org/abs/2401.15914)
17. [Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing](https://arxiv.org/abs/2401.15855)
18. [Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes](https://arxiv.org/abs/2401.15834)
19. [Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation](https://arxiv.org/abs/2401.15688)
20. [Data-Free Generalized Zero-Shot Learning](https://arxiv.org/abs/2401.15657)
21. [IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models](https://arxiv.org/abs/2401.15559)
22. [Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks](https://arxiv.org/abs/2401.15275)
23. [HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy](https://arxiv.org/abs/2401.15207)
24. [Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning](https://arxiv.org/abs/2401.15111)