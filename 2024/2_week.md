1. [PALP: Prompt Aligned Personalization of Text-to-Image Models](https://arxiv.org/abs/2401.06105)
2. [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/abs/2401.06209)
3. [Learning to Prompt Segment Anything Models](https://arxiv.org/abs/2401.04651)
4. [MOODv2: Masked Image Modeling for Out-of-Distribution Detection](https://arxiv.org/abs/2401.02611)
5. [SeTformer is What You Need for Vision and Language](https://arxiv.org/abs/2401.03540)
6. [Fully Attentional Networks with Self-emerging Token Labeling](https://arxiv.org/abs/2401.03844)
7. [MS-DETR: Efficient DETR Training with Mixed Supervision](https://arxiv.org/abs/2401.03989)
8. [GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance](https://arxiv.org/abs/2401.00260)
9. [BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model](https://arxiv.org/abs/2401.02317)
10. [Large Language Models as Visual Cross-Domain Learners](https://arxiv.org/abs/2401.03253)
11. [Less is More : A Closer Look at Multi-Modal Few-Shot Learning](https://arxiv.org/abs/2401.05010)