https://arxiv.org/abs/2405.12981 | [2405.12981] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention
https://arxiv.org/abs/2405.12970 | [2405.12970] Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control
https://arxiv.org/abs/2405.12914 | [2405.12914] An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation
https://arxiv.org/abs/2405.12819 | [2405.12819] Large Language Models Meet NLP: A Survey
https://arxiv.org/abs/2405.12752 | [2405.12752] C3L: Content Correlated Vision-Language Instruction Tuning Data Generation via Contrastive Learning
https://arxiv.org/abs/2405.12328 | [2405.12328] Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation
https://arxiv.org/abs/2405.12609 | [2405.12609] Mamba in Speech: Towards an Alternative to Self-Attention
https://arxiv.org/abs/2405.12872 | [2405.12872] Spatial-aware Attention Generative Adversarial Network for Semi-supervised Anomaly Detection in Medical Image
https://arxiv.org/abs/2405.12318 | [2405.12318] Hierarchical SegNet with Channel and Context Attention for Accurate Lung Segmentation in Chest X-ray Images