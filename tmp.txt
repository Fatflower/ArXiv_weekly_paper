https://arxiv.org/abs/2404.01294 | [2404.01294] CosmicMan: A Text-to-Image Foundation Model for Humans
https://arxiv.org/abs/2404.01291 | [2404.01291] Evaluating Text-to-Visual Generation with Image-to-Text Generation
https://arxiv.org/abs/2404.01282 | [2404.01282] LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization
https://arxiv.org/abs/2404.01260 | [2404.01260] Bridging Remote Sensors with Multisensor Geospatial Foundation Models
https://arxiv.org/abs/2404.01232 | [2404.01232] Open-Vocabulary Federated Learning with Multimodal Prototyping
https://arxiv.org/abs/2404.01179 | [2404.01179] BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning
https://arxiv.org/abs/2404.01127 | [2404.01127] Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation
https://arxiv.org/abs/2404.01077 | [2404.01077] Efficient Prompting Methods for Large Language Models: A Survey
https://arxiv.org/abs/2404.01065 | [2404.01065] T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation
https://arxiv.org/abs/2404.00983 | [2404.00983] Continual Learning for Smart City: A Survey
https://arxiv.org/abs/2404.00949 | [2404.00949] Harnessing The Power of Attention For Patch-Based Biomedical Image Classification
https://arxiv.org/abs/2404.00936 | [2404.00936] A Comprehensive Review of Knowledge Distillation in Computer Vision
https://arxiv.org/abs/2404.00918 | [2404.00918] Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation
https://arxiv.org/abs/2404.00901 | [2404.00901] Slightly Shift New Classes to Remember Old Classes for Video Class-Incremental Learning
https://arxiv.org/abs/2404.00860 | [2404.00860] Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance
https://arxiv.org/abs/2404.00790 | [2404.00790] Rehearsal-Free Modular and Compositional Continual Learning for Language Models
https://arxiv.org/abs/2404.00781 | [2404.00781] Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning
https://arxiv.org/abs/2404.00675 | [2404.00675] LLM meets Vision-Language Models for Zero-Shot One-Class Classification
https://arxiv.org/abs/2404.00650 | [2404.00650] Deep Instruction Tuning for Segment Anything Model
https://arxiv.org/abs/2404.00648 | [2404.00648] SpiralMLP: A Lightweight Vision MLP Architecture
https://arxiv.org/abs/2404.00603 | [2404.00603] Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning
https://arxiv.org/abs/2404.00532 | [2404.00532] LLMs are Good Action Recognizers
https://arxiv.org/abs/2404.00417 | [2404.00417] Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation
https://arxiv.org/abs/2404.00384 | [2404.00384] TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias
https://arxiv.org/abs/2404.00376 | [2404.00376] Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks
https://arxiv.org/abs/2404.00380 | [2404.00380] DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation
https://arxiv.org/abs/2404.00323 | [2404.00323] CLIP-driven Outliers Synthesis for few-shot OOD detection
https://arxiv.org/abs/2404.00312 | [2404.00312] Bayesian Exploration of Pre-trained Models for Low-shot Image Classification
https://arxiv.org/abs/2404.00264 | [2404.00264] DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation
https://arxiv.org/abs/2404.00262 | [2404.00262] Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation
https://arxiv.org/abs/2404.00228 | [2404.00228] InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
https://arxiv.org/abs/2404.00216 | [2404.00216] Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark
https://arxiv.org/abs/2404.00122 | [2404.00122] AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation
https://arxiv.org/abs/2404.01102 | [2404.01102] Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation
https://arxiv.org/abs/2403.20330 | [2403.20330] Are We on the Right Way for Evaluating Large Vision-Language Models?
https://arxiv.org/abs/2403.20320 | [2403.20320] MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning
https://arxiv.org/abs/2403.20317 | [2403.20317] Convolutional Prompting meets Language Models for Continual Learning
https://arxiv.org/abs/2403.20312 | [2403.20312] Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations
https://arxiv.org/abs/2403.20306 | [2403.20306] Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference
https://arxiv.org/abs/2403.20288 | [2403.20288] Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain
https://arxiv.org/abs/2403.20284 | [2403.20284] LayerNorm: A key component in parameter-efficient fine-tuning
https://arxiv.org/abs/2403.20271 | [2403.20271] Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want
https://arxiv.org/abs/2403.20253 | [2403.20253] MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation
https://arxiv.org/abs/2403.20194 | [2403.20194] ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models
https://arxiv.org/abs/2403.20158 | [2403.20158] ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models
https://arxiv.org/abs/2403.20126 | [2403.20126] ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning
https://arxiv.org/abs/2403.20105 | [2403.20105] FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models
https://arxiv.org/abs/2403.20103 | [2403.20103] NLP for Counterspeech against Hate: A Survey and How-To Guide
https://arxiv.org/abs/2403.20086 | [2403.20086] Selective Attention-based Modulation for Continual Learning
https://arxiv.org/abs/2403.20078 | [2403.20078] Negative Label Guided OOD Detection with Pretrained Vision-Language Models
https://arxiv.org/abs/2403.19979 | [2403.19979] Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer
https://arxiv.org/abs/2403.19950 | [2403.19950] Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data
https://arxiv.org/abs/2403.19949 | [2403.19949] FairCLIP: Harnessing Fairness in Vision-Language Learning
https://arxiv.org/abs/2403.19895 | [2403.19895] An Information-Theoretic Framework for Out-of-Distribution Generalization
https://arxiv.org/abs/2403.19887 | [2403.19887] Jamba: A Hybrid Transformer-Mamba Language Model
https://arxiv.org/abs/2403.19826 | [2403.19826] Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation
https://arxiv.org/abs/2403.19792 | [2403.19792] MAPL: Model Agnostic Peer-to-peer Learning
https://arxiv.org/abs/2403.19776 | [2403.19776] CLoRA: A Contrastive Approach to Compose Multiple LoRA Models
https://arxiv.org/abs/2403.19754 | [2403.19754] GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation
https://arxiv.org/abs/2403.19669 | [2403.19669] Analyzing the Roles of Language and Vision in Learning from Limited Data
https://arxiv.org/abs/2403.20035 | [2403.20035] UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation
https://arxiv.org/abs/2403.19880 | [2403.19880] Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks