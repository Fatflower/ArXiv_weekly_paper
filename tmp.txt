https://arxiv.org/abs/2407.19996 | [2407.19996] Reproducibility Study of "ITI-GEN: Inclusive Text-to-Image Generation"
https://arxiv.org/abs/2407.19987 | [2407.19987] HOBOTAN: Efficient Higher Order Binary Optimization Solver with Tensor Networks and PyTorch
https://arxiv.org/abs/2407.19985 | [2407.19985] Mixture of Nested Experts: Adaptive Processing of Visual Tokens
https://arxiv.org/abs/2407.19889 | [2407.19889] Self-Supervised Learning for Text Recognition: A Critical Survey
https://arxiv.org/abs/2407.19888 | [2407.19888] Yucca: A Deep Learning Framework For Medical Image Analysis
https://arxiv.org/abs/2407.19849 | [2407.19849] Normality Addition via Normality Detection in Industrial Image Anomaly Detection Models
https://arxiv.org/abs/2407.19832 | [2407.19832] ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2
https://arxiv.org/abs/2407.19807 | [2407.19807] Cool-Fusion: Fuse Large Language Models without Training
https://arxiv.org/abs/2407.19795 | [2407.19795] VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks
https://arxiv.org/abs/2407.19778 | [2407.19778] Multimodal Large Language Models for Bioimage Analysis
https://arxiv.org/abs/2407.19679 | [2407.19679] Harnessing Large Vision and Language Models in Agriculture: A Review
https://arxiv.org/abs/2407.19651 | [2407.19651] ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck
https://arxiv.org/abs/2407.19564 | [2407.19564] Forecast-PEFT: Parameter-Efficient Fine-Tuning for Pre-trained Motion Forecasting Models
https://arxiv.org/abs/2407.19546 | [2407.19546] XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image Pre-Training
https://arxiv.org/abs/2407.19512 | [2407.19512] Large-scale cervical precancerous screening via AI-assisted cytology whole slide image analysis
https://arxiv.org/abs/2407.19474 | [2407.19474] Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models
https://arxiv.org/abs/2407.19409 | [2407.19409] LLAVADI: What Matters For Multimodal Large Language Models Distillation
https://arxiv.org/abs/2407.19205 | [2407.19205] Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's Impact on Spatio-Temporal Cross-Attentions
https://arxiv.org/abs/2407.19185 | [2407.19185] LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models
https://arxiv.org/abs/2407.18970 | [2407.18970] Region Guided Attention Network for Retinal Vessel Segmentation
https://arxiv.org/abs/2407.18920 | [2407.18920] Optimising Hard Prompts with Few-Shot Meta-Prompting
https://arxiv.org/abs/2407.20171 | [2407.20171] Diffusion Feedback Helps CLIP See Better