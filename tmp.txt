https://arxiv.org/abs/2408.15997 | [2408.15997] Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need
https://arxiv.org/abs/2408.15998 | [2408.15998] Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders
https://arxiv.org/abs/2408.15980 | [2408.15980] In-Context Imitation Learning via Next-Token Prediction
https://arxiv.org/abs/2408.15924 | [2408.15924] Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning
https://arxiv.org/abs/2408.15881 | [2408.15881] LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation
https://arxiv.org/abs/2408.15868 | [2408.15868] GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model
https://arxiv.org/abs/2408.15796 | [2408.15796] Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models
https://arxiv.org/abs/2408.15769 | [2408.15769] A Survey on Evaluation of Multimodal Large Language Models
https://arxiv.org/abs/2408.15740 | [2408.15740] MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms
https://arxiv.org/abs/2408.15664 | [2408.15664] Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts
https://arxiv.org/abs/2408.15657 | [2408.15657] TeFF: Tracking-enhanced Forgetting-free Few-shot 3D LiDAR Semantic Segmentation
https://arxiv.org/abs/2408.15641 | [2408.15641] MMDRFuse: Distilled Mini-Model with Dynamic Refresh for Multi-Modality Image Fusion
https://arxiv.org/abs/2408.15580 | [2408.15580] Hierarchical Visual Categories Modeling: A Joint Representation Learning and Density Estimation Framework for Out-of-Distribution Detection
https://arxiv.org/abs/2408.15566 | [2408.15566] TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning
https://arxiv.org/abs/2408.15545 | [2408.15545] SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding
https://arxiv.org/abs/2408.16769 | [2408.16769] PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning
https://arxiv.org/abs/2408.16768 | [2408.16768] SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners
https://arxiv.org/abs/2408.16766 | [2408.16766] CSGO: Content-Style Composition in Text-to-Image Generation
https://arxiv.org/abs/2408.16737 | [2408.16737] Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling
https://arxiv.org/abs/2408.16729 | [2408.16729] Prediction-Feedback DETR for Temporal Action Detection
https://arxiv.org/abs/2408.16634 | [2408.16634] RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model
https://arxiv.org/abs/2408.16577 | [2408.16577] Seeking the Sufficiency and Necessity Causal Features in Multimodal Representation Learning
https://arxiv.org/abs/2408.16486 | [2408.16486] Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning
https://arxiv.org/abs/2408.16425 | [2408.16425] A Comparative Study of Hyperparameter Tuning Methods
https://arxiv.org/abs/2408.16236 | [2408.16236] Neural Spectral Decomposition for Dataset Distillation
https://arxiv.org/abs/2408.16224 | [2408.16224] LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models
https://arxiv.org/abs/2408.16219 | [2408.16219] Training-free Video Temporal Grounding using Large-scale Pre-trained Models
https://arxiv.org/abs/2408.16195 | [2408.16195] DLM-VMTL:A Double Layer Mapper for heterogeneous data video Multi-task prompt learning
https://arxiv.org/abs/2408.16176 | [2408.16176] VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images
https://arxiv.org/abs/2408.16130 | [2408.16130] Using Backbone Foundation Model for Evaluating Fairness in Chest Radiography Without Demographic Data
https://arxiv.org/abs/2408.16189 | [2408.16189] A More Unified Theory of Transfer Learning