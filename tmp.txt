https://arxiv.org/abs/2402.04253 | [2402.04253] AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls
https://arxiv.org/abs/2402.04252 | [2402.04252] EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters
https://arxiv.org/abs/2402.04248 | [2402.04248] Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks
https://arxiv.org/abs/2402.04177 | [2402.04177] Scaling Laws for Downstream Task Performance of Large Language Models
https://arxiv.org/abs/2402.04139 | [2402.04139] U-shaped Vision Mamba for Single Image Dehazing
https://arxiv.org/abs/2402.04087 | [2402.04087] A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation
https://arxiv.org/abs/2402.04009 | [2402.04009] Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning
https://arxiv.org/abs/2402.03917 | [2402.03917] Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning
https://arxiv.org/abs/2402.03846 | [2402.03846] Efficient Generation of Hidden Outliers for Improved Outlier Detection
https://arxiv.org/abs/2402.03795 | [2402.03795] Energy-based Domain-Adaptive Segmentation with Depth Guidance
https://arxiv.org/abs/2402.03785 | [2402.03785] Weakly Supervised Anomaly Detection via Knowledge-Data Alignment
https://arxiv.org/abs/2402.03782 | [2402.03782] Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More
https://arxiv.org/abs/2402.03754 | [2402.03754] Intensive Vision-guided Network for Radiology Report Generation
https://arxiv.org/abs/2402.03752 | [2402.03752] Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images
https://arxiv.org/abs/2402.03749 | [2402.03749] Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models
https://arxiv.org/abs/2402.03708 | [2402.03708] SISP: A Benchmark Dataset for Fine-grained Ship Instance Segmentation in Panchromatic Satellite Images
https://arxiv.org/abs/2402.03700 | [2402.03700] GenLens: A Systematic Evaluation of Visual GenAI Model Outputs
https://arxiv.org/abs/2402.03660 | [2402.03660] Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
https://arxiv.org/abs/2402.03631 | [2402.03631] CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model
https://arxiv.org/abs/2402.03627 | [2402.03627] Partially Recentralization Softmax Loss for Vision-Language Models Robustness
https://arxiv.org/abs/2402.03592 | [2402.03592] GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation
https://arxiv.org/abs/2402.03570 | [2402.03570] Diffusion World Model
https://arxiv.org/abs/2402.03502 | [2402.03502] How Does Unlabeled Data Provably Help Out-of-Distribution Detection?
https://arxiv.org/abs/2305.12423 | [2305.12423] Task-Oriented Communication with Out-of-Distribution Detection: An Information Bottleneck Framework
https://arxiv.org/abs/2402.03328 | [2402.03328] Large-scale Generative AI Models Lack Visual Number Sense
https://arxiv.org/abs/2402.03526 | [2402.03526] nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model
https://arxiv.org/abs/2402.03558 | [2402.03558] Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?