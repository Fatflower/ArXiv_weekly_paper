https://arxiv.org/abs/2409.14021 | [2409.14021] BrainDreamer: Reasoning-Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance
https://arxiv.org/abs/2409.13999 | [2409.13999] Multiple-Exit Tuning: Towards Inference-Efficient Adaptation for Vision Transformer
https://arxiv.org/abs/2409.13983 | [2409.13983] Enhanced Semantic Segmentation for Large-Scale and Imbalanced Point Clouds
https://arxiv.org/abs/2409.13984 | [2409.13984] Cycle-Consistency Uncertainty Estimation for Visual Prompting based One-Shot Defect Segmentation
https://arxiv.org/abs/2409.13728 | [2409.13728] Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts
https://arxiv.org/abs/2409.14874 | [2409.14874] Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images
https://arxiv.org/abs/2409.15028 | [2409.15028] Region Mixup
https://arxiv.org/abs/2409.14993 | [2409.14993] Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond
https://arxiv.org/abs/2409.14983 | [2409.14983] Dynamic Integration of Task-Specific Adapters for Class Incremental Learning
https://arxiv.org/abs/2409.14846 | [2409.14846] A-VL: Adaptive Attention for Large Vision-Language Models
https://arxiv.org/abs/2409.14759 | [2409.14759] VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models
https://arxiv.org/abs/2409.14703 | [2409.14703] MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification
https://arxiv.org/abs/2409.14607 | [2409.14607] Patch Ranking: Efficient CLIP by Learning to Rank Local Patches
https://arxiv.org/abs/2409.14195 | [2409.14195] The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends
https://arxiv.org/abs/2409.15278 | [2409.15278] PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions
https://arxiv.org/abs/2409.15277 | [2409.15277] A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?
https://arxiv.org/abs/2409.15250 | [2409.15250] ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models