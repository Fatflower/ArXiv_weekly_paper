https://arxiv.org/abs/2405.02266 | [2405.02266] On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?
https://arxiv.org/abs/2405.02246 | [2405.02246] What matters when building vision-language models?
https://arxiv.org/abs/2405.02179 | [2405.02179] Training-Free Deepfake Voice Recognition by Leveraging Large-Scale Pre-Trained Models
https://arxiv.org/abs/2405.02155 | [2405.02155] Multi-method Integration with Confidence-based Weighting for Zero-shot Image Classification
https://arxiv.org/abs/2405.02165 | [2405.02165] EEG2TEXT: Open Vocabulary EEG-to-Text Decoding with EEG Pre-Training and Multi-View Transformer
https://arxiv.org/abs/2405.02144 | [2405.02144] MedReadMe: A Systematic Study for Fine-grained Sentence Readability in Medical Domain
https://arxiv.org/abs/2405.02024 | [2405.02024] Analyzing Narrative Processing in Large Language Models (LLMs): Using GPT4 to test BERT
https://arxiv.org/abs/2405.02010 | [2405.02010] The Trade-off between Performance, Efficiency, and Fairness in Adapter Modules for Text Classification
https://arxiv.org/abs/2405.01885 | [2405.01885] Enhancing Micro Gesture Recognition for Emotion Understanding via Context-aware Visual-Text Contrastive Learning
https://arxiv.org/abs/2405.01825 | [2405.01825] Improving Concept Alignment in Vision-Language Concept Bottleneck Models
https://arxiv.org/abs/2405.01769 | [2405.01769] A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law
https://arxiv.org/abs/2405.01725 | [2405.01725] Development of Skip Connection in Deep Neural Networks for Computer Vision and Medical Image Analysis: A Survey
https://arxiv.org/abs/2405.02162 | [2405.02162] Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models