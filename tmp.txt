https://arxiv.org/abs/2406.00209 | [2406.00209] Mamba State-Space Models Can Be Strong Downstream Learners
https://arxiv.org/abs/2406.00146 | [2406.00146] A Survey of Deep Learning Audio Generation Methods
https://arxiv.org/abs/2406.00134 | [2406.00134] Anomaly Detection in Dynamic Graphs: A Comprehensive Survey
https://arxiv.org/abs/2406.00053 | [2406.00053] Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting
https://arxiv.org/abs/2406.00023 | [2406.00023] LocMoE+: Enhanced Router with Token Feature Awareness for Efficient LLM Pre-Training
https://arxiv.org/abs/2406.00667 | [2406.00667] An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging
https://arxiv.org/abs/2406.00449 | [2406.00449] Dual Hyperspectral Mamba for Efficient Spectral Compressive Imaging
https://arxiv.org/abs/2406.00279 | [2406.00279] Hybrid attention structure preserving network for reconstruction of under-sampled OCT images
https://arxiv.org/abs/2406.00237 | [2406.00237] A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases
https://arxiv.org/abs/2405.21070 | [2405.21070] Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights
https://arxiv.org/abs/2405.21050 | [2405.21050] Spectrum-Aware Parameter Efficient Fine-Tuning for Diffusion Models
https://arxiv.org/abs/2405.21036 | [2405.21036] A-PETE: Adaptive Prototype Explanations of Tree Ensembles
https://arxiv.org/abs/2405.20991 | [2405.20991] Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models
https://arxiv.org/abs/2405.20797 | [2405.20797] Ovis: Structural Embedding Alignment for Multimodal Large Language Model
https://arxiv.org/abs/2405.20759 | [2405.20759] Information Theoretic Text-to-Image Alignment
https://arxiv.org/abs/2405.20735 | [2405.20735] Language Augmentation in CLIP for Improved Anatomy Detection on Multi-modal Medical Images
https://arxiv.org/abs/2405.20650 | [2405.20650] GenMix: Combining Generative and Mixture Data Augmentation for Medical Image Classification
https://arxiv.org/abs/2405.20646 | [2405.20646] Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item
https://arxiv.org/abs/2405.20620 | [2405.20620] "Forgetting" in Machine Learning and Beyond: A Survey
https://arxiv.org/abs/2405.20607 | [2405.20607] Textual Inversion and Self-supervised Refinement for Radiology Report Generation
https://arxiv.org/abs/2406.00663 | [2406.00663] SimSAM: Zero-shot Medical Image Segmentation via Simulated Interaction
https://arxiv.org/abs/2406.00644 | [2406.00644] Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance
https://arxiv.org/abs/2406.00628 | [2406.00628] Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models
https://arxiv.org/abs/2406.00606 | [2406.00606] LLMs Could Autonomously Learn Without External Supervision
https://arxiv.org/abs/2406.00545 | [2406.00545] Memory-guided Network with Uncertainty-based Feature Augmentation for Few-shot Semantic Segmentation
https://arxiv.org/abs/2406.00532 | [2406.00532] Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques
https://arxiv.org/abs/2406.00529 | [2406.00529] On the Use of Anchoring for Training Vision Models
https://arxiv.org/abs/2406.00510 | [2406.00510] Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection
https://arxiv.org/abs/2406.00410 | [2406.00410] Posterior Label Smoothing for Node Classification
https://arxiv.org/abs/2406.00345 | [2406.00345] DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection
https://arxiv.org/abs/2406.02609 | [2406.02609] Less is More: Pseudo-Label Filtering for Continual Test-Time Adaptation
https://arxiv.org/abs/2406.03430 | [2406.03430] Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis
https://arxiv.org/abs/2406.03173 | [2406.03173] Multi-Task Multi-Scale Contrastive Knowledge Distillation for Efficient Medical Image Segmentation
https://arxiv.org/abs/2406.02547 | [2406.02547] Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning
https://arxiv.org/abs/2406.02543 | [2406.02543] To Believe or Not to Believe Your LLM
https://arxiv.org/abs/2406.02539 | [2406.02539] Parrot: Multilingual Visual Instruction Tuning
https://arxiv.org/abs/2406.02435 | [2406.02435] Generative Active Learning for Long-tailed Instance Segmentation
https://arxiv.org/abs/2406.02428 | [2406.02428] Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning
https://arxiv.org/abs/2406.02347 | [2406.02347] Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation
https://arxiv.org/abs/2406.02327 | [2406.02327] Continual Unsupervised Out-of-Distribution Detection
https://arxiv.org/abs/2406.02202 | [2406.02202] Can CLIP help CLIP in learning 3D?
https://arxiv.org/abs/2406.01956 | [2406.01956] Enhance Image-to-Image Generation with LLaVA Prompt and Negative Prompt
https://arxiv.org/abs/2406.01604 | [2406.01604] An Empirical Study of Excitation and Aggregation Design Adaptions in CLIP4Clip for Video-Text Retrieval
https://arxiv.org/abs/2406.02480 | [2406.02480] Fairness Evolution in Continual Learning for Medical Imaging
https://arxiv.org/abs/2406.01583 | [2406.01583] Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP
https://arxiv.org/abs/2406.01561 | [2406.01561] Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation
https://arxiv.org/abs/2406.01432 | [2406.01432] ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models
https://arxiv.org/abs/2406.01264 | [2406.01264] FreeTumor: Advance Tumor Segmentation via Large-Scale Tumor Synthesis
https://arxiv.org/abs/2406.01210 | [2406.01210] GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer
https://arxiv.org/abs/2406.01203 | [2406.01203] Scaling Up Deep Clustering Methods Beyond ImageNet-1K
https://arxiv.org/abs/2406.01170 | [2406.01170] Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure
https://arxiv.org/abs/2406.00985 | [2406.00985] MultiEdits: Simultaneous Multi-Aspect Editing with Text-to-Image Diffusion Models
https://arxiv.org/abs/2406.00958 | [2406.00958] Navigating Conflicting Views: Harnessing Trust for Learning
https://arxiv.org/abs/2406.00956 | [2406.00956] Improving Segment Anything on the Fly: Auxiliary Online Learning and Adaptive Fusion for Medical Image Segmentation
https://arxiv.org/abs/2406.00936 | [2406.00936] A Survey of Useful LLM Evaluation
https://arxiv.org/abs/2406.00806 | [2406.00806] Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection
https://arxiv.org/abs/2406.00672 | [2406.00672] Task-oriented Embedding Counts: Heuristic Clustering-driven Feature Fine-tuning for Whole Slide Image Classification
https://arxiv.org/abs/2406.00670 | [2406.00670] Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation
https://arxiv.org/abs/2406.03150 | [2406.03150] Sample-specific Masks for Visual Reprogramming-based Prompting
https://arxiv.org/abs/2406.03146 | [2406.03146] Tiny models from tiny data: Textual and null-text inversion for few-shot distillation
https://arxiv.org/abs/2406.03140 | [2406.03140] Continual Traffic Forecasting via Mixture of Experts
https://arxiv.org/abs/2406.03065 | [2406.03065] Decision Boundary-aware Knowledge Consolidation Generates Better Instance-Incremental Learner
https://arxiv.org/abs/2406.02915 | [2406.02915] Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models
https://arxiv.org/abs/2406.03216 | [2406.03216] Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need
https://arxiv.org/abs/2406.03411 | [2406.03411] Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach
https://arxiv.org/abs/2406.03396 | [2406.03396] Noisy Data Visualization using Functional Data Analysis
https://arxiv.org/abs/2406.03303 | [2406.03303] Learning Visual Prompts for Guiding the Attention of Vision Transformers
https://arxiv.org/abs/2406.04344 | [2406.04344] Verbalized Machine Learning: Revisiting Machine Learning with Language Models
https://arxiv.org/abs/2406.04332 | [2406.04332] Coarse-To-Fine Tensor Trains for Compact Visual Representations
https://arxiv.org/abs/2406.04328 | [2406.04328] The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning
https://arxiv.org/abs/2406.04306 | [2406.04306] Semantically Diverse Language Generation for Uncertainty Estimation in Language Models
https://arxiv.org/abs/2406.04303 | [2406.04303] Vision-LSTM: xLSTM as Generic Vision Backbone
https://arxiv.org/abs/2406.04284 | [2406.04284] What is Dataset Distillation Learning?
https://arxiv.org/abs/2406.04276 | [2406.04276] Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks
https://arxiv.org/abs/2406.04221 | [2406.04221] Matching Anything by Segmenting Anything
https://arxiv.org/abs/2406.04207 | [2406.04207] CDMamba: Remote Sensing Image Change Detection with Mamba
https://arxiv.org/abs/2406.04032 | [2406.04032] Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis
https://arxiv.org/abs/2406.04031 | [2406.04031] Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt
https://arxiv.org/abs/2406.03917 | [2406.03917] Frequency-based Matcher for Long-tailed Semantic Segmentation
https://arxiv.org/abs/2406.03872 | [2406.03872] BLSP-Emo: Towards Empathetic Large Speech-Language Models
https://arxiv.org/abs/2406.03793 | [2406.03793] Low-Rank Similarity Mining for Multimodal Dataset Distillation
https://arxiv.org/abs/2406.03792 | [2406.03792] Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning
https://arxiv.org/abs/2406.03730 | [2406.03730] FastGAS: Fast Graph-based Annotation Selection for In-Context Learning
https://arxiv.org/abs/2406.03642 | [2406.03642] Is Free Self-Alignment Possible?
https://arxiv.org/abs/2406.03586 | [2406.03586] CountCLIP -- [Re] Teaching CLIP to Count to Ten
https://arxiv.org/abs/2405.19334 | [2405.19334] LLMs Meet Multimodal Generation and Editing: A Survey
https://arxiv.org/abs/2406.03902 | [2406.03902] C^2RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction
https://arxiv.org/abs/2406.03628 | [2406.03628] Synthetic Oversampling: Theory and A Practical Approach Using LLMs to Address Data Imbalance
https://arxiv.org/abs/2406.03496 | [2406.03496] Wings: Learning Multimodal LLMs without Text-only Forgetting
https://arxiv.org/abs/2406.03459 | [2406.03459] LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection