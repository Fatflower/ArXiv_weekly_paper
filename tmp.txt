https://arxiv.org/abs/2404.13033 | [2404.13033] Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs
https://arxiv.org/abs/2404.13039 | [2404.13039] LaPA: Latent Prompt Assist Model For Medical Visual Question Answering
https://arxiv.org/abs/2404.13046 | [2404.13046] MoVA: Adapting Mixture of Vision Experts to Multimodal Context
https://arxiv.org/abs/2404.12948 | [2404.12948] Next Generation Loss Function for Image Classification
https://arxiv.org/abs/2404.12922 | [2404.12922] Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images
https://arxiv.org/abs/2404.12908 | [2404.12908] Robust CLIP-Based Detector for Exposing Diffusion Model-Generated Images
https://arxiv.org/abs/2404.12866 | [2404.12866] How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning?
https://arxiv.org/abs/2404.12839 | [2404.12839] ECOR: Explainable CLIP for Object Recognition
https://arxiv.org/abs/2404.12803 | [2404.12803] TextSquare: Scaling up Text-Centric Visual Instruction Tuning
https://arxiv.org/abs/2404.12588 | [2404.12588] Cross-Modal Adapter: Parameter-Efficient Transfer Learning Approach for Vision-Language Models
https://arxiv.org/abs/2404.12541 | [2404.12541] GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models
https://arxiv.org/abs/2404.12526 | [2404.12526] Adaptive Memory Replay for Continual Learning