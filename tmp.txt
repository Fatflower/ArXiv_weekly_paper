https://arxiv.org/abs/2402.14815 | [2402.14815] Demographic Bias of Expert-Level Vision-Language Foundation Models in Medical Imaging
https://arxiv.org/abs/2402.14809 | [2402.14809] CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
https://arxiv.org/abs/2402.14818 | [2402.14818] PALO: A Polyglot Large Multimodal Model for 5B People
https://arxiv.org/abs/2402.14812 | [2402.14812] WeakSAM: Segment Anything Meets Weakly-supervised Instance-level Recognition
https://arxiv.org/abs/2402.14760 | [2402.14760] Generalizing Reward Modeling for Out-of-Distribution Preference Learning
https://arxiv.org/abs/2402.14683 | [2402.14683] Visual Hallucinations of Multi-modal Large Language Models
https://arxiv.org/abs/2402.14611 | [2402.14611] Overcoming Dimensional Collapse in Self-supervised Contrastive Learning for Medical Image Segmentation
https://arxiv.org/abs/2402.14577 | [2402.14577] Debiasing Text-to-Image Diffusion Models
https://arxiv.org/abs/2402.14492 | [2402.14492] INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning
https://arxiv.org/abs/2402.14474 | [2402.14474] Data Science with LLMs and Interpretable Models
https://arxiv.org/abs/2402.14418 | [2402.14418] Uncertainty-Aware Evaluation for Vision-Language Models
https://arxiv.org/abs/2402.14289 | [2402.14289] TinyLLaVA: A Framework of Small-scale Large Multimodal Models
https://arxiv.org/abs/2402.14114 | [2402.14114] Multi-organ Self-supervised Contrastive Learning for Breast Lesion Segmentation