https://arxiv.org/abs/2402.13253 | [2402.13253] BiMediX: Bilingual Medical Mixture of Experts LLM
https://arxiv.org/abs/2402.13232 | [2402.13232] A Touch, Vision, and Language Dataset for Multimodal Alignment
https://arxiv.org/abs/2402.13116 | [2402.13116] A Survey on Knowledge Distillation of Large Language Models
https://arxiv.org/abs/2402.13088 | [2402.13088] Slot-VLM: SlowFast Slots for Video-Language Modeling
https://arxiv.org/abs/2402.12938 | [2402.12938] UniCell: Universal Cell Nucleus Classification via Prompt Learning
https://arxiv.org/abs/2402.12862 | [2402.12862] Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation
https://arxiv.org/abs/2402.12847 | [2402.12847] Instruction-tuned Language Models are Better Knowledge Learners
https://arxiv.org/abs/2402.12819 | [2402.12819] Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
https://arxiv.org/abs/2402.12760 | [2402.12760] A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis
https://arxiv.org/abs/2402.12749 | [2402.12749] Me LLaMA: Foundation Large Language Models for Medical Applications
https://arxiv.org/abs/2402.12736 | [2402.12736] CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning
https://arxiv.org/abs/2402.12656 | [2402.12656] HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts
https://arxiv.org/abs/2402.12624 | [2402.12624] Efficient Parameter Mining and Freezing for Continual Object Detection
https://arxiv.org/abs/2402.12490 | [2402.12490] Towards Cross-Domain Continual Learning
https://arxiv.org/abs/2402.12451 | [2402.12451] The (R)Evolution of Multimodal Large Language Models: A Survey
https://arxiv.org/abs/2402.12735 | [2402.12735] Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference