https://arxiv.org/abs/2405.19335 | [2405.19335] X-VILA: Cross-Modality Alignment for Large Language Model
https://arxiv.org/abs/2405.19334 | [2405.19334] LLMs Meet Multimodal Generation and Editing: A Survey
https://arxiv.org/abs/2405.19333 | [2405.19333] Multi-Modal Generative Embedding Model
https://arxiv.org/abs/2405.19325 | [2405.19325] Nearest Neighbor Speculative Decoding for LLM Generation and Attribution
https://arxiv.org/abs/2405.19315 | [2405.19315] Matryoshka Query Transformer for Large Vision-Language Models
https://arxiv.org/abs/2405.19299 | [2405.19299] Expert-Guided Extinction of Toxic Tokens for Debiased Generation
https://arxiv.org/abs/2405.19266 | [2405.19266] PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications
https://arxiv.org/abs/2405.19262 | [2405.19262] Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models
https://arxiv.org/abs/2405.19247 | [2405.19247] Comparative Study of Neighbor-based Methods for Local Outlier Detection
https://arxiv.org/abs/2405.19234 | [2405.19234] Forward-Backward Knowledge Distillation for Continual Clustering
https://arxiv.org/abs/2405.19162 | [2405.19162] Does learning the right latent variables necessarily improve in-context learning?
https://arxiv.org/abs/2405.19093 | [2405.19093] Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation
https://arxiv.org/abs/2405.19092 | [2405.19092] Benchmarking and Improving Detail Image Caption
https://arxiv.org/abs/2405.19086 | [2405.19086] MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors
https://arxiv.org/abs/2405.19085 | [2405.19085] Patch-enhanced Mask Encoder Prompt Image Generation
https://arxiv.org/abs/2405.19074 | [2405.19074] Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning
https://arxiv.org/abs/2405.18897 | [2405.18897] MLAE: Masked LoRA Experts for Parameter-Efficient Fine-Tuning
https://arxiv.org/abs/2405.18774 | [2405.18774] LLaMA-Reg: Using LLaMA 2 for Unsupervised Medical Image Registration
https://arxiv.org/abs/2405.18756 | [2405.18756] Provable Contrastive Continual Learning
https://arxiv.org/abs/2405.18653 | [2405.18653] Recent Advances of Foundation Language Models-based Continual Learning: A Survey
https://arxiv.org/abs/2405.18635 | [2405.18635] When and How Does In-Distribution Label Help Out-of-Distribution Detection?
https://arxiv.org/abs/2405.18541 | [2405.18541] Low-Rank Few-Shot Adaptation of Vision-Language Models
https://arxiv.org/abs/2405.18437 | [2405.18437] Transductive Zero-Shot and Few-Shot CLIP
https://arxiv.org/abs/2405.18533 | [2405.18533] Cardiovascular Disease Detection from Multi-View Chest X-rays with BI-Mamba
https://arxiv.org/abs/2405.20053 | [2405.20053] Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads
https://arxiv.org/abs/2405.20015 | [2405.20015] Efficient LLM-Jailbreaking by Introducing Visual Modality
https://arxiv.org/abs/2405.19950 | [2405.19950] MM-Lego: Modular Biomedical Multimodal Models with Minimal Fine-Tuning
https://arxiv.org/abs/2405.19941 | [2405.19941] Synthetic Patients: Simulating Difficult Conversations with Multimodal Generative AI for Medical Education
https://arxiv.org/abs/2405.19902 | [2405.19902] Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection
https://arxiv.org/abs/2405.19899 | [2405.19899] Open-Set Domain Adaptation for Semantic Segmentation
https://arxiv.org/abs/2405.19882 | [2405.19882] PixOOD: Pixel-Level Out-of-Distribution Detection
https://arxiv.org/abs/2405.19804 | [2405.19804] Exploring Key Factors for Long-Term Vessel Incident Risk Prediction
https://arxiv.org/abs/2405.19708 | [2405.19708] Text Guided Image Editing with Automatic Concept Locating and Forgetting
https://arxiv.org/abs/2405.19670 | [2405.19670] One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models
https://arxiv.org/abs/2405.19638 | [2405.19638] Learning Robust Correlation with Foundation Model for Weakly-Supervised Few-Shot Segmentation
https://arxiv.org/abs/2405.19597 | [2405.19597] SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors
https://arxiv.org/abs/2405.19592 | [2405.19592] Why Larger Language Models Do In-context Learning Differently?
https://arxiv.org/abs/2405.19568 | [2405.19568] Organizing Background to Explore Latent Classes for Incremental Few-shot Semantic Segmentation
https://arxiv.org/abs/2405.19547 | [2405.19547] CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning
https://arxiv.org/abs/2405.19387 | [2405.19387] Video Anomaly Detection in 10 Years: A Survey and Outlook
https://arxiv.org/abs/2310.11952 | [2310.11952] Recasting Continual Learning as Sequence Modeling
https://arxiv.org/abs/2405.20236 | [2405.20236] Disentangling and Mitigating the Impact of Task Similarity for Continual Learning
https://arxiv.org/abs/2405.20339 | [2405.20339] Visual Perception by Large Language Model's Weights
https://arxiv.org/abs/2405.20335 | [2405.20335] Xwin-LM: Strong and Scalable Alignment Practice for LLMs
https://arxiv.org/abs/2405.20333 | [2405.20333] SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical Videos
https://arxiv.org/abs/2405.20319 | [2405.20319] ParSEL: Parameterized Shape Editing with Language
https://arxiv.org/abs/2405.20315 | [2405.20315] ANAH: Analytical Annotation of Hallucinations in Large Language Models
https://arxiv.org/abs/2405.20305 | [2405.20305] Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models
https://arxiv.org/abs/2405.20252 | [2405.20252] Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization
https://arxiv.org/abs/2405.20234 | [2405.20234] Context Injection Attacks on Large Language Models
https://arxiv.org/abs/2405.20215 | [2405.20215] TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models
https://arxiv.org/abs/2405.20204 | [2405.20204] Jina CLIP: Your CLIP Model Is Also Your Text Retriever
https://arxiv.org/abs/2405.20192 | [2405.20192] TAIA: Large Language Models are Out-of-Distribution Data Learners
https://arxiv.org/abs/2405.20175 | [2405.20175] InstructionCP: A fast approach to transfer Large Language Models into target language
https://arxiv.org/abs/2405.20141 | [2405.20141] OpenDAS: Domain Adaptation for Open-Vocabulary Segmentation
https://arxiv.org/abs/2405.20142 | [2405.20142] MSSC-BiMamba: Multimodal Sleep Stage Classification and Early Diagnosis of Sleep Disorders with Bidirectional Mamba
https://arxiv.org/abs/2405.20099 | [2405.20099] Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks
https://arxiv.org/abs/2405.20089 | [2405.20089] The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities
https://arxiv.org/abs/2405.20059 | [2405.20059] Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation