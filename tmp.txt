https://arxiv.org/abs/2407.17467 | [2407.17467] CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models
https://arxiv.org/abs/2407.17353 | [2407.17353] Scalify: scale propagation for efficient low-precision LLM training
https://arxiv.org/abs/2407.17331 | [2407.17331] Multi-label Cluster Discrimination for Visual Representation Learning
https://arxiv.org/abs/2407.17261 | [2407.17261] Embedding-Free Transformer with Inference Spatial Reduction for Efficient Semantic Segmentation
https://arxiv.org/abs/2407.17219 | [2407.17219] Graph Neural Networks: A suitable Alternative to MLPs in Latent 3D Medical Image Classification?
https://arxiv.org/abs/2407.17083 | [2407.17083] When Text and Images Don't Mix: Bias-Correcting Language-Image Similarity Scores for Anomaly Detection
https://arxiv.org/abs/2407.17060 | [2407.17060] High Efficiency Image Compression for Large Visual-Language Models
https://arxiv.org/abs/2407.16977 | [2407.16977] Selective Vision-Language Subspace Projection for Few-shot CLIP
https://arxiv.org/abs/2407.16953 | [2407.16953] Open Challenges on Fairness of Artificial Intelligence in Medical Imaging Applications
https://arxiv.org/abs/2407.16946 | [2407.16946] Automatic Categorization of GitHub Actions with Transformers and Few-shot Learning
https://arxiv.org/abs/2407.16920 | [2407.16920] Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning
https://arxiv.org/abs/2407.16826 | [2407.16826] SINDER: Repairing the Singular Defects of DINOv2