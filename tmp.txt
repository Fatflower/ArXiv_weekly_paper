https://arxiv.org/abs/2406.09294 | [2406.09294] You Don't Need Data-Augmentation in Self-Supervised Learning
https://arxiv.org/abs/2406.09317 | [2406.09317] Common and Rare Fundus Diseases Identification Using Vision-Language Foundation Model with Knowledge of Over 400 Diseases
https://arxiv.org/abs/2406.09327 | [2406.09327] Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN Pipeline applied on PSMA PET/CT Scans
https://arxiv.org/abs/2406.09384 | [2406.09384] Reflecting on the State of Rehearsal-free Continual Learning with Pretrained Models
https://arxiv.org/abs/2406.08528 | [2406.08528] Adaptive Teaching with Shared Classifier for Knowledge Distillation
https://arxiv.org/abs/2406.08659 | [2406.08659] Vivid-ZOO: Multi-View Video Generation with Diffusion Model
https://arxiv.org/abs/2406.08798 | [2406.08798] FouRA: Fourier Low Rank Adaptation
https://arxiv.org/abs/2406.08816 | [2406.08816] ToSA: Token Selective Attention for Efficient Vision Transformers
https://arxiv.org/abs/2406.09003 | [2406.09003] Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality Generation
https://arxiv.org/abs/2406.09112 | [2406.09112] Large-Scale Evaluation of Open-Set Image Classification Techniques
https://arxiv.org/abs/2406.09162 | [2406.09162] EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts
https://arxiv.org/abs/2406.09296 | [2406.09296] Parameter-Efficient Active Learning for Foundational models
https://arxiv.org/abs/2406.09385 | [2406.09385] Towards Vision-Language Geo-Foundation Model: A Survey
https://arxiv.org/abs/2406.09398 | [2406.09398] Real-Time Deepfake Detection in the Real-World
https://arxiv.org/abs/2406.09400 | [2406.09400] Yo'LLaVA: Your Personalized Language and Vision Assistant
https://arxiv.org/abs/2406.09406 | [2406.09406] 4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities
https://arxiv.org/abs/2406.09408 | [2406.09408] Data Attribution for Text-to-Image Models by Unlearning Synthesized Images