https://arxiv.org/abs/2409.00685 | [2409.00685] Accurate Forgetting for All-in-One Image Restoration Model
https://arxiv.org/abs/2409.00618 | [2409.00618] YOLOO: You Only Learn from Others Once
https://arxiv.org/abs/2409.00563 | [2409.00563] Sparse Mamba: Reinforcing Controllability In Structural State Space Models
https://arxiv.org/abs/2409.00556 | [2409.00556] FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model
https://arxiv.org/abs/2409.00547 | [2409.00547] Data Augmentation for Image Classification using Generative AI
https://arxiv.org/abs/2409.00543 | [2409.00543] How Does Diverse Interpretability of Textual Prompts Impact Medical Vision-Language Zero-Shot Tasks?
https://arxiv.org/abs/2409.00530 | [2409.00530] Incremental Open-set Domain Adaptation
https://arxiv.org/abs/2409.00133 | [2409.00133] A Survey for Large Language Models in Biomedicine
https://arxiv.org/abs/2409.00084 | [2409.00084] Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models
https://arxiv.org/abs/2409.02919 | [2409.02919] HiPrompt: Tuning-free Higher-Resolution Generation with Hierarchical MLLM Prompts
https://arxiv.org/abs/2409.02889 | [2409.02889] LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture
https://arxiv.org/abs/2409.02882 | [2409.02882] Benchmarking Spurious Bias in Few-Shot Image Classifiers
https://arxiv.org/abs/2409.02850 | [2409.02850] Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning
https://arxiv.org/abs/2409.02795 | [2409.02795] Towards a Unified View of Preference Learning for Large Language Models: A Survey
https://arxiv.org/abs/2409.02751 | [2409.02751] A Comparative Study of Pre-training and Self-training
https://arxiv.org/abs/2409.02729 | [2409.02729] MedUnA: Language guided Unsupervised Adaptation of Vision-Language Models for Medical Image Classification
https://arxiv.org/abs/2409.02686 | [2409.02686] Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs
https://arxiv.org/abs/2409.02691 | [2409.02691] LLM-Assisted Visual Analytics: Opportunities and Challenges
https://arxiv.org/abs/2409.02708 | [2409.02708] Few-shot Multi-Task Learning of Linear Invariant Features with Meta Subspace Pursuit
https://arxiv.org/abs/2409.02664 | [2409.02664] Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection
https://arxiv.org/abs/2409.02608 | [2409.02608] A Medical Multimodal Large Language Model for Pediatric Pneumonia
https://arxiv.org/abs/2409.02567 | [2409.02567] Evaluation Study on SAM 2 for Class-agnostic Instance-level Segmentation
https://arxiv.org/abs/2409.02561 | [2409.02561] Vision-Language Navigation with Continual Learning
https://arxiv.org/abs/2409.02513 | [2409.02513] SG-MIM: Structured Knowledge Guided Efficient Pre-training for Dense Prediction
https://arxiv.org/abs/2409.02429 | [2409.02429] Training-free Color-Style Disentanglement for Constrained Text-to-Image Synthesis
https://arxiv.org/abs/2409.02390 | [2409.02390] Neural Dynamics Model of Visual Decision-Making: Learning from Human Experts
https://arxiv.org/abs/2409.02253 | [2409.02253] How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?
https://arxiv.org/abs/2409.02060 | [2409.02060] OLMoE: Open Mixture-of-Experts Language Models
https://arxiv.org/abs/2409.02007 | [2409.02007] PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for Efficient Point Cloud Classification
https://arxiv.org/abs/2409.01980 | [2409.01980] Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey
https://arxiv.org/abs/2409.01930 | [2409.01930] Efficient LLM Context Distillation
https://arxiv.org/abs/2409.01835 | [2409.01835] Towards Generative Class Prompt Learning for Few-shot Visual Recognition
https://arxiv.org/abs/2409.01821 | [2409.01821] When Does Visual Prompting Outperform Linear Probing for Vision-Language Models? A Likelihood Perspective
https://arxiv.org/abs/2409.01728 | [2409.01728] Shuffle Mamba: State Space Models with Random Shuffle for Multi-Modal Image Fusion
https://arxiv.org/abs/2409.01572 | [2409.01572] LSSF-Net: Lightweight Segmentation with Self-Awareness, Spatial Attention, and Focal Modulation
https://arxiv.org/abs/2409.01571 | [2409.01571] CT-SDM: A Sampling Diffusion Model for Sparse-View CT Reconstruction across All Sampling Rates
https://arxiv.org/abs/2409.01483 | [2409.01483] Revisiting SMoE Language Models by Evaluating Inefficiencies with Task Specific Expert Pruning
https://arxiv.org/abs/2409.01207 | [2409.01207] Towards General Industrial Intelligence: A Survey on IIoT-Enhanced Continual Large Models
https://arxiv.org/abs/2409.01175 | [2409.01175] Logit Scaling for Out-of-Distribution Detection
https://arxiv.org/abs/2409.01128 | [2409.01128] Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning
https://arxiv.org/abs/2409.01109 | [2409.01109] SOOD-ImageNet: a Large-Scale Dataset for Semantic Out-Of-Distribution Image Classification and Semantic Segmentation
https://arxiv.org/abs/2409.01035 | [2409.01035] Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning
https://arxiv.org/abs/2409.00924 | [2409.00924] MedSAM-U: Uncertainty-Guided Auto Multi-Prompt Adaptation for Reliable MedSAM
https://arxiv.org/abs/2409.00788 | [2409.00788] Modeling Text-Label Alignment for Hierarchical Text Classification
https://arxiv.org/abs/2409.00698 | [2409.00698] Enhancing Remote Sensing Vision-Language Models for Zero-Shot Scene Classification
https://arxiv.org/abs/2409.00695 | [2409.00695] Curriculum Prompting Foundation Models for Medical Image Segmentation