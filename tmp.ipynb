{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. [CosmicMan: A Text-to-Image Foundation Model for Humans](https://arxiv.org/abs/2404.01294)\n",
      "2. [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291)\n",
      "3. [LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization](https://arxiv.org/abs/2404.01282)\n",
      "4. [Bridging Remote Sensors with Multisensor Geospatial Foundation Models](https://arxiv.org/abs/2404.01260)\n",
      "5. [Open-Vocabulary Federated Learning with Multimodal Prototyping](https://arxiv.org/abs/2404.01232)\n",
      "6. [BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2404.01179)\n",
      "7. [Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation](https://arxiv.org/abs/2404.01127)\n",
      "8. [Efficient Prompting Methods for Large Language Models: A Survey](https://arxiv.org/abs/2404.01077)\n",
      "9. [T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation](https://arxiv.org/abs/2404.01065)\n",
      "10. [Continual Learning for Smart City: A Survey](https://arxiv.org/abs/2404.00983)\n",
      "11. [Harnessing The Power of Attention For Patch-Based Biomedical Image Classification](https://arxiv.org/abs/2404.00949)\n",
      "12. [A Comprehensive Review of Knowledge Distillation in Computer Vision](https://arxiv.org/abs/2404.00936)\n",
      "13. [Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2404.00918)\n",
      "14. [Slightly Shift New Classes to Remember Old Classes for Video Class-Incremental Learning](https://arxiv.org/abs/2404.00901)\n",
      "15. [Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance](https://arxiv.org/abs/2404.00860)\n",
      "16. [Rehearsal-Free Modular and Compositional Continual Learning for Language Models](https://arxiv.org/abs/2404.00790)\n",
      "17. [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2404.00781)\n",
      "18. [LLM meets Vision-Language Models for Zero-Shot One-Class Classification](https://arxiv.org/abs/2404.00675)\n",
      "19. [Deep Instruction Tuning for Segment Anything Model](https://arxiv.org/abs/2404.00650)\n",
      "20. [SpiralMLP: A Lightweight Vision MLP Architecture](https://arxiv.org/abs/2404.00648)\n",
      "21. [Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning](https://arxiv.org/abs/2404.00603)\n",
      "22. [LLMs are Good Action Recognizers](https://arxiv.org/abs/2404.00532)\n",
      "23. [Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation](https://arxiv.org/abs/2404.00417)\n",
      "24. [TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias](https://arxiv.org/abs/2404.00384)\n",
      "25. [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/abs/2404.00376)\n",
      "26. [DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2404.00380)\n",
      "27. [CLIP-driven Outliers Synthesis for few-shot OOD detection](https://arxiv.org/abs/2404.00323)\n",
      "28. [Bayesian Exploration of Pre-trained Models for Low-shot Image Classification](https://arxiv.org/abs/2404.00312)\n",
      "29. [DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation](https://arxiv.org/abs/2404.00264)\n",
      "30. [Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2404.00262)\n",
      "31. [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228)\n",
      "32. [Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark](https://arxiv.org/abs/2404.00216)\n",
      "33. [AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation](https://arxiv.org/abs/2404.00122)\n",
      "34. [Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation](https://arxiv.org/abs/2404.01102)\n",
      "35. [Are We on the Right Way for Evaluating Large Vision-Language Models?](https://arxiv.org/abs/2403.20330)\n",
      "36. [MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning](https://arxiv.org/abs/2403.20320)\n",
      "37. [Convolutional Prompting meets Language Models for Continual Learning](https://arxiv.org/abs/2403.20317)\n",
      "38. [Learn \"No\" to Say \"Yes\" Better: Improving Vision-Language Models via Negations](https://arxiv.org/abs/2403.20312)\n",
      "39. [Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference](https://arxiv.org/abs/2403.20306)\n",
      "40. [Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain](https://arxiv.org/abs/2403.20288)\n",
      "41. [LayerNorm: A key component in parameter-efficient fine-tuning](https://arxiv.org/abs/2403.20284)\n",
      "42. [Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://arxiv.org/abs/2403.20271)\n",
      "43. [MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation](https://arxiv.org/abs/2403.20253)\n",
      "44. [ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models](https://arxiv.org/abs/2403.20194)\n",
      "45. [ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models](https://arxiv.org/abs/2403.20158)\n",
      "46. [ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning](https://arxiv.org/abs/2403.20126)\n",
      "47. [FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models](https://arxiv.org/abs/2403.20105)\n",
      "48. [NLP for Counterspeech against Hate: A Survey and How-To Guide](https://arxiv.org/abs/2403.20103)\n",
      "49. [Selective Attention-based Modulation for Continual Learning](https://arxiv.org/abs/2403.20086)\n",
      "50. [Negative Label Guided OOD Detection with Pretrained Vision-Language Models](https://arxiv.org/abs/2403.20078)\n",
      "51. [Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer](https://arxiv.org/abs/2403.19979)\n",
      "52. [Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data](https://arxiv.org/abs/2403.19950)\n",
      "53. [FairCLIP: Harnessing Fairness in Vision-Language Learning](https://arxiv.org/abs/2403.19949)\n",
      "54. [An Information-Theoretic Framework for Out-of-Distribution Generalization](https://arxiv.org/abs/2403.19895)\n",
      "55. [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)\n",
      "56. [Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation](https://arxiv.org/abs/2403.19826)\n",
      "57. [MAPL: Model Agnostic Peer-to-peer Learning](https://arxiv.org/abs/2403.19792)\n",
      "58. [CLoRA: A Contrastive Approach to Compose Multiple LoRA Models](https://arxiv.org/abs/2403.19776)\n",
      "59. [GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation](https://arxiv.org/abs/2403.19754)\n",
      "60. [Analyzing the Roles of Language and Vision in Learning from Limited Data](https://arxiv.org/abs/2403.19669)\n",
      "61. [UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation](https://arxiv.org/abs/2403.20035)\n",
      "62. [Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks](https://arxiv.org/abs/2403.19880)\n"
     ]
    }
   ],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=0)\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
