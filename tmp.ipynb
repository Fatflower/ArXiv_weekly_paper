{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19. [V-IRL: Grounding Virtual Intelligence in Real Life](https://arxiv.org/abs/2402.03310)\n",
      "20. [HASSOD: Hierarchical Adaptive Self-Supervised Object Detection](https://arxiv.org/abs/2402.03311)\n",
      "21. [Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?](https://arxiv.org/abs/2402.03305)\n",
      "22. [Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://arxiv.org/abs/2402.03302)\n",
      "23. [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303)\n",
      "24. [Zero-shot Object-Level OOD Detection with Context-Aware Inpainting](https://arxiv.org/abs/2402.03292)\n",
      "25. [InstanceDiffusion: Instance-level Control for Image Generation](https://arxiv.org/abs/2402.03290)\n",
      "26. [Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2402.03286)\n",
      "27. [Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models](https://arxiv.org/abs/2402.03284)\n",
      "28. [CLIP Can Understand Depth](https://arxiv.org/abs/2402.03251)\n",
      "29. [FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition](https://arxiv.org/abs/2402.03241)\n",
      "30. [The Matrix: A Bayesian learning model for LLMs](https://arxiv.org/abs/2402.03175)\n",
      "31. [RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification](https://arxiv.org/abs/2402.03166)\n",
      "32. [Is Mamba Capable of In-Context Learning?](https://arxiv.org/abs/2402.03170)\n",
      "33. [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/abs/2402.03094)\n",
      "34. [Text-Guided Image Clustering](https://arxiv.org/abs/2402.02996)\n",
      "35. [Retrieval-Augmented Score Distillation for Text-to-3D Generation](https://arxiv.org/abs/2402.02972)\n",
      "36. [Kernel PCA for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02949)\n",
      "37. [Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey](https://arxiv.org/abs/2402.02941)\n",
      "38. [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)\n",
      "39. [Causal Feature Selection for Responsible Machine Learning](https://arxiv.org/abs/2402.02696)\n",
      "40. [Learning with Mixture of Prototypes for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02653)\n",
      "41. [VM-UNet: Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2402.02491)\n",
      "42. [Why are hyperbolic neural networks effective? A study on hierarchical representation capability](https://arxiv.org/abs/2402.02478)\n",
      "43. [Deep Spectral Improvement for Unsupervised Image Instance Segmentation](https://arxiv.org/abs/2402.02474)\n",
      "44. [BECLR: Batch Enhanced Contrastive Few-Shot Learning](https://arxiv.org/abs/2402.02444)\n",
      "45. [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382)\n",
      "46. [Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation](https://arxiv.org/abs/2402.02367)\n",
      "47. [The Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364)\n",
      "48. [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://arxiv.org/abs/2402.02347)\n",
      "49. [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://arxiv.org/abs/2402.02340)\n",
      "50. [Your Diffusion Model is Secretly a Certifiably Robust Classifier](https://arxiv.org/abs/2402.02316)\n",
      "51. [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314)\n",
      "52. [Polyp-DAM: Polyp segmentation via depth anything model](https://arxiv.org/abs/2402.02298)\n",
      "53. [Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2402.02286)\n",
      "54. [SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking](https://arxiv.org/abs/2402.02285)\n",
      "55. [Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey](https://arxiv.org/abs/2402.02242)\n",
      "56. [Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets](https://arxiv.org/abs/2402.02245)\n",
      "57. [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://arxiv.org/abs/2402.02207)\n",
      "58. [GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events](https://arxiv.org/abs/2402.02205)\n",
      "59. [Prompting Diverse Ideas: Increasing AI Idea Variance](https://arxiv.org/abs/2402.01727)\n",
      "60. [Are Large Language Models Good Prompt Optimizers?](https://arxiv.org/abs/2402.02101)\n",
      "61. [Trustworthiness of $\\mathbb{X}$ Users: A One-Class Classification Approach](https://arxiv.org/abs/2402.02066)\n",
      "62. [MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning](https://arxiv.org/abs/2402.02045)\n",
      "63. [ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.02029)\n",
      "64. [Understanding Time Series Anomaly State Detection through One-Class Classification](https://arxiv.org/abs/2402.02007)\n",
      "65. [Robust Counterfactual Explanations in Machine Learning: A Survey](https://arxiv.org/abs/2402.01928)\n",
      "66. [A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm](https://arxiv.org/abs/2402.01684)\n",
      "67. [Sample, estimate, aggregate: A recipe for causal discovery foundation models](https://arxiv.org/abs/2402.01929)\n",
      "68. [SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?](https://arxiv.org/abs/2402.01832)\n",
      "69. [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761)\n",
      "70. [VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models](https://arxiv.org/abs/2402.01735)\n"
     ]
    }
   ],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=18)\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
