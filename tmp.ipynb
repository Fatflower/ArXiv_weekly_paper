{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63. [AGHINT: Attribute-Guided Representation Learning on Heterogeneous Information Networks with Transformer](https://arxiv.org/abs/2404.10443)\n",
      "64. [Know Yourself Better: Diverse Discriminative Feature Learning Improves Open Set Recognition](https://arxiv.org/abs/2404.10370)\n",
      "65. [Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review](https://arxiv.org/abs/2404.11597)\n",
      "66. [Variational Bayesian Last Layers](https://arxiv.org/abs/2404.11599)\n",
      "67. [JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on Long-Tailed OCTA](https://arxiv.org/abs/2404.11525)\n",
      "68. [VBR: A Vision Benchmark in Rome](https://arxiv.org/abs/2404.11322)\n",
      "69. [Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM](https://arxiv.org/abs/2404.11209)\n",
      "70. [Exploring the Transferability of Visual Prompting for Multimodal Large Language Models](https://arxiv.org/abs/2404.11207)\n",
      "71. [LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)\n",
      "72. [Many-Shot In-Context Learning](https://arxiv.org/abs/2404.11018)\n",
      "73. [Control Theoretic Approach to Fine-Tuning and Transfer Learning](https://arxiv.org/abs/2404.11013)\n",
      "74. [Incubating Text Classifiers Following User Instruction with Nothing but LLM](https://arxiv.org/abs/2404.10877)\n",
      "75. [BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390)\n",
      "76. [Moving Object Segmentation: All You Need Is SAM (and Flow)](https://arxiv.org/abs/2404.12389)\n",
      "77. [Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models](https://arxiv.org/abs/2404.12387)\n",
      "78. [SOHES: Self-supervised Open-world Hierarchical Entity Segmentation](https://arxiv.org/abs/2404.12386)\n",
      "79. [Gradient-Regularized Out-of-Distribution Detection](https://arxiv.org/abs/2404.12368)\n",
      "80. [When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes](https://arxiv.org/abs/2404.12365)\n",
      "81. [V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning](https://arxiv.org/abs/2404.12353)\n",
      "82. [Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment](https://arxiv.org/abs/2404.12318)\n",
      "83. [Observation, Analysis, and Solution: Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-Training](https://arxiv.org/abs/2404.12210)\n",
      "84. [How to Benchmark Vision Foundation Models for Semantic Segmentation?](https://arxiv.org/abs/2404.12172)\n",
      "85. [Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models](https://arxiv.org/abs/2404.12139)\n",
      "86. [MaskCD: A Remote Sensing Change Detection Network Based on Mask Classification](https://arxiv.org/abs/2404.12081)\n",
      "87. [Data-free Knowledge Distillation for Fine-grained Visual Categorization](https://arxiv.org/abs/2404.12037)\n",
      "88. [What does CLIP know about peeling a banana?](https://arxiv.org/abs/2404.12015)\n",
      "89. [Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation](https://arxiv.org/abs/2404.11981)\n",
      "90. [Progressive Multi-modal Conditional Prompt Tuning](https://arxiv.org/abs/2404.11864)\n",
      "91. [TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation](https://arxiv.org/abs/2404.11824)\n",
      "92. [Prompt-Driven Feature Diffusion for Open-World Semi-Supervised Learning](https://arxiv.org/abs/2404.11795)\n",
      "93. [Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach](https://arxiv.org/abs/2404.11732)\n"
     ]
    }
   ],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=62)\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
