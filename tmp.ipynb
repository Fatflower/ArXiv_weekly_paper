{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. [CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning](https://arxiv.org/abs/2403.10245)\n",
      "2. [Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks](https://arxiv.org/abs/2403.10097)\n",
      "3. [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)\n",
      "4. [Group-Mix SAM: Lightweight Solution for Industrial Assembly Line Applications](https://arxiv.org/abs/2403.10053)\n",
      "5. [TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model](https://arxiv.org/abs/2403.10047)\n",
      "6. [Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation](https://arxiv.org/abs/2403.10039)\n",
      "7. [EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba](https://arxiv.org/abs/2403.09977)\n",
      "8. [GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery](https://arxiv.org/abs/2403.09974)\n",
      "9. [RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training](https://arxiv.org/abs/2403.09948)\n",
      "10. [MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation](https://arxiv.org/abs/2403.09850)\n",
      "11. [An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models](https://arxiv.org/abs/2403.09766)\n",
      "12. [COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced Medical Image Representation](https://arxiv.org/abs/2403.09672)\n",
      "13. [Cardiac Magnetic Resonance 2D+T Short- and Long-axis Segmentation via Spatio-temporal SAM Adaptation](https://arxiv.org/abs/2403.10009)\n",
      "14. [Frozen Feature Augmentation for Few-Shot Image Classification](https://arxiv.org/abs/2403.10519)\n",
      "15. [Energy Correction Model in the Feature Space for Out-of-Distribution Detection](https://arxiv.org/abs/2403.10403)\n",
      "16. [CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning](https://arxiv.org/abs/2403.10391)\n",
      "17. [Investigating grammatical abstraction in language models using few-shot learning of novel noun gender](https://arxiv.org/abs/2403.10338)\n",
      "18. [Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models](https://arxiv.org/abs/2403.10287)\n",
      "19. [Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks](https://arxiv.org/abs/2403.10097)\n",
      "20. [Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt](https://arxiv.org/abs/2403.09857)\n",
      "21. [FastSAM3D: An Efficient Segment Anything Model for 3D Volumetric Medical Images](https://arxiv.org/abs/2403.09827)\n",
      "22. [Distilling Datasets Into Less Than One Image](https://arxiv.org/abs/2403.12040)\n",
      "23. [Zero-Shot Image Feature Consensus with Deep Functional Maps](https://arxiv.org/abs/2403.12038)\n",
      "24. [One-Step Image Translation with Text-to-Image Models](https://arxiv.org/abs/2403.12036)\n",
      "25. [Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2403.12030)\n",
      "26. [SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules](https://arxiv.org/abs/2403.11887)\n",
      "27. [Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation](https://arxiv.org/abs/2403.11808)\n",
      "28. [LSKNet: A Foundation Lightweight Backbone for Remote Sensing](https://arxiv.org/abs/2403.11735)\n",
      "29. [Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection](https://arxiv.org/abs/2403.11667)\n",
      "30. [MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks](https://arxiv.org/abs/2403.11646)\n",
      "31. [LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models](https://arxiv.org/abs/2403.11627)\n",
      "32. [CRS-Diff: Controllable Generative Remote Sensing Foundation Model](https://arxiv.org/abs/2403.11614)\n",
      "33. [A survey of synthetic data augmentation methods in computer vision](https://arxiv.org/abs/2403.10075)\n",
      "34. [Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models](https://arxiv.org/abs/2403.12966)\n",
      "35. [Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models](https://arxiv.org/abs/2403.12964)\n",
      "36. [MEDBind: Unifying Language and Multimodal Medical Data Embeddings](https://arxiv.org/abs/2403.12894)\n",
      "37. [RelationVLM: Making Large Vision-Language Models Understand Visual Relations](https://arxiv.org/abs/2403.12801)\n",
      "38. [DreamDA: Generative Data Augmentation with Diffusion Models](https://arxiv.org/abs/2403.12803)\n",
      "39. [Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation](https://arxiv.org/abs/2403.12767)\n",
      "40. [Towards Multimodal In-Context Learning for Vision & Language Models](https://arxiv.org/abs/2403.12736)\n",
      "41. [Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection](https://arxiv.org/abs/2403.12580)\n",
      "42. [Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images](https://arxiv.org/abs/2403.12570)\n",
      "43. [Confidence Self-Calibration for Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2403.12559)\n",
      "44. [UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All](https://arxiv.org/abs/2403.12532)\n",
      "45. [CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation](https://arxiv.org/abs/2403.12455)\n",
      "46. [DMAD: Dual Memory Bank for Real-World Anomaly Detection](https://arxiv.org/abs/2403.12362)\n",
      "47. [A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models](https://arxiv.org/abs/2403.12052)\n",
      "48. [TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer](https://arxiv.org/abs/2403.12481)\n",
      "49. [Non-negative Contrastive Learning](https://arxiv.org/abs/2403.12459)\n",
      "50. [Do Generated Data Always Help Contrastive Learning?](https://arxiv.org/abs/2403.12448)\n",
      "51. [Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts](https://arxiv.org/abs/2403.12326)\n",
      "52. [Editing Massive Concepts in Text-to-Image Diffusion Models](https://arxiv.org/abs/2403.13807)\n",
      "53. [Learning from Models and Data for Visual Grounding](https://arxiv.org/abs/2403.13804)\n",
      "54. [ZigMa: Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802)\n",
      "55. [SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning](https://arxiv.org/abs/2403.13684)\n",
      "56. [Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers](https://arxiv.org/abs/2403.13677)\n",
      "57. [ProMamba: Prompt-Mamba for polyp segmentation](https://arxiv.org/abs/2403.13660)\n",
      "58. [Meta-Point Learning and Refining for Category-Agnostic Pose Estimation](https://arxiv.org/abs/2403.13647)\n",
      "59. [H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2403.13642)\n",
      "60. [VL-Mamba: Exploring State Space Models for Multimodal Learning](https://arxiv.org/abs/2403.13600)\n",
      "61. [IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models](https://arxiv.org/abs/2403.13535)\n",
      "62. [Scale Decoupled Distillation](https://arxiv.org/abs/2403.13512)\n",
      "63. [MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining](https://arxiv.org/abs/2403.13430)\n",
      "64. [Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model](https://arxiv.org/abs/2403.13324)\n",
      "65. [Improved EATFormer: A Vision Transformer for Medical Image Classification](https://arxiv.org/abs/2403.13167)\n",
      "66. [HuLP: Human-in-the-Loop for Prognosis](https://arxiv.org/abs/2403.13078)\n",
      "67. [TAPTR: Tracking Any Point with Transformers as Detection](https://arxiv.org/abs/2403.13042)\n",
      "68. [BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning](https://arxiv.org/abs/2403.12986)\n",
      "69. [Bridge the Modality and Capacity Gaps in Vision-Language Model Selection](https://arxiv.org/abs/2403.13797)\n",
      "70. [REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2403.13522)\n",
      "71. [CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models](https://arxiv.org/abs/2403.13467)\n",
      "72. [HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models](https://arxiv.org/abs/2403.13447)\n",
      "73. [Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection](https://arxiv.org/abs/2403.13349)\n",
      "74. [A Unified and General Framework for Continual Learning](https://arxiv.org/abs/2403.13249)\n",
      "75. [Trustworthiness of Pretrained Transformers for Lung Cancer Segmentation](https://arxiv.org/abs/2403.13113)\n",
      "76. [Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning](https://arxiv.org/abs/2403.14616)\n",
      "77. [T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy](https://arxiv.org/abs/2403.14610)\n",
      "78. [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)\n",
      "79. [PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model](https://arxiv.org/abs/2403.14598)\n",
      "80. [DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video](https://arxiv.org/abs/2403.14548)\n",
      "81. [Learning to Project for Cross-Task Knowledge Distillation](https://arxiv.org/abs/2403.14494)\n",
      "82. [Analysing Diffusion Segmentation for Medical Images](https://arxiv.org/abs/2403.14440)\n",
      "83. [A Bag of Tricks for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2403.14392)\n",
      "84. [HySim: An Efficient Hybrid Similarity Measure for Patch Matching in Image Inpainting](https://arxiv.org/abs/2403.14292)\n",
      "85. [Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference](https://arxiv.org/abs/2403.14213)\n",
      "86. [Unsupervised Audio-Visual Segmentation with Modality Alignment](https://arxiv.org/abs/2403.14203)\n",
      "87. [Volumetric Environment Representation for Vision-Language Navigation](https://arxiv.org/abs/2403.14158)\n",
      "88. [Empowering Segmentation Ability to Multi-modal Large Language Models](https://arxiv.org/abs/2403.14141)\n",
      "89. [Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup](https://arxiv.org/abs/2403.14137)\n",
      "90. [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/abs/2403.14119)\n",
      "91. [MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation](https://arxiv.org/abs/2403.14103)\n"
     ]
    }
   ],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=0)\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
