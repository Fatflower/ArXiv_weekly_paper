{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "def result_write_md(markdown_file, output_lines):\n",
    "    with open(markdown_file, 'a') as m_file:\n",
    "        m_file.seek(0, 2)\n",
    "        if m_file.tell() > 0:\n",
    "            m_file.write('\\n')\n",
    "        m_file.write(output_lines)\n",
    "    print(\"Susseccfully write to file!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71. [X-VILA: Cross-Modality Alignment for Large Language Model](https://arxiv.org/abs/2405.19335)\n",
      "72. [LLMs Meet Multimodal Generation and Editing: A Survey](https://arxiv.org/abs/2405.19334)\n",
      "73. [Multi-Modal Generative Embedding Model](https://arxiv.org/abs/2405.19333)\n",
      "74. [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)\n",
      "75. [Matryoshka Query Transformer for Large Vision-Language Models](https://arxiv.org/abs/2405.19315)\n",
      "76. [Expert-Guided Extinction of Toxic Tokens for Debiased Generation](https://arxiv.org/abs/2405.19299)\n",
      "77. [PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications](https://arxiv.org/abs/2405.19266)\n",
      "78. [Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models](https://arxiv.org/abs/2405.19262)\n",
      "79. [Comparative Study of Neighbor-based Methods for Local Outlier Detection](https://arxiv.org/abs/2405.19247)\n",
      "80. [Forward-Backward Knowledge Distillation for Continual Clustering](https://arxiv.org/abs/2405.19234)\n",
      "81. [Does learning the right latent variables necessarily improve in-context learning?](https://arxiv.org/abs/2405.19162)\n",
      "82. [Multi-stage Retrieve and Re-rank Model for Automatic Medical Coding Recommendation](https://arxiv.org/abs/2405.19093)\n",
      "83. [Benchmarking and Improving Detail Image Caption](https://arxiv.org/abs/2405.19092)\n",
      "84. [MEMoE: Enhancing Model Editing with Mixture of Experts Adaptors](https://arxiv.org/abs/2405.19086)\n",
      "85. [Patch-enhanced Mask Encoder Prompt Image Generation](https://arxiv.org/abs/2405.19085)\n",
      "86. [Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning](https://arxiv.org/abs/2405.19074)\n",
      "87. [MLAE: Masked LoRA Experts for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.18897)\n",
      "88. [LLaMA-Reg: Using LLaMA 2 for Unsupervised Medical Image Registration](https://arxiv.org/abs/2405.18774)\n",
      "89. [Provable Contrastive Continual Learning](https://arxiv.org/abs/2405.18756)\n",
      "90. [Recent Advances of Foundation Language Models-based Continual Learning: A Survey](https://arxiv.org/abs/2405.18653)\n",
      "91. [When and How Does In-Distribution Label Help Out-of-Distribution Detection?](https://arxiv.org/abs/2405.18635)\n",
      "92. [Low-Rank Few-Shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2405.18541)\n",
      "93. [Transductive Zero-Shot and Few-Shot CLIP](https://arxiv.org/abs/2405.18437)\n",
      "94. [Cardiovascular Disease Detection from Multi-View Chest X-rays with BI-Mamba](https://arxiv.org/abs/2405.18533)\n",
      "95. [Would I Lie To You? Inference Time Alignment of Language Models using Direct Preference Heads](https://arxiv.org/abs/2405.20053)\n",
      "96. [Efficient LLM-Jailbreaking by Introducing Visual Modality](https://arxiv.org/abs/2405.20015)\n",
      "97. [MM-Lego: Modular Biomedical Multimodal Models with Minimal Fine-Tuning](https://arxiv.org/abs/2405.19950)\n",
      "98. [Synthetic Patients: Simulating Difficult Conversations with Multimodal Generative AI for Medical Education](https://arxiv.org/abs/2405.19941)\n",
      "99. [Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection](https://arxiv.org/abs/2405.19902)\n",
      "100. [Open-Set Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2405.19899)\n",
      "101. [PixOOD: Pixel-Level Out-of-Distribution Detection](https://arxiv.org/abs/2405.19882)\n",
      "102. [Exploring Key Factors for Long-Term Vessel Incident Risk Prediction](https://arxiv.org/abs/2405.19804)\n",
      "103. [Text Guided Image Editing with Automatic Concept Locating and Forgetting](https://arxiv.org/abs/2405.19708)\n",
      "104. [One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2405.19670)\n",
      "105. [Learning Robust Correlation with Foundation Model for Weakly-Supervised Few-Shot Segmentation](https://arxiv.org/abs/2405.19638)\n",
      "106. [SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors](https://arxiv.org/abs/2405.19597)\n",
      "107. [Why Larger Language Models Do In-context Learning Differently?](https://arxiv.org/abs/2405.19592)\n",
      "108. [Organizing Background to Explore Latent Classes for Incremental Few-shot Semantic Segmentation](https://arxiv.org/abs/2405.19568)\n",
      "109. [CLIPLoss and Norm-Based Data Selection Methods for Multimodal Contrastive Learning](https://arxiv.org/abs/2405.19547)\n",
      "110. [Video Anomaly Detection in 10 Years: A Survey and Outlook](https://arxiv.org/abs/2405.19387)\n",
      "111. [Recasting Continual Learning as Sequence Modeling](https://arxiv.org/abs/2310.11952)\n",
      "112. [Disentangling and Mitigating the Impact of Task Similarity for Continual Learning](https://arxiv.org/abs/2405.20236)\n",
      "113. [Visual Perception by Large Language Model's Weights](https://arxiv.org/abs/2405.20339)\n",
      "114. [Xwin-LM: Strong and Scalable Alignment Practice for LLMs](https://arxiv.org/abs/2405.20335)\n",
      "115. [SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical Videos](https://arxiv.org/abs/2405.20333)\n",
      "116. [ParSEL: Parameterized Shape Editing with Language](https://arxiv.org/abs/2405.20319)\n",
      "117. [ANAH: Analytical Annotation of Hallucinations in Large Language Models](https://arxiv.org/abs/2405.20315)\n",
      "118. [Can't make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models](https://arxiv.org/abs/2405.20305)\n",
      "119. [Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization](https://arxiv.org/abs/2405.20252)\n",
      "120. [Context Injection Attacks on Large Language Models](https://arxiv.org/abs/2405.20234)\n",
      "121. [TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models](https://arxiv.org/abs/2405.20215)\n",
      "122. [Jina CLIP: Your CLIP Model Is Also Your Text Retriever](https://arxiv.org/abs/2405.20204)\n",
      "123. [TAIA: Large Language Models are Out-of-Distribution Data Learners](https://arxiv.org/abs/2405.20192)\n",
      "124. [InstructionCP: A fast approach to transfer Large Language Models into target language](https://arxiv.org/abs/2405.20175)\n",
      "125. [OpenDAS: Domain Adaptation for Open-Vocabulary Segmentation](https://arxiv.org/abs/2405.20141)\n",
      "126. [MSSC-BiMamba: Multimodal Sleep Stage Classification and Early Diagnosis of Sleep Disorders with Bidirectional Mamba](https://arxiv.org/abs/2405.20142)\n",
      "127. [Defensive Prompt Patch: A Robust and Interpretable Defense of LLMs against Jailbreak Attacks](https://arxiv.org/abs/2405.20099)\n",
      "128. [The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities](https://arxiv.org/abs/2405.20089)\n",
      "129. [Spectral Mapping of Singing Voices: U-Net-Assisted Vocal Segmentation](https://arxiv.org/abs/2405.20059)\n"
     ]
    }
   ],
   "source": [
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=70)\n",
    "\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susseccfully write to file!\n"
     ]
    }
   ],
   "source": [
    "# 写入Readme.md文件\n",
    "markdown_file = 'README.md'\n",
    "output_lines = converted_text\n",
    "result_write_md(markdown_file, output_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susseccfully write to file!\n"
     ]
    }
   ],
   "source": [
    "# 写入本周的markdown文件\n",
    "markdown_file = '2024/22_week.md'\n",
    "output_lines = converted_text\n",
    "result_write_md(markdown_file, output_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyt38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "176c92cd5e5485b42e3e73d2f25e60c2ed8f24d584687c4dae8d36d44ade605d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
