{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82. [OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning](https://arxiv.org/abs/2403.09634)\n",
      "83. [PosSAM: Panoptic Open-vocabulary Segment Anything](https://arxiv.org/abs/2403.09620)\n",
      "84. [Explore In-Context Segmentation via Latent Diffusion Models](https://arxiv.org/abs/2403.09616)\n",
      "85. [PrompTHis: Visualizing the Process and Influence of Prompt Editing during Text-to-Image Creation](https://arxiv.org/abs/2403.09615)\n",
      "86. [Renovating Names in Open-Vocabulary Segmentation Benchmarks](https://arxiv.org/abs/2403.09593)\n",
      "87. [Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation](https://arxiv.org/abs/2403.09572)\n",
      "88. [Less is More: Data Value Estimation for Visual Instruction Tuning](https://arxiv.org/abs/2403.09559)\n",
      "89. [EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning](https://arxiv.org/abs/2403.09502)\n",
      "90. [Anomaly Detection by Adapting a pre-trained Vision Language Model](https://arxiv.org/abs/2403.09493)\n",
      "91. [XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via Concept-guided Context Optimization](https://arxiv.org/abs/2403.09410)\n",
      "92. [Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks](https://arxiv.org/abs/2403.09377)\n",
      "93. [Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection](https://arxiv.org/abs/2403.09313)\n",
      "94. [Annotation Free Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2403.09307)\n",
      "95. [Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models](https://arxiv.org/abs/2403.09296)\n",
      "96. [Anatomical Structure-Guided Medical Vision-Language Pre-training](https://arxiv.org/abs/2403.09294)\n",
      "97. [WSI-SAM: Multi-resolution Segment Anything Model (SAM) for histopathology whole-slide images](https://arxiv.org/abs/2403.09257)\n",
      "98. [Customizing Segmentation Foundation Model via Prompt Learning for Instance Segmentation](https://arxiv.org/abs/2403.09199)\n",
      "99. [UniCode: Learning a Unified Codebook for Multimodal Large Language Models](https://arxiv.org/abs/2403.09072)\n",
      "100. [A Continued Pretrained LLM Approach for Automatic Medical Note Generation](https://arxiv.org/abs/2403.09057)\n",
      "101. [Towards a theory of model distillation](https://arxiv.org/abs/2403.09053)\n",
      "102. [The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models?](https://arxiv.org/abs/2403.09037)\n",
      "103. [PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2403.08967)\n",
      "104. [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946)\n",
      "105. [DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation](https://arxiv.org/abs/2403.08857)\n",
      "106. [TINA: Think, Interaction, and Action Framework for Zero-Shot Vision Language Navigation](https://arxiv.org/abs/2403.08833)\n",
      "107. [LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2403.08822)\n",
      "108. [CoBra: Complementary Branch Fusing Class and Semantic Knowledge for Robust Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2403.08801)\n",
      "109. [Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks](https://arxiv.org/abs/2403.08793)\n",
      "110. [Image-Text Out-Of-Context Detection Using Synthetic Multimodal Misinformation](https://arxiv.org/abs/2403.08783)\n",
      "111. [Leveraging Chat-Based Large Vision Language Models for Multimodal Out-Of-Context Detection](https://arxiv.org/abs/2403.08776)\n",
      "112. [Veagle: Advancements in Multimodal Representation Learning](https://arxiv.org/abs/2403.08773)\n",
      "113. [VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2403.09157)\n",
      "114. [PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models](https://arxiv.org/abs/2403.08851)\n"
     ]
    }
   ],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=81)\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
