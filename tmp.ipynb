{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41. [Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs](https://arxiv.org/abs/2404.07449)\n",
      "42. [CopilotCAD: Empowering Radiologists with Report Completion Models and Quantitative Evidence from Medical Image Foundation Models](https://arxiv.org/abs/2404.07424)\n",
      "43. [Object-Conditioned Energy-Based Attention Map Alignment in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.07389)\n",
      "44. [Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://arxiv.org/abs/2404.07214)\n",
      "45. [Uncertainty-guided annotation enhances segmentation with the human-in-the-loop](https://arxiv.org/abs/2404.07208)\n",
      "46. [LUCF-Net: Lightweight U-shaped Cascade Fusion Network for Medical Image Segmentation](https://arxiv.org/abs/2404.07473)\n",
      "47. [Rethinking Perceptual Metrics for Medical Image Translation](https://arxiv.org/abs/2404.07318)\n",
      "48. [G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images](https://arxiv.org/abs/2404.07474)\n",
      "49. [Structure-aware Fine-tuning for Code Pre-trained Models](https://arxiv.org/abs/2404.07471)\n",
      "50. [Scalable Language Model with Generalized Continual Learning](https://arxiv.org/abs/2404.07470)\n",
      "51. [Connecting NeRFs, Images, and Text](https://arxiv.org/abs/2404.07993)\n",
      "52. [OpenBias: Open-set Bias Detection in Text-to-Image Generative Models](https://arxiv.org/abs/2404.07990)\n",
      "53. [Rho-1: Not All Tokens Are What You Need](https://arxiv.org/abs/2404.07965)\n",
      "54. [Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation](https://arxiv.org/abs/2404.07933)\n",
      "55. [Calibration of Continual Learning Models](https://arxiv.org/abs/2404.07817)\n",
      "56. [PRAM: Place Recognition Anywhere Model for Efficient Visual Localization](https://arxiv.org/abs/2404.07785)\n",
      "57. [Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification](https://arxiv.org/abs/2404.07754)\n",
      "58. [Realistic Continual Learning Approach using Pre-trained Models](https://arxiv.org/abs/2404.07729)\n",
      "59. [ViM-UNet: Vision Mamba for Biomedical Segmentation](https://arxiv.org/abs/2404.07705)\n",
      "60. [Flatness Improves Backbone Generalisation in Few-shot Classification](https://arxiv.org/abs/2404.07696)\n",
      "61. [Multi-Image Visual Question Answering for Unsupervised Anomaly Detection](https://arxiv.org/abs/2404.07622)\n",
      "62. [Medical mT5: An Open-Source Multilingual Text-to-Text LLM for The Medical Domain](https://arxiv.org/abs/2404.07613)\n",
      "63. [Multi-rater Prompting for Ambiguous Medical Image Segmentation](https://arxiv.org/abs/2404.07580)\n",
      "64. [Decomposing Label Space, Format and Discrimination: Rethinking How LLMs Respond and Solve Tasks via In-Context Learning](https://arxiv.org/abs/2404.07546)\n",
      "65. [How is Visual Attention Influenced by Text Guidance? Database and Model](https://arxiv.org/abs/2404.07537)\n",
      "66. [Remembering Transformer for Continual Learning](https://arxiv.org/abs/2404.07518)\n",
      "67. [GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models](https://arxiv.org/abs/2404.07206)\n",
      "68. [BRAVE: Broadening the visual encoding of vision-language models](https://arxiv.org/abs/2404.07204)\n",
      "69. [Move Anything with Layered Scene Diffusion](https://arxiv.org/abs/2404.07178)\n",
      "70. [Continuous Language Model Interpolation for Dynamic and Controllable Text Generation](https://arxiv.org/abs/2404.07117)\n",
      "71. [Toward industrial use of continual learning : new metrics proposal for class incremental learning](https://arxiv.org/abs/2404.06972)\n",
      "72. [Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark](https://arxiv.org/abs/2404.06859)\n",
      "73. [Extracting Clean and Balanced Subset for Noisy Long-tailed Classification](https://arxiv.org/abs/2404.06795)\n",
      "74. [Adapting LLaMA Decoder to Vision Transformer](https://arxiv.org/abs/2404.06773)\n",
      "75. [Incremental XAI: Memorable Understanding of AI with Incremental Explanations](https://arxiv.org/abs/2404.06733)\n",
      "76. [SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models](https://arxiv.org/abs/2404.06666)\n",
      "77. [Calibrating Higher-Order Statistics for Few-Shot Class-Incremental Learning with Pre-trained Vision Transformers](https://arxiv.org/abs/2404.06622)\n",
      "78. [GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation](https://arxiv.org/abs/2404.06609)\n",
      "79. [Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation](https://arxiv.org/abs/2404.06542)\n",
      "80. [Hyperparameter Selection in Continual Learning](https://arxiv.org/abs/2404.06466)\n",
      "81. [Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot Medical Image Segmentation](https://arxiv.org/abs/2404.06362)\n",
      "82. [Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models](https://arxiv.org/abs/2404.06309)\n",
      "83. [Anchor-based Robust Finetuning of Vision-Language Models](https://arxiv.org/abs/2404.06244)\n",
      "84. [VI-OOD: A Unified Representation Learning Framework for Textual Out-of-distribution Detection](https://arxiv.org/abs/2404.06217)\n",
      "85. [EPL: Evidential Prototype Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2404.06181)\n",
      "86. [Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2404.06177)\n",
      "87. [Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis](https://arxiv.org/abs/2404.05997)\n",
      "88. [Masked Modeling Duo: Towards a Universal Audio Pre-training Framework](https://arxiv.org/abs/2404.06095)\n",
      "89. [LATUP-Net: A Lightweight 3D Attention U-Net with Parallel Convolutions for Brain Tumor Segmentation](https://arxiv.org/abs/2404.05911)\n"
     ]
    }
   ],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "\n",
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=40)\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
