{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_in_file(file_path, cnt=1):\n",
    "    output_lines = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            url, title = line.strip().split(\" | \")\n",
    "            paper_title = title.split(\"] \")[1]\n",
    "            formatted_line = f\"{i+cnt+1}. [{paper_title}]({url})\"\n",
    "            output_lines.append(formatted_line)\n",
    "            \n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "def result_write_md(markdown_file, output_lines):\n",
    "    with open(markdown_file, 'a') as m_file:\n",
    "        m_file.seek(0, 2)\n",
    "        if m_file.tell() > 0:\n",
    "            m_file.write('\\n')\n",
    "        m_file.write(output_lines)\n",
    "    print(\"Susseccfully write to file!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. [Mamba State-Space Models Can Be Strong Downstream Learners](https://arxiv.org/abs/2406.00209)\n",
      "2. [A Survey of Deep Learning Audio Generation Methods](https://arxiv.org/abs/2406.00146)\n",
      "3. [Anomaly Detection in Dynamic Graphs: A Comprehensive Survey](https://arxiv.org/abs/2406.00134)\n",
      "4. [Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting](https://arxiv.org/abs/2406.00053)\n",
      "5. [LocMoE+: Enhanced Router with Token Feature Awareness for Efficient LLM Pre-Training](https://arxiv.org/abs/2406.00023)\n",
      "6. [An Early Investigation into the Utility of Multimodal Large Language Models in Medical Imaging](https://arxiv.org/abs/2406.00667)\n",
      "7. [Dual Hyperspectral Mamba for Efficient Spectral Compressive Imaging](https://arxiv.org/abs/2406.00449)\n",
      "8. [Hybrid attention structure preserving network for reconstruction of under-sampled OCT images](https://arxiv.org/abs/2406.00279)\n",
      "9. [A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases](https://arxiv.org/abs/2406.00237)\n",
      "10. [Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights](https://arxiv.org/abs/2405.21070)\n",
      "11. [Spectrum-Aware Parameter Efficient Fine-Tuning for Diffusion Models](https://arxiv.org/abs/2405.21050)\n",
      "12. [A-PETE: Adaptive Prototype Explanations of Tree Ensembles](https://arxiv.org/abs/2405.21036)\n",
      "13. [Hard Cases Detection in Motion Prediction by Vision-Language Foundation Models](https://arxiv.org/abs/2405.20991)\n",
      "14. [Ovis: Structural Embedding Alignment for Multimodal Large Language Model](https://arxiv.org/abs/2405.20797)\n",
      "15. [Information Theoretic Text-to-Image Alignment](https://arxiv.org/abs/2405.20759)\n",
      "16. [Language Augmentation in CLIP for Improved Anatomy Detection on Multi-modal Medical Images](https://arxiv.org/abs/2405.20735)\n",
      "17. [GenMix: Combining Generative and Mixture Data Augmentation for Medical Image Classification](https://arxiv.org/abs/2405.20650)\n",
      "18. [Large Language Models Enhanced Sequential Recommendation for Long-tail User and Item](https://arxiv.org/abs/2405.20646)\n",
      "19. [\"Forgetting\" in Machine Learning and Beyond: A Survey](https://arxiv.org/abs/2405.20620)\n",
      "20. [Textual Inversion and Self-supervised Refinement for Radiology Report Generation](https://arxiv.org/abs/2405.20607)\n",
      "21. [SimSAM: Zero-shot Medical Image Segmentation via Simulated Interaction](https://arxiv.org/abs/2406.00663)\n",
      "22. [Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance](https://arxiv.org/abs/2406.00644)\n",
      "23. [Transforming Computer Security and Public Trust Through the Exploration of Fine-Tuning Large Language Models](https://arxiv.org/abs/2406.00628)\n",
      "24. [LLMs Could Autonomously Learn Without External Supervision](https://arxiv.org/abs/2406.00606)\n",
      "25. [Memory-guided Network with Uncertainty-based Feature Augmentation for Few-shot Semantic Segmentation](https://arxiv.org/abs/2406.00545)\n",
      "26. [Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable Artificial Intelligence (XAI) Techniques](https://arxiv.org/abs/2406.00532)\n",
      "27. [On the Use of Anchoring for Training Vision Models](https://arxiv.org/abs/2406.00529)\n",
      "28. [Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection](https://arxiv.org/abs/2406.00510)\n",
      "29. [Posterior Label Smoothing for Node Classification](https://arxiv.org/abs/2406.00410)\n",
      "30. [DeCoOp: Robust Prompt Tuning with Out-of-Distribution Detection](https://arxiv.org/abs/2406.00345)\n",
      "31. [Less is More: Pseudo-Label Filtering for Continual Test-Time Adaptation](https://arxiv.org/abs/2406.02609)\n",
      "32. [Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis](https://arxiv.org/abs/2406.03430)\n",
      "33. [Multi-Task Multi-Scale Contrastive Knowledge Distillation for Efficient Medical Image Segmentation](https://arxiv.org/abs/2406.03173)\n",
      "34. [Leveraging Visual Tokens for Extended Text Contexts in Multi-Modal Learning](https://arxiv.org/abs/2406.02547)\n",
      "35. [To Believe or Not to Believe Your LLM](https://arxiv.org/abs/2406.02543)\n",
      "36. [Parrot: Multilingual Visual Instruction Tuning](https://arxiv.org/abs/2406.02539)\n",
      "37. [Generative Active Learning for Long-tailed Instance Segmentation](https://arxiv.org/abs/2406.02435)\n",
      "38. [Harnessing Neural Unit Dynamics for Effective and Scalable Class-Incremental Learning](https://arxiv.org/abs/2406.02428)\n",
      "39. [Flash Diffusion: Accelerating Any Conditional Diffusion Model for Few Steps Image Generation](https://arxiv.org/abs/2406.02347)\n",
      "40. [Continual Unsupervised Out-of-Distribution Detection](https://arxiv.org/abs/2406.02327)\n",
      "41. [Can CLIP help CLIP in learning 3D?](https://arxiv.org/abs/2406.02202)\n",
      "42. [Enhance Image-to-Image Generation with LLaVA Prompt and Negative Prompt](https://arxiv.org/abs/2406.01956)\n",
      "43. [An Empirical Study of Excitation and Aggregation Design Adaptions in CLIP4Clip for Video-Text Retrieval](https://arxiv.org/abs/2406.01604)\n",
      "44. [Fairness Evolution in Continual Learning for Medical Imaging](https://arxiv.org/abs/2406.02480)\n",
      "45. [Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP](https://arxiv.org/abs/2406.01583)\n",
      "46. [Long and Short Guidance in Score identity Distillation for One-Step Text-to-Image Generation](https://arxiv.org/abs/2406.01561)\n",
      "47. [ED-SAM: An Efficient Diffusion Sampling Approach to Domain Generalization in Vision-Language Foundation Models](https://arxiv.org/abs/2406.01432)\n",
      "48. [FreeTumor: Advance Tumor Segmentation via Large-Scale Tumor Synthesis](https://arxiv.org/abs/2406.01264)\n",
      "49. [GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer](https://arxiv.org/abs/2406.01210)\n",
      "50. [Scaling Up Deep Clustering Methods Beyond ImageNet-1K](https://arxiv.org/abs/2406.01203)\n",
      "51. [Zero-Shot Out-of-Distribution Detection with Outlier Label Exposure](https://arxiv.org/abs/2406.01170)\n",
      "52. [MultiEdits: Simultaneous Multi-Aspect Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2406.00985)\n",
      "53. [Navigating Conflicting Views: Harnessing Trust for Learning](https://arxiv.org/abs/2406.00958)\n",
      "54. [Improving Segment Anything on the Fly: Auxiliary Online Learning and Adaptive Fusion for Medical Image Segmentation](https://arxiv.org/abs/2406.00956)\n",
      "55. [A Survey of Useful LLM Evaluation](https://arxiv.org/abs/2406.00936)\n",
      "56. [Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection](https://arxiv.org/abs/2406.00806)\n",
      "57. [Task-oriented Embedding Counts: Heuristic Clustering-driven Feature Fine-tuning for Whole Slide Image Classification](https://arxiv.org/abs/2406.00672)\n",
      "58. [Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2406.00670)\n",
      "59. [Sample-specific Masks for Visual Reprogramming-based Prompting](https://arxiv.org/abs/2406.03150)\n",
      "60. [Tiny models from tiny data: Textual and null-text inversion for few-shot distillation](https://arxiv.org/abs/2406.03146)\n",
      "61. [Continual Traffic Forecasting via Mixture of Experts](https://arxiv.org/abs/2406.03140)\n",
      "62. [Decision Boundary-aware Knowledge Consolidation Generates Better Instance-Incremental Learner](https://arxiv.org/abs/2406.03065)\n",
      "63. [Visual-Text Cross Alignment: Refining the Similarity Score in Vision-Language Models](https://arxiv.org/abs/2406.02915)\n",
      "64. [Choice of PEFT Technique in Continual Learning: Prompt Tuning is Not All You Need](https://arxiv.org/abs/2406.03216)\n",
      "65. [Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach](https://arxiv.org/abs/2406.03411)\n",
      "66. [Noisy Data Visualization using Functional Data Analysis](https://arxiv.org/abs/2406.03396)\n",
      "67. [Learning Visual Prompts for Guiding the Attention of Vision Transformers](https://arxiv.org/abs/2406.03303)\n",
      "68. [Verbalized Machine Learning: Revisiting Machine Learning with Language Models](https://arxiv.org/abs/2406.04344)\n",
      "69. [Coarse-To-Fine Tensor Trains for Compact Visual Representations](https://arxiv.org/abs/2406.04332)\n",
      "70. [The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning](https://arxiv.org/abs/2406.04328)\n",
      "71. [Semantically Diverse Language Generation for Uncertainty Estimation in Language Models](https://arxiv.org/abs/2406.04306)\n",
      "72. [Vision-LSTM: xLSTM as Generic Vision Backbone](https://arxiv.org/abs/2406.04303)\n",
      "73. [What is Dataset Distillation Learning?](https://arxiv.org/abs/2406.04284)\n",
      "74. [Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks](https://arxiv.org/abs/2406.04276)\n",
      "75. [Matching Anything by Segmenting Anything](https://arxiv.org/abs/2406.04221)\n",
      "76. [CDMamba: Remote Sensing Image Change Detection with Mamba](https://arxiv.org/abs/2406.04207)\n",
      "77. [Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis](https://arxiv.org/abs/2406.04032)\n",
      "78. [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt](https://arxiv.org/abs/2406.04031)\n",
      "79. [Frequency-based Matcher for Long-tailed Semantic Segmentation](https://arxiv.org/abs/2406.03917)\n",
      "80. [BLSP-Emo: Towards Empathetic Large Speech-Language Models](https://arxiv.org/abs/2406.03872)\n",
      "81. [Low-Rank Similarity Mining for Multimodal Dataset Distillation](https://arxiv.org/abs/2406.03793)\n",
      "82. [Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning](https://arxiv.org/abs/2406.03792)\n",
      "83. [FastGAS: Fast Graph-based Annotation Selection for In-Context Learning](https://arxiv.org/abs/2406.03730)\n",
      "84. [Is Free Self-Alignment Possible?](https://arxiv.org/abs/2406.03642)\n",
      "85. [CountCLIP -- [Re](https://arxiv.org/abs/2406.03586)\n",
      "86. [LLMs Meet Multimodal Generation and Editing: A Survey](https://arxiv.org/abs/2405.19334)\n",
      "87. [C^2RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2406.03902)\n",
      "88. [Synthetic Oversampling: Theory and A Practical Approach Using LLMs to Address Data Imbalance](https://arxiv.org/abs/2406.03628)\n",
      "89. [Wings: Learning Multimodal LLMs without Text-only Forgetting](https://arxiv.org/abs/2406.03496)\n",
      "90. [LW-DETR: A Transformer Replacement to YOLO for Real-Time Detection](https://arxiv.org/abs/2406.03459)\n"
     ]
    }
   ],
   "source": [
    "# 请将 'path_to_your_file.txt' 替换为你的txt文件路径\n",
    "file_path = 'tmp.txt'\n",
    "converted_text = convert_text_in_file(file_path, cnt=0)\n",
    "\n",
    "print(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susseccfully write to file!\n"
     ]
    }
   ],
   "source": [
    "# 写入Readme.md文件\n",
    "markdown_file = 'README.md'\n",
    "output_lines = converted_text\n",
    "result_write_md(markdown_file, output_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susseccfully write to file!\n"
     ]
    }
   ],
   "source": [
    "# 写入本周的markdown文件\n",
    "markdown_file = '2024/23_week.md'\n",
    "output_lines = converted_text\n",
    "result_write_md(markdown_file, output_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyt38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "176c92cd5e5485b42e3e73d2f25e60c2ed8f24d584687c4dae8d36d44ade605d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
