# Awesome_arXiv_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).

## Recommended Papers for Week 52, 2023
0. [Point Transformer V3: Simpler, Faster, Stronger](https://arxiv.org/abs/2312.10035)
1. [Osprey: Pixel Understanding with Visual Instruction Tuning](https://arxiv.org/abs/2312.10032)
2. [Challenges with unsupervised LLM knowledge discovery](https://arxiv.org/abs/2312.10029)
3. [Diagnosing and Rectifying Fake OOD Invariance: A Restructured Causal Approach](https://arxiv.org/abs/2312.09758)
4. [Weakly-Supervised 3D Visual Grounding based on Visual Linguistic Alignment](https://arxiv.org/abs/2312.09625)
5. [MobileSAMv2: Faster Segment Anything to Everything](https://arxiv.org/abs/2312.09579)
6. [Investigating Responsible AI for Scientific Research: An Empirical Study](https://arxiv.org/abs/2312.09561)
7. [Prompt-based Distribution Alignment for Unsupervised Domain Adaptation](https://arxiv.org/abs/2312.09553)
8. [Adaptive Integration of Partial Label Learning and Negative Learning for Enhanced Noisy Label Learning](https://arxiv.org/abs/2312.09505)
9. [Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks](https://arxiv.org/abs/2312.09469)
10. [Adaptive Integration of Partial Label Learning and Negative Learning for Enhanced Noisy Label Learning](https://arxiv.org/abs/2312.09505)
11. [SQA-SAM: Segmentation Quality Assessment for Medical Images Utilizing the Segment Anything Model](https://arxiv.org/abs/2312.09899)
12. [Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning](https://arxiv.org/abs/2312.11420)
13. [Leveraging Normalization Layer in Adapters With Progressive Learning and Adaptive Distillation for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2312.11260)
14. [UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts](https://arxiv.org/abs/2312.11171)
15. [Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters](https://arxiv.org/abs/2312.10813)
16. [Addressing Sample Inefficiency in Multi-View Representation Learning](https://arxiv.org/abs/2312.10725)
17. [Pedestrian Attribute Recognition via CLIP based Prompt Vision-Language Fusion](https://arxiv.org/abs/2312.10692)
18. [Out-of-Distribution Detection in Long-Tailed Recognition with Calibrated Outlier Class Learning](https://arxiv.org/abs/2312.10686)
19. [Silkie: Preference Distillation for Large Visual Language Models](https://arxiv.org/abs/2312.10665)
20. [An Evaluation of GPT-4V and Gemini in Online VQA](https://arxiv.org/abs/2312.10637)
21. [Cut your annotation cost: An empirical study on the use of weak, noisy, and SAM-generated annotations for segmentation network training](https://arxiv.org/abs/2312.10600)
22. [Simple Image-level Classification Improves Open-vocabulary Object Detection](https://arxiv.org/abs/2312.10439)
23. [RetailKLIP : Finetuning OpenCLIP backbone using metric learning on a single GPU for Zero-shot retail product image classification](https://arxiv.org/abs/2312.10282)
24. [GPT-doctor: Customizing Large Language Models for Medical Consultation](https://arxiv.org/abs/2312.10225)
25. [Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey](https://arxiv.org/abs/2312.10163)
26. [FoMo-Bench: a multi-modal, multi-scale and multi-task Forest Monitoring Benchmark for remote sensing foundation models](https://arxiv.org/abs/2312.10114)
27. [Deep Metric Learning for Computer Vision: A Brief Overview](https://arxiv.org/abs/2312.10046)
28. [UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts](https://arxiv.org/abs/2312.11171)
29. [UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray Classification](https://arxiv.org/abs/2312.11171)
30. [LaViP:Language-Grounded Visual Prompts](https://arxiv.org/abs/2312.11038)
31. [Toward enriched Cognitive Learning with XAI](https://arxiv.org/abs/2312.10945)
32. [Roll With the Punches: Expansion and Shrinkage of Soft Label Selection for Semi-supervised Fine-Grained Learning](https://arxiv.org/abs/2312.12237)
33. [Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model](https://arxiv.org/abs/2312.12232)
34. [PEPT: Expert Finding Meets Personalized Pre-training](https://arxiv.org/abs/2312.12162)
35. [Exploring the Residual Stream of Transformers](https://arxiv.org/abs/2312.12141)
36. [VITA: 'Carefully Chosen and Weighted Less' Is Better in Medication Recommendation](https://arxiv.org/abs/2312.12100)
37. [Can ChatGPT be Your Personal Medical Assistant?](https://arxiv.org/abs/2312.12006)
38. [Diffusing More Objects for Semi-Supervised Domain Adaptation with Less Labeling](https://arxiv.org/abs/2312.12000)
39. [Sparse is Enough in Fine-tuning Pre-trained Large Language Model](https://arxiv.org/abs/2312.11875)
40. [Beyond Prototypes: Semantic Anchor Regularization for Better Representation Learning](https://arxiv.org/abs/2312.11872)
41. [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)
42. [Learning Object State Changes in Videos: An Open-World Perspective](https://arxiv.org/abs/2312.11782)
43. [Understanding the Multi-modal Prompts of the Pre-trained Vision-Language Model](https://arxiv.org/abs/2312.11570)
44. [A Survey of Reasoning with Foundation Models](https://arxiv.org/abs/2312.11562)
45. [CLIPSyntel: CLIP and LLM Synergy for Multimodal Question Summarization in Healthcare](https://arxiv.org/abs/2312.11541)
46. [Fast Decision Boundary based Out-of-Distribution Detector](https://arxiv.org/abs/2312.11536)
47. [Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model](https://arxiv.org/abs/2312.12423)
48. [Weakly Supervised Open-Vocabulary Object Detection](https://arxiv.org/abs/2312.12437)
49. [A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise](https://arxiv.org/abs/2312.12436)
50. [Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models](https://arxiv.org/abs/2312.12416)
51. [Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning](https://arxiv.org/abs/2312.12379)
52. [Visual AI and Linguistic Intelligence Through Steerability and Composability](https://arxiv.org/abs/2312.12383)
53. [CLIP-DINOiser: Teaching CLIP a few DINO tricks](https://arxiv.org/abs/2312.12359)
54. [Tracking Any Object Amodally  ](https://arxiv.org/abs/2312.12433)
55. [Generative Multimodal Models are In-Context Learners](https://arxiv.org/abs/2312.13286)
56. [Conditional Image Generation with Pretrained Generative Model](https://arxiv.org/abs/2312.13253)
57. [LlaMaVAE: Guiding Large Language Model Generation via Continuous Latent Sentence Spaces](https://arxiv.org/abs/2312.13208)
58. [SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized Zero-Shot Learning](https://arxiv.org/abs/2312.13100)
59. [TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training](https://arxiv.org/abs/2312.12828)
60. [Spectral Prompt Tuning:Unveiling Unseen Classes for Zero-Shot Semantic Segmentation](https://arxiv.org/abs/2312.12754)
61. [MetaSegNet: Metadata-collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images](https://arxiv.org/abs/2312.12735)
62. [Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation](https://arxiv.org/abs/2312.12470)