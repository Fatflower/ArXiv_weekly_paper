# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)|[25_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/25_week.md)|[26_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/26_week.md)|[27_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/27_week.md)|[28_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/28_week.md)|[29_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/29_week.md)|[30_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/30_week.md)|[31_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/31_week.md)|[32_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/32_week.md)



<!-- | | | | | -->

## Recommended Papers for Week 32, 2024
1. [Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)](https://arxiv.org/abs/2408.00932)
2. [Medical SAM 2: Segment medical images as video via Segment Anything Model 2](https://arxiv.org/abs/2408.00874)
3. [PINNs for Medical Image Analysis: A Survey](https://arxiv.org/abs/2408.01026)
4. [Conditional LoRA Parameter Generation](https://arxiv.org/abs/2408.01415)
5. [Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer](https://arxiv.org/abs/2408.01402)
6. [NOLO: Navigate Only Look Once](https://arxiv.org/abs/2408.01384)
7. [Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2408.01356)
8. [Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs](https://arxiv.org/abs/2408.01355)
9. [A Survey of Mamba](https://arxiv.org/abs/2408.01129)
10. [Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer](https://arxiv.org/abs/2408.01119)
11. [Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning](https://arxiv.org/abs/2408.01076)
12. [Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model](https://arxiv.org/abs/2408.01044)
13. [POA: Pre-training Once for Models of All Sizes](https://arxiv.org/abs/2408.01031)
14. [FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation](https://arxiv.org/abs/2408.00998)
15. [Prototype Learning for Micro-gesture Classification](https://arxiv.org/abs/2408.03097)
16. [Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond](https://arxiv.org/abs/2408.02983)
17. [The Need for a Big World Simulator: A Scientific Challenge for Continual Learning](https://arxiv.org/abs/2408.02930)
18. [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)
19. [Biomedical SAM 2: Segment Anything in Biomedical Images and Videos](https://arxiv.org/abs/2408.03286)
20. [Learning to Learn without Forgetting using Attention](https://arxiv.org/abs/2408.03219)
21. [How Well Can Vision Language Models See Image Details?](https://arxiv.org/abs/2408.03940)
22. [AdapMTL: Adaptive Pruning Framework for Multitask Learning Model](https://arxiv.org/abs/2408.03913)
23. [Target Prompting for Information Extraction with Vision Language Model](https://arxiv.org/abs/2408.03834)
24. [AI Foundation Models in Remote Sensing: A Survey](https://arxiv.org/abs/2408.03464)
25. [SAM2-PATH: A better segment anything model for semantic segmentation in digital pathology](https://arxiv.org/abs/2408.03651)
26. [Distillation Learning Guided by Image Reconstruction for One-Shot Medical Image Segmentation](https://arxiv.org/abs/2408.03616)