# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)

<!-- | | | | | -->

## Recommended Papers for Week 12, 2024
1. [CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning](https://arxiv.org/abs/2403.10245)
2. [Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks](https://arxiv.org/abs/2403.10097)
3. [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
4. [Group-Mix SAM: Lightweight Solution for Industrial Assembly Line Applications](https://arxiv.org/abs/2403.10053)
5. [TextBlockV2: Towards Precise-Detection-Free Scene Text Spotting with Pre-trained Language Model](https://arxiv.org/abs/2403.10047)
6. [Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument Segmentation](https://arxiv.org/abs/2403.10039)
7. [EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba](https://arxiv.org/abs/2403.09977)
8. [GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery](https://arxiv.org/abs/2403.09974)
9. [RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training](https://arxiv.org/abs/2403.09948)
10. [MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation](https://arxiv.org/abs/2403.09850)
11. [An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models](https://arxiv.org/abs/2403.09766)
12. [COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced Medical Image Representation](https://arxiv.org/abs/2403.09672)
13. [Cardiac Magnetic Resonance 2D+T Short- and Long-axis Segmentation via Spatio-temporal SAM Adaptation](https://arxiv.org/abs/2403.10009)
14. [Frozen Feature Augmentation for Few-Shot Image Classification](https://arxiv.org/abs/2403.10519)
15. [Energy Correction Model in the Feature Space for Out-of-Distribution Detection](https://arxiv.org/abs/2403.10403)
16. [CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning](https://arxiv.org/abs/2403.10391)
17. [Investigating grammatical abstraction in language models using few-shot learning of novel noun gender](https://arxiv.org/abs/2403.10338)
18. [Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models](https://arxiv.org/abs/2403.10287)
19. [Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks](https://arxiv.org/abs/2403.10097)
20. [Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt](https://arxiv.org/abs/2403.09857)
21. [FastSAM3D: An Efficient Segment Anything Model for 3D Volumetric Medical Images](https://arxiv.org/abs/2403.09827)
22. [Distilling Datasets Into Less Than One Image](https://arxiv.org/abs/2403.12040)
23. [Zero-Shot Image Feature Consensus with Deep Functional Maps](https://arxiv.org/abs/2403.12038)
24. [One-Step Image Translation with Text-to-Image Models](https://arxiv.org/abs/2403.12036)
25. [Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2403.12030)
26. [SuperLoRA: Parameter-Efficient Unified Adaptation of Multi-Layer Attention Modules](https://arxiv.org/abs/2403.11887)
27. [Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation](https://arxiv.org/abs/2403.11808)
28. [LSKNet: A Foundation Lightweight Backbone for Remote Sensing](https://arxiv.org/abs/2403.11735)
29. [Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for Unsupervised Anomaly Detection](https://arxiv.org/abs/2403.11667)
30. [MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks](https://arxiv.org/abs/2403.11646)
31. [LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models](https://arxiv.org/abs/2403.11627)
32. [CRS-Diff: Controllable Generative Remote Sensing Foundation Model](https://arxiv.org/abs/2403.11614)
33. [A survey of synthetic data augmentation methods in computer vision](https://arxiv.org/abs/2403.10075)
34. [Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models](https://arxiv.org/abs/2403.12966)
35. [Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language Models](https://arxiv.org/abs/2403.12964)
36. [MEDBind: Unifying Language and Multimodal Medical Data Embeddings](https://arxiv.org/abs/2403.12894)
37. [RelationVLM: Making Large Vision-Language Models Understand Visual Relations](https://arxiv.org/abs/2403.12801)
38. [DreamDA: Generative Data Augmentation with Diffusion Models](https://arxiv.org/abs/2403.12803)
39. [Inter- and intra-uncertainty based feature aggregation model for semi-supervised histopathology image segmentation](https://arxiv.org/abs/2403.12767)
40. [Towards Multimodal In-Context Learning for Vision & Language Models](https://arxiv.org/abs/2403.12736)
41. [Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection](https://arxiv.org/abs/2403.12580)
42. [Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images](https://arxiv.org/abs/2403.12570)
43. [Confidence Self-Calibration for Multi-Label Class-Incremental Learning](https://arxiv.org/abs/2403.12559)
44. [UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All](https://arxiv.org/abs/2403.12532)
45. [CLIP-VIS: Adapting CLIP for Open-Vocabulary Video Instance Segmentation](https://arxiv.org/abs/2403.12455)
46. [DMAD: Dual Memory Bank for Real-World Anomaly Detection](https://arxiv.org/abs/2403.12362)
47. [A Dataset and Benchmark for Copyright Protection from Text-to-Image Diffusion Models](https://arxiv.org/abs/2403.12052)
48. [TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer](https://arxiv.org/abs/2403.12481)
49. [Non-negative Contrastive Learning](https://arxiv.org/abs/2403.12459)
50. [Do Generated Data Always Help Contrastive Learning?](https://arxiv.org/abs/2403.12448)
51. [Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts](https://arxiv.org/abs/2403.12326)
52. [Editing Massive Concepts in Text-to-Image Diffusion Models](https://arxiv.org/abs/2403.13807)
53. [Learning from Models and Data for Visual Grounding](https://arxiv.org/abs/2403.13804)
54. [ZigMa: Zigzag Mamba Diffusion Model](https://arxiv.org/abs/2403.13802)
55. [SPTNet: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning](https://arxiv.org/abs/2403.13684)
56. [Retina Vision Transformer (RetinaViT): Introducing Scaled Patches into Vision Transformers](https://arxiv.org/abs/2403.13677)
57. [ProMamba: Prompt-Mamba for polyp segmentation](https://arxiv.org/abs/2403.13660)
58. [Meta-Point Learning and Refining for Category-Agnostic Pose Estimation](https://arxiv.org/abs/2403.13647)
59. [H-vmunet: High-order Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2403.13642)
60. [VL-Mamba: Exploring State Space Models for Multimodal Learning](https://arxiv.org/abs/2403.13600)
61. [IDAdapter: Learning Mixed Features for Tuning-Free Personalization of Text-to-Image Models](https://arxiv.org/abs/2403.13535)
62. [Scale Decoupled Distillation](https://arxiv.org/abs/2403.13512)
63. [MTP: Advancing Remote Sensing Foundation Model via Multi-Task Pretraining](https://arxiv.org/abs/2403.13430)
64. [Out-of-Distribution Detection Using Peer-Class Generated by Large Language Model](https://arxiv.org/abs/2403.13324)
65. [Improved EATFormer: A Vision Transformer for Medical Image Classification](https://arxiv.org/abs/2403.13167)
66. [HuLP: Human-in-the-Loop for Prognosis](https://arxiv.org/abs/2403.13078)
67. [TAPTR: Tracking Any Point with Transformers as Detection](https://arxiv.org/abs/2403.13042)
68. [BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced Feature-Level Contrastive Learning](https://arxiv.org/abs/2403.12986)
69. [Bridge the Modality and Capacity Gaps in Vision-Language Model Selection](https://arxiv.org/abs/2403.13797)
70. [REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning](https://arxiv.org/abs/2403.13522)
71. [CLIPSwarm: Generating Drone Shows from Text Prompts with Vision-Language Models](https://arxiv.org/abs/2403.13467)
72. [HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models](https://arxiv.org/abs/2403.13447)
73. [Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified Anomaly Detection](https://arxiv.org/abs/2403.13349)
74. [A Unified and General Framework for Continual Learning](https://arxiv.org/abs/2403.13249)
75. [Trustworthiness of Pretrained Transformers for Lung Cancer Segmentation](https://arxiv.org/abs/2403.13113)
76. [Hierarchical Text-to-Vision Self Supervised Alignment for Improved Histopathology Representation Learning](https://arxiv.org/abs/2403.14616)
77. [T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy](https://arxiv.org/abs/2403.14610)
78. [MyVLM: Personalizing VLMs for User-Specific Queries](https://arxiv.org/abs/2403.14599)
79. [PSALM: Pixelwise SegmentAtion with Large Multi-Modal Model](https://arxiv.org/abs/2403.14598)
80. [DINO-Tracker: Taming DINO for Self-Supervised Point Tracking in a Single Video](https://arxiv.org/abs/2403.14548)
81. [Learning to Project for Cross-Task Knowledge Distillation](https://arxiv.org/abs/2403.14494)
82. [Analysing Diffusion Segmentation for Medical Images](https://arxiv.org/abs/2403.14440)
83. [A Bag of Tricks for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2403.14392)
84. [HySim: An Efficient Hybrid Similarity Measure for Patch Matching in Image Inpainting](https://arxiv.org/abs/2403.14292)
85. [Toward Multi-class Anomaly Detection: Exploring Class-aware Unified Model against Inter-class Interference](https://arxiv.org/abs/2403.14213)
86. [Unsupervised Audio-Visual Segmentation with Modality Alignment](https://arxiv.org/abs/2403.14203)
87. [Volumetric Environment Representation for Vision-Language Navigation](https://arxiv.org/abs/2403.14158)
88. [Empowering Segmentation Ability to Multi-modal Large Language Models](https://arxiv.org/abs/2403.14141)
89. [Improving Image Classification Accuracy through Complementary Intra-Class and Inter-Class Mixup](https://arxiv.org/abs/2403.14137)
90. [C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion](https://arxiv.org/abs/2403.14119)
91. [MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation](https://arxiv.org/abs/2403.14103)