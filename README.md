# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)

<!-- | | | | | -->

## Recommended Papers for Week 5, 2024
1. [From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities](https://arxiv.org/abs/2401.15071)
2. [Masked Pre-trained Model Enables Universal Zero-shot Denoiser](https://arxiv.org/abs/2401.14966)
3. [Memory-Inspired Temporal Prompt Interaction for Text-Image Classification](https://arxiv.org/abs/2401.14856)
4. [TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts](https://arxiv.org/abs/2401.14828)
5. [PL-FSCIL: Harnessing the Power of Prompts for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2401.14807)
6. [SSR: SAM is a Strong Regularizer for domain adaptive semantic segmentation](https://arxiv.org/abs/2401.14686)
7. [Revisiting Active Learning in the Era of Vision Foundation Models](https://arxiv.org/abs/2401.14555)
8. [K-QA: A Real-World Medical Q&A Benchmark](https://arxiv.org/abs/2401.14493)
9. [Strategic Usage in a Multi-Learner Setting](https://arxiv.org/abs/2401.16422)
10. [InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model](https://arxiv.org/abs/2401.16420)
11. [Scaling Sparse Fine-Tuning to Large Language Models](https://arxiv.org/abs/2401.16405)
12. [Continual Learning with Pre-Trained Models: A Survey](https://arxiv.org/abs/2401.16386)
13. [Spatial-Aware Latent Initialization for Controllable Image Generation](https://arxiv.org/abs/2401.16157)
14. [Sample Weight Estimation Using Meta-Updates for Online Continual Learning](https://arxiv.org/abs/2401.15973)
15. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2401.15947)
16. [Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization](https://arxiv.org/abs/2401.15914)
17. [Cross-Scale MAE: A Tale of Multi-Scale Exploitation in Remote Sensing](https://arxiv.org/abs/2401.15855)
18. [Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes](https://arxiv.org/abs/2401.15834)
19. [Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation](https://arxiv.org/abs/2401.15688)
20. [Data-Free Generalized Zero-Shot Learning](https://arxiv.org/abs/2401.15657)
21. [IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models](https://arxiv.org/abs/2401.15559)
22. [Dynamic Transformer Architecture for Continual Learning of Multimodal Tasks](https://arxiv.org/abs/2401.15275)
23. [HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy](https://arxiv.org/abs/2401.15207)
24. [Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive Learning](https://arxiv.org/abs/2401.15111)
25. [YOLO-World: Real-Time Open-Vocabulary Object Detection](https://arxiv.org/abs/2401.17270)
26. [ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models](https://arxiv.org/abs/2401.17267)
27. [Weaver: Foundation Models for Creative Writing](https://arxiv.org/abs/2401.17268)
28. [Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning](https://arxiv.org/abs/2401.17186)
29. [ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained Visual Categorization](https://arxiv.org/abs/2401.17050)
30. [EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain](https://arxiv.org/abs/2401.16822)
31. [Pick-and-Draw: Training-free Semantic Guidance for Text-to-Image Personalization](https://arxiv.org/abs/2401.16762)
32. [MESA: Matching Everything by Segmenting Anything](https://arxiv.org/abs/2401.16741)
33. [Depth Anything in Medical Images: A Comparative Study](https://arxiv.org/abs/2401.16600)
34. [Binding Touch to Everything: Learning Unified Multimodal Tactile Representations](https://arxiv.org/abs/2401.18084)
35. [Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?](https://arxiv.org/abs/2401.18070)
36. [Paramanu: A Family of Novel Efficient Indic Generative Foundation Language Models](https://arxiv.org/abs/2401.18034)
37. [Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation](https://arxiv.org/abs/2401.17904)
38. [Convolution Meets LoRA: Parameter Efficient Finetuning for Segment Anything Model](https://arxiv.org/abs/2401.17868)
39. [Leveraging Swin Transformer for Local-to-Global Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2401.17828)
40. [Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation](https://arxiv.org/abs/2401.17664)
41. [All Beings Are Equal in Open Set Recognition](https://arxiv.org/abs/2401.17654)
42. [Assertion Detection Large Language Model In-context Learning LoRA Fine-tuning](https://arxiv.org/abs/2401.17602)
43. [Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data](https://arxiv.org/abs/2401.17600)
44. [Local Feature Matching Using Deep Learning: A Survey](https://arxiv.org/abs/2401.17592)
45. [Data-Effective Learning: A Comprehensive Medical Benchmark](https://arxiv.org/abs/2401.17542)
46. [Customizing Language Model Responses with Contrastive In-Context Learning](https://arxiv.org/abs/2401.17390)
47. [Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection](https://arxiv.org/abs/2402.00865)
48. [Evaluating Large Language Models for Generalization and Robustness via Data Compression](https://arxiv.org/abs/2402.00861)
49. [Can Large Language Models Understand Context?](https://arxiv.org/abs/2402.00858)
50. [Unlearnable Algorithms for In-context Learning](https://arxiv.org/abs/2402.00751)
51. [Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data](https://arxiv.org/abs/2402.00743)
52. [Dynamic Texture Transfer using PatchMatch and Transformers](https://arxiv.org/abs/2402.00606)
53. [Continuous Unsupervised Domain Adaptation Using Stabilized Representations and Experience Replay](https://arxiv.org/abs/2402.00580)
54. [Bias Mitigating Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2402.00481)
55. [Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection](https://arxiv.org/abs/2402.00448)
56. [CPT: Competence-progressive Training Strategy for Few-shot Node Classification](https://arxiv.org/abs/2402.00450)
57. [Efficient Exploration for LLMs](https://arxiv.org/abs/2402.00396)
58. [A Survey on Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2402.00253)
59. [Geometry aware 3D generation from in-the-wild images in ImageNet](https://arxiv.org/abs/2402.00225)
60. [The Impact of Language Adapters in Cross-Lingual Transfer for NLU](https://arxiv.org/abs/2402.00149)
61. [Episodic-free Task Selection for Few-shot Learning](https://arxiv.org/abs/2402.00092)
62. [Detecting Multimedia Generated by Large AI Models: A Survey](https://arxiv.org/abs/2402.00045)
63. [LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition](https://arxiv.org/abs/2402.00033)