# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)|[25_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/25_week.md)|[26_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/26_week.md)|[27_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/27_week.md)|[28_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/28_week.md)|[29_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/29_week.md)|[30_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/30_week.md)|[31_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/31_week.md)|[32_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/32_week.md)



<!-- | | | | | -->

## Recommended Papers for Week 32, 2024
1. [Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)](https://arxiv.org/abs/2408.00932)
2. [Medical SAM 2: Segment medical images as video via Segment Anything Model 2](https://arxiv.org/abs/2408.00874)
3. [PINNs for Medical Image Analysis: A Survey](https://arxiv.org/abs/2408.01026)
4. [Conditional LoRA Parameter Generation](https://arxiv.org/abs/2408.01415)
5. [Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer](https://arxiv.org/abs/2408.01402)
6. [NOLO: Navigate Only Look Once](https://arxiv.org/abs/2408.01384)
7. [Balanced Residual Distillation Learning for 3D Point Cloud Class-Incremental Semantic Segmentation](https://arxiv.org/abs/2408.01356)
8. [Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs](https://arxiv.org/abs/2408.01355)
9. [A Survey of Mamba](https://arxiv.org/abs/2408.01129)
10. [Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer](https://arxiv.org/abs/2408.01119)
11. [Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for Continual Learning](https://arxiv.org/abs/2408.01076)
12. [Boosting Gaze Object Prediction via Pixel-level Supervision from Vision Foundation Model](https://arxiv.org/abs/2408.01044)
13. [POA: Pre-training Once for Models of All Sizes](https://arxiv.org/abs/2408.01031)
14. [FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation](https://arxiv.org/abs/2408.00998)
15. [Prototype Learning for Micro-gesture Classification](https://arxiv.org/abs/2408.03097)
16. [Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond](https://arxiv.org/abs/2408.02983)
17. [The Need for a Big World Simulator: A Scientific Challenge for Continual Learning](https://arxiv.org/abs/2408.02930)
18. [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)
19. [Biomedical SAM 2: Segment Anything in Biomedical Images and Videos](https://arxiv.org/abs/2408.03286)
20. [Learning to Learn without Forgetting using Attention](https://arxiv.org/abs/2408.03219)
21. [How Well Can Vision Language Models See Image Details?](https://arxiv.org/abs/2408.03940)
22. [AdapMTL: Adaptive Pruning Framework for Multitask Learning Model](https://arxiv.org/abs/2408.03913)
23. [Target Prompting for Information Extraction with Vision Language Model](https://arxiv.org/abs/2408.03834)
24. [AI Foundation Models in Remote Sensing: A Survey](https://arxiv.org/abs/2408.03464)
25. [SAM2-PATH: A better segment anything model for semantic segmentation in digital pathology](https://arxiv.org/abs/2408.03651)
26. [Distillation Learning Guided by Image Reconstruction for One-Shot Medical Image Segmentation](https://arxiv.org/abs/2408.03616)
27. [Transformer Explainer: Interactive Learning of Text-Generative Models](https://arxiv.org/abs/2408.04619)
28. [Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models](https://arxiv.org/abs/2408.04594)
29. [Learn To Learn More Precisely](https://arxiv.org/abs/2408.04590)
30. [Unveiling the Power of Sparse Neural Networks for Feature Selection](https://arxiv.org/abs/2408.04583)
31. [SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More](https://arxiv.org/abs/2408.04579)
32. [How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression](https://arxiv.org/abs/2408.04532)
33. [What could go wrong? Discovering and describing failure modes in computer vision](https://arxiv.org/abs/2408.04471)
34. [MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models](https://arxiv.org/abs/2408.04388)
35. [AggSS: An Aggregated Self-Supervised Approach for Class-Incremental Learning](https://arxiv.org/abs/2408.04347)
36. [CoBooM: Codebook Guided Bootstrapping for Medical Image Representation Learning](https://arxiv.org/abs/2408.04262)
37. [ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive Language-Image Pre-traning Model](https://arxiv.org/abs/2408.04145)
38. [Is SAM 2 Better than SAM in Medical Image Segmentation?](https://arxiv.org/abs/2408.04212)
39. [Self-Taught Evaluators](https://arxiv.org/abs/2408.02666)
40. [Interactive 3D Medical Image Segmentation with SAM 2](https://arxiv.org/abs/2408.02635)
41. [Progressively Selective Label Enhancement for Language Model Alignment](https://arxiv.org/abs/2408.02599)
42. [BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba](https://arxiv.org/abs/2408.02600)
43. [Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs](https://arxiv.org/abs/2408.02571)
44. [Cross-Modality Clustering-based Self-Labeling for Multimodal Data Classification](https://arxiv.org/abs/2408.02568)
45. [UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model](https://arxiv.org/abs/2408.02503)
46. [Fairness and Bias Mitigation in Computer Vision: A Survey](https://arxiv.org/abs/2408.02464)
47. [FPT+: A Parameter and Memory Efficient Transfer Learning Method for High-resolution Medical Image Classification](https://arxiv.org/abs/2408.02426)
48. [Machine Learning Applications in Medical Prognostics: A Comprehensive Review](https://arxiv.org/abs/2408.02344)
49. [Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation](https://arxiv.org/abs/2408.02297)
50. [Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary Concepts](https://arxiv.org/abs/2408.02265)
51. [Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs](https://arxiv.org/abs/2408.02261)
52. [EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier Logits](https://arxiv.org/abs/2408.02052)
53. [Deep Spectral Methods for Unsupervised Ultrasound Image Interpretation](https://arxiv.org/abs/2408.02043)
54. [Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2408.02039)
55. [Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models](https://arxiv.org/abs/2408.02032)
56. [DeMansia: Mamba Never Forgets Any Tokens](https://arxiv.org/abs/2408.01986)
57. [AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion Model](https://arxiv.org/abs/2408.01960)
58. [CAF-YOLO: A Robust Framework for Multi-Scale Lesion Detection in Biomedical Imagery](https://arxiv.org/abs/2408.01897)
59. [TS-SAM: Fine-Tuning Segment-Anything Model for Downstream Tasks](https://arxiv.org/abs/2408.01835)
60. [Domain penalisation for improved Out-of-Distribution Generalisation](https://arxiv.org/abs/2408.01746)
61. [AVESFormer: Efficient Transformer Design for Real-Time Audio-Visual Segmentation](https://arxiv.org/abs/2408.01708)
62. [Bayesian Active Learning for Semantic Segmentation](https://arxiv.org/abs/2408.01694)
63. [VLG-CBM: Training Concept Bottleneck Models with Vision-Language Guidance](https://arxiv.org/abs/2408.01432)
64. [NuLite -- Lightweight and Fast Model for Nuclei Instance Segmentation and Classification](https://arxiv.org/abs/2408.01797)
65. [MedUHIP: Towards Human-In-the-Loop Medical Segmentation](https://arxiv.org/abs/2408.01620)