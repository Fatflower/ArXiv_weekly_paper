# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|

<!-- | | | | | -->

## Recommended Papers for Week 6, 2024
1. [Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting](https://arxiv.org/abs/2402.01440)
2. [From Words to Molecules: A Survey of Large Language Models in Chemistry](https://arxiv.org/abs/2402.01439)
3. [XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision](https://arxiv.org/abs/2402.01410)
4. [Continual Learning for Large Language Models: A Survey](https://arxiv.org/abs/2402.01364)
5. [FindingEmo: An Image Dataset for Emotion Recognition in the Wild](https://arxiv.org/abs/2402.01355)
6. [Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes](https://arxiv.org/abs/2402.01352)
7. [CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay](https://arxiv.org/abs/2402.01348)
8. [Can MLLMs Perform Text-to-Image In-Context Learning?](https://arxiv.org/abs/2402.01293)
9. [On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification](https://arxiv.org/abs/2402.01274)
10. [Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?](https://arxiv.org/abs/2402.01241)
11. [Segment Any Change](https://arxiv.org/abs/2402.01188)
12. [A Comprehensive Survey on 3D Content Generation](https://arxiv.org/abs/2402.01166)
13. [Efficient Prompt Caching via Embedding Similarity](https://arxiv.org/abs/2402.01173)
14. [A Single Simple Patch is All You Need for AI-generated Image Detection](https://arxiv.org/abs/2402.01123)
15. [A Survey for Foundation Models in Autonomous Driving](https://arxiv.org/abs/2402.01105)
16. [Compositional Generative Modeling: A Single Model is Not All You Need](https://arxiv.org/abs/2402.01103)
17. [Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning](https://arxiv.org/abs/2402.00910)
18. [VISION-MAE: A Foundation Model for Medical Image Segmentation and Classification](https://arxiv.org/abs/2402.01034)
19. [V-IRL: Grounding Virtual Intelligence in Real Life](https://arxiv.org/abs/2402.03310)
20. [HASSOD: Hierarchical Adaptive Self-Supervised Object Detection](https://arxiv.org/abs/2402.03311)
21. [Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?](https://arxiv.org/abs/2402.03305)
22. [Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://arxiv.org/abs/2402.03302)
23. [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303)
24. [Zero-shot Object-Level OOD Detection with Context-Aware Inpainting](https://arxiv.org/abs/2402.03292)
25. [InstanceDiffusion: Instance-level Control for Image Generation](https://arxiv.org/abs/2402.03290)
26. [Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2402.03286)
27. [Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models](https://arxiv.org/abs/2402.03284)
28. [CLIP Can Understand Depth](https://arxiv.org/abs/2402.03251)
29. [FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition](https://arxiv.org/abs/2402.03241)
30. [The Matrix: A Bayesian learning model for LLMs](https://arxiv.org/abs/2402.03175)
31. [RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification](https://arxiv.org/abs/2402.03166)
32. [Is Mamba Capable of In-Context Learning?](https://arxiv.org/abs/2402.03170)
33. [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/abs/2402.03094)
34. [Text-Guided Image Clustering](https://arxiv.org/abs/2402.02996)
35. [Retrieval-Augmented Score Distillation for Text-to-3D Generation](https://arxiv.org/abs/2402.02972)
36. [Kernel PCA for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02949)
37. [Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey](https://arxiv.org/abs/2402.02941)
38. [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)
39. [Causal Feature Selection for Responsible Machine Learning](https://arxiv.org/abs/2402.02696)
40. [Learning with Mixture of Prototypes for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02653)
41. [VM-UNet: Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2402.02491)
42. [Why are hyperbolic neural networks effective? A study on hierarchical representation capability](https://arxiv.org/abs/2402.02478)
43. [Deep Spectral Improvement for Unsupervised Image Instance Segmentation](https://arxiv.org/abs/2402.02474)
44. [BECLR: Batch Enhanced Contrastive Few-Shot Learning](https://arxiv.org/abs/2402.02444)
45. [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382)
46. [Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation](https://arxiv.org/abs/2402.02367)
47. [The Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364)
48. [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://arxiv.org/abs/2402.02347)
49. [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://arxiv.org/abs/2402.02340)
50. [Your Diffusion Model is Secretly a Certifiably Robust Classifier](https://arxiv.org/abs/2402.02316)
51. [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314)
52. [Polyp-DAM: Polyp segmentation via depth anything model](https://arxiv.org/abs/2402.02298)
53. [Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2402.02286)
54. [SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking](https://arxiv.org/abs/2402.02285)
55. [Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey](https://arxiv.org/abs/2402.02242)
56. [Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets](https://arxiv.org/abs/2402.02245)
57. [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://arxiv.org/abs/2402.02207)
58. [GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events](https://arxiv.org/abs/2402.02205)
59. [Prompting Diverse Ideas: Increasing AI Idea Variance](https://arxiv.org/abs/2402.01727)
60. [Are Large Language Models Good Prompt Optimizers?](https://arxiv.org/abs/2402.02101)
61. [Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification Approach](https://arxiv.org/abs/2402.02066)
62. [MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning](https://arxiv.org/abs/2402.02045)
63. [ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.02029)
64. [Understanding Time Series Anomaly State Detection through One-Class Classification](https://arxiv.org/abs/2402.02007)
65. [Robust Counterfactual Explanations in Machine Learning: A Survey](https://arxiv.org/abs/2402.01928)
66. [A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm](https://arxiv.org/abs/2402.01684)
67. [Sample, estimate, aggregate: A recipe for causal discovery foundation models](https://arxiv.org/abs/2402.01929)
68. [SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?](https://arxiv.org/abs/2402.01832)
69. [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761)
70. [VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models](https://arxiv.org/abs/2402.01735)
71. [AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls](https://arxiv.org/abs/2402.04253)
72. [EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters](https://arxiv.org/abs/2402.04252)
73. [Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks](https://arxiv.org/abs/2402.04248)
74. [Scaling Laws for Downstream Task Performance of Large Language Models](https://arxiv.org/abs/2402.04177)
75. [U-shaped Vision Mamba for Single Image Dehazing](https://arxiv.org/abs/2402.04139)
76. [A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation](https://arxiv.org/abs/2402.04087)
77. [Low-rank Attention Side-Tuning for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.04009)
78. [Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning](https://arxiv.org/abs/2402.03917)
79. [Efficient Generation of Hidden Outliers for Improved Outlier Detection](https://arxiv.org/abs/2402.03846)
80. [Energy-based Domain-Adaptive Segmentation with Depth Guidance](https://arxiv.org/abs/2402.03795)
81. [Weakly Supervised Anomaly Detection via Knowledge-Data Alignment](https://arxiv.org/abs/2402.03785)
82. [Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More](https://arxiv.org/abs/2402.03782)
83. [Intensive Vision-guided Network for Radiology Report Generation](https://arxiv.org/abs/2402.03754)
84. [Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images](https://arxiv.org/abs/2402.03752)
85. [Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models](https://arxiv.org/abs/2402.03749)
86. [SISP: A Benchmark Dataset for Fine-grained Ship Instance Segmentation in Panchromatic Satellite Images](https://arxiv.org/abs/2402.03708)
87. [GenLens: A Systematic Evaluation of Visual GenAI Model Outputs](https://arxiv.org/abs/2402.03700)
88. [Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm](https://arxiv.org/abs/2402.03660)
89. [CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model](https://arxiv.org/abs/2402.03631)
90. [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](https://arxiv.org/abs/2402.03627)
91. [GRASP: GRAph-Structured Pyramidal Whole Slide Image Representation](https://arxiv.org/abs/2402.03592)
92. [Diffusion World Model](https://arxiv.org/abs/2402.03570)
93. [How Does Unlabeled Data Provably Help Out-of-Distribution Detection?](https://arxiv.org/abs/2402.03502)
94. [Task-Oriented Communication with Out-of-Distribution Detection: An Information Bottleneck Framework](https://arxiv.org/abs/2305.12423)
95. [Large-scale Generative AI Models Lack Visual Number Sense](https://arxiv.org/abs/2402.03328)
96. [nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark Detection with State Space Model](https://arxiv.org/abs/2402.03526)
97. [Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?](https://arxiv.org/abs/2402.03558)
98. [A Survey on Domain Generalization for Medical Image Analysis](https://arxiv.org/abs/2402.05035)
99. [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/abs/2402.05008)
100. [ConvLoRA and AdaBN based Domain Adaptation via Self-Training](https://arxiv.org/abs/2402.04964)
101. [Text or Image? What is More Important in Cross-Domain Generalization Capabilities of Hate Meme Detection Models?](https://arxiv.org/abs/2402.04967)
102. [Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation](https://arxiv.org/abs/2402.04929)
103. [Personalized Text Generation with Fine-Grained Linguistic Control](https://arxiv.org/abs/2402.04914)
104. [L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ](https://arxiv.org/abs/2402.04902)
105. [Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning](https://arxiv.org/abs/2402.04833)
106. [BOWLL: A Deceptively Simple Open World Lifelong Learner](https://arxiv.org/abs/2402.04814)
107. [MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](https://arxiv.org/abs/2402.04788)
108. [Boundary-aware Contrastive Learning for Semi-supervised Nuclei Instance Segmentation](https://arxiv.org/abs/2402.04756)
109. [Large Language Models As Faithful Explainers](https://arxiv.org/abs/2402.04678)
110. [Open-Vocabulary Calibration for Vision-Language Models](https://arxiv.org/abs/2402.04655)
111. [LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors](https://arxiv.org/abs/2402.04630)
112. [MEMORYLLM: Towards Self-Updatable Large Language Models](https://arxiv.org/abs/2402.04624)
113. [InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory](https://arxiv.org/abs/2402.04617)
114. [TinyLLM: Learning a Small Student from Multiple Large Language Models](https://arxiv.org/abs/2402.04616)
115. [ScreenAI: A Vision-Language Model for UI and Infographics Understanding](https://arxiv.org/abs/2402.04615)
116. [Sparse Anatomical Prompt Semi-Supervised Learning with Masked Image Modeling for CBCT Tooth Segmentation](https://arxiv.org/abs/2402.04587)
117. [FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models](https://arxiv.org/abs/2402.04555)
118. [BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Robust Vision](https://arxiv.org/abs/2402.04519)
119. [Detection Transformer for Teeth Detection, Segmentation, and Numbering in Oral Rare Diseases: Focus on Data Augmentation and Inpainting Techniques](https://arxiv.org/abs/2402.04408)
120. [Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2402.04401)
121. [Mamba-UNet: UNet-Like Pure Visual Mamba for Medical Image Segmentation](https://arxiv.org/abs/2402.05079)
122. [Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery](https://arxiv.org/abs/2402.05860)
123. [TaE: Task-aware Expandable Representation for Long Tail Class Incremental Learning](https://arxiv.org/abs/2402.05797)
124. [Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images](https://arxiv.org/abs/2402.05779)
125. [MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis](https://arxiv.org/abs/2402.05408)
126. [Enhancing Zero-shot Counting via Language-guided Exemplar Learning](https://arxiv.org/abs/2402.05394)
127. [Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection](https://arxiv.org/abs/2402.05294)
128. [$λ$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space](https://arxiv.org/abs/2402.05195)
129. [A Closer Look at the Limitations of Instruction Tuning](https://arxiv.org/abs/2402.05119)
130. [Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models](https://arxiv.org/abs/2402.05210)
131. [ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation](https://arxiv.org/abs/2402.05902)