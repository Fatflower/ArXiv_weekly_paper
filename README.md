# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)

<!-- | | | | | -->

## Recommended Papers for Week 16, 2024
1. [Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2404.08531)
2. [Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2404.08181)
3. [Latent Guard: a Safety Framework for Text-to-image Generation](https://arxiv.org/abs/2404.08031)
4. [A Large-Scale Evaluation of Speech Foundation Models](https://arxiv.org/abs/2404.09385)
5. [Diffusion Models Meet Remote Sensing: Principles, Methods, and Perspectives](https://arxiv.org/abs/2404.08926)
6. [PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification](https://arxiv.org/abs/2404.08915)
7. [EIVEN: Efficient Implicit Attribute Value Extraction using Multimodal LLM](https://arxiv.org/abs/2404.08886)
8. [Is Next Token Prediction Sufficient for GPT? Exploration on Code Logic Comprehension](https://arxiv.org/abs/2404.08885)
9. [Generative AI Agent for Next-Generation MIMO Design: Fundamentals, Challenges, and Vision](https://arxiv.org/abs/2404.08878)
10. [Aligning LLMs for FL-free Program Repair](https://arxiv.org/abs/2404.08877)
11. [LLM In-Context Recall is Prompt Dependent](https://arxiv.org/abs/2404.08865)
12. [Voice Attribute Editing with Text Prompt](https://arxiv.org/abs/2404.08857)
13. [The Illusion of State in State-Space Models](https://arxiv.org/abs/2404.08819)
14. [Detecting AI-Generated Images via CLIP](https://arxiv.org/abs/2404.08788)
15. [LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning](https://arxiv.org/abs/2404.08767)
16. [`Eyes of a Hawk and Ears of a Fox': Part Prototype Network for Generalized Zero-Shot Learning](https://arxiv.org/abs/2404.08761)
17. [Your Finetuned Large Language Model is Already a Powerful Out-of-distribution Detector](https://arxiv.org/abs/2404.08679)
18. [MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.09977)
19. [How to build the best medical image segmentation algorithm using foundation models: a comprehensive empirical study with Segment Anything Model](https://arxiv.org/abs/2404.09957)
20. [Evolving Interpretable Visual Classifiers with Large Language Models](https://arxiv.org/abs/2404.09941)
21. [Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://arxiv.org/abs/2404.09932)
22. [Evaluating the Explainability of Attributes and Prototypes for a Medical Classification Model](https://arxiv.org/abs/2404.09917)
23. [Conditional Prototype Rectification Prompt Learning](https://arxiv.org/abs/2404.09872)
24. [Impact of Preference Noise on the Alignment Performance of Generative Language Models](https://arxiv.org/abs/2404.09824)
25. [TextCoT: Zoom In for Enhanced Multimodal Text-Rich Image Understanding](https://arxiv.org/abs/2404.09797)
26. [The Devil is in the Few Shots: Iterative Visual Knowledge Completion for Few-shot Learning](https://arxiv.org/abs/2404.09778)
27. [nnU-Net Revisited: A Call for Rigorous Validation in 3D Medical Image Segmentation](https://arxiv.org/abs/2404.09556)
28. [State Space Model for New-Generation Network Alternative to Transformers: A Survey](https://arxiv.org/abs/2404.09516)
29. [Q2A: Querying Implicit Fully Continuous Feature Pyramid to Align Features for Medical Image Segmentation](https://arxiv.org/abs/2404.09472)
30. [RankCLIP: Ranking-Consistent Language-Image Pretraining](https://arxiv.org/abs/2404.09387)
31. [Towards Practical Tool Usage for Continually Learning LLMs](https://arxiv.org/abs/2404.09339)
32. [Weight Copy and Low-Rank Adaptation for Few-Shot Distillation of Vision Transformers](https://arxiv.org/abs/2404.09326)
33. [DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection](https://arxiv.org/abs/2404.09216)
34. [TextHawk: Exploring Efficient Fine-Grained Perception of Multimodal Large Language Models](https://arxiv.org/abs/2404.09204)
35. [Change Guiding Network: Incorporating Change Prior to Guide Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2404.09179)
36. [GeMQuAD : Generating Multilingual Question Answering Datasets from Large Language Models using Few Shot Learning](https://arxiv.org/abs/2404.09163)
37. [Fusion-Mamba for Cross-modality Object Detection](https://arxiv.org/abs/2404.09146)
38. [MING-MOE: Enhancing Medical Multi-Task Learning in Large Language Models with Sparse Mixture of Low-Rank Adapter Experts](https://arxiv.org/abs/2404.09027)
39. [Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies](https://arxiv.org/abs/2404.09022)
40. [Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning](https://arxiv.org/abs/2404.08985)
41. [MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes](https://arxiv.org/abs/2404.08968)
42. [AMU-Tuning: Effective Logit Bias for CLIP-based Few-shot Learning](https://arxiv.org/abs/2404.08958)
43. [Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2404.08951)
44. [Label-free Anomaly Detection in Aerial Agricultural Images with Masked Image Modeling](https://arxiv.org/abs/2404.08931)
45. [MOWA: Multiple-in-One Image Warping Model](https://arxiv.org/abs/2404.10716)
46. [Contextrast: Contextual Contrastive Learning for Semantic Segmentation](https://arxiv.org/abs/2404.10633)
47. [Private Attribute Inference from Images with Vision-Language Models](https://arxiv.org/abs/2404.10618)
48. [Construction of Domain-specified Japanese Large Language Model for Finance through Continual Pre-training](https://arxiv.org/abs/2404.10555)
49. [A Sentiment Analysis of Medical Text Based on Deep Learning](https://arxiv.org/abs/2404.10503)
50. [Self-Supervised Visual Preference Alignment](https://arxiv.org/abs/2404.10501)
51. [Toward a Realistic Benchmark for Out-of-Distribution Detection](https://arxiv.org/abs/2404.10474)
52. [Optimization of Prompt Learning via Multi-Knowledge Representation for Vision-Language Models](https://arxiv.org/abs/2404.10357)
53. [Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2404.10322)
54. [Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs](https://arxiv.org/abs/2404.10308)
55. [MoE-TinyMed: Mixture of Experts for Tiny Medical Large Vision-Language Models](https://arxiv.org/abs/2404.10237)
56. [Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V](https://arxiv.org/abs/2404.10220)
57. [Salient Object-Aware Background Generation using Text-Guided Diffusion Models](https://arxiv.org/abs/2404.10157)
58. [SegFormer3D: an Efficient Transformer for 3D Medical Image Segmentation](https://arxiv.org/abs/2404.10156)
59. [Dual Modalities of Text: Visual and Textual Generative Pre-training](https://arxiv.org/abs/2404.10710)
60. [Mixed Prototype Consistency Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2404.10717)
61. [LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?](https://arxiv.org/abs/2404.10763)
62. [Watch Your Step: Optimal Retrieval for Continual Learning at Scale](https://arxiv.org/abs/2404.10758)
63. [AGHINT: Attribute-Guided Representation Learning on Heterogeneous Information Networks with Transformer](https://arxiv.org/abs/2404.10443)
64. [Know Yourself Better: Diverse Discriminative Feature Learning Improves Open Set Recognition](https://arxiv.org/abs/2404.10370)
65. [Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review](https://arxiv.org/abs/2404.11597)
66. [Variational Bayesian Last Layers](https://arxiv.org/abs/2404.11599)
67. [JointViT: Modeling Oxygen Saturation Levels with Joint Supervision on Long-Tailed OCTA](https://arxiv.org/abs/2404.11525)
68. [VBR: A Vision Benchmark in Rome](https://arxiv.org/abs/2404.11322)
69. [Prompt-Guided Generation of Structured Chest X-Ray Report Using a Pre-trained LLM](https://arxiv.org/abs/2404.11209)
70. [Exploring the Transferability of Visual Prompting for Multimodal Large Language Models](https://arxiv.org/abs/2404.11207)
71. [LMEraser: Large Model Unlearning through Adaptive Prompt Tuning](https://arxiv.org/abs/2404.11056)
72. [Many-Shot In-Context Learning](https://arxiv.org/abs/2404.11018)
73. [Control Theoretic Approach to Fine-Tuning and Transfer Learning](https://arxiv.org/abs/2404.11013)
74. [Incubating Text Classifiers Following User Instruction with Nothing but LLM](https://arxiv.org/abs/2404.10877)
75. [BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390)
76. [Moving Object Segmentation: All You Need Is SAM (and Flow)](https://arxiv.org/abs/2404.12389)
77. [Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models](https://arxiv.org/abs/2404.12387)
78. [SOHES: Self-supervised Open-world Hierarchical Entity Segmentation](https://arxiv.org/abs/2404.12386)
79. [Gradient-Regularized Out-of-Distribution Detection](https://arxiv.org/abs/2404.12368)
80. [When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes](https://arxiv.org/abs/2404.12365)
81. [V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning](https://arxiv.org/abs/2404.12353)
82. [Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment](https://arxiv.org/abs/2404.12318)
83. [Observation, Analysis, and Solution: Exploring Strong Lightweight Vision Transformers via Masked Image Modeling Pre-Training](https://arxiv.org/abs/2404.12210)
84. [How to Benchmark Vision Foundation Models for Semantic Segmentation?](https://arxiv.org/abs/2404.12172)
85. [Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models](https://arxiv.org/abs/2404.12139)
86. [MaskCD: A Remote Sensing Change Detection Network Based on Mask Classification](https://arxiv.org/abs/2404.12081)
87. [Data-free Knowledge Distillation for Fine-grained Visual Categorization](https://arxiv.org/abs/2404.12037)
88. [What does CLIP know about peeling a banana?](https://arxiv.org/abs/2404.12015)
89. [Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation](https://arxiv.org/abs/2404.11981)
90. [Progressive Multi-modal Conditional Prompt Tuning](https://arxiv.org/abs/2404.11864)
91. [TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation](https://arxiv.org/abs/2404.11824)
92. [Prompt-Driven Feature Diffusion for Open-World Semi-Supervised Learning](https://arxiv.org/abs/2404.11795)
93. [Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach](https://arxiv.org/abs/2404.11732)