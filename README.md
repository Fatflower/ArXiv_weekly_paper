# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|

<!-- | | | | | -->

## Recommended Papers for Week 4, 2024
1. [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](https://arxiv.org/abs/2401.10862)
2. [Removal and Selection: Improving RGB-Infrared Object Detection via Coarse-to-Fine Fusion](https://arxiv.org/abs/2401.10731)
3. [AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference](https://arxiv.org/abs/2401.10652)
4. [OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy](https://arxiv.org/abs/2401.10559)
5. [Unified View Imputation and Feature Selection Learning for Incomplete Multi-view Data](https://arxiv.org/abs/2401.10549)
6. [Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences](https://arxiv.org/abs/2401.10529)
7. [On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss](https://arxiv.org/abs/2401.10526)
8. [Focaler-IoU: More Focused Intersection over Union Loss](https://arxiv.org/abs/2401.10525)
9. [LDReg: Local Dimensionality Regularized Self-Supervised Learning](https://arxiv.org/abs/2401.10474)
10. [Investigating Training Strategies and Model Robustness of Low-Rank Adaptation for Language Modeling in Speech Recognition](https://arxiv.org/abs/2401.10447)
11. [Vulnerabilities of Foundation Model Integrated Federated Learning Under Adversarial Threats](https://arxiv.org/abs/2401.10375)
12. [EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model](https://arxiv.org/abs/2401.10278)
13. [Exploring Simple Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2401.12217)
14. [Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models](https://arxiv.org/abs/2401.12215)
15. [CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation](https://arxiv.org/abs/2401.12208)
16. [APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference](https://arxiv.org/abs/2401.12200)
17. [Revisiting Demonstration Selection Strategies in In-Context Learning](https://arxiv.org/abs/2401.12087)
18. [Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling](https://arxiv.org/abs/2401.12039)
19. [SignVTCL: Multi-Modal Continuous Sign Language Recognition Enhanced by Visual-Textual Contrastive Learning](https://arxiv.org/abs/2401.11847)
20. [SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2401.11791)
21. [Colorectal Polyp Segmentation in the Deep Learning Era: A Comprehensive Survey](https://arxiv.org/abs/2401.11734)
22. [Augmenting Prototype Network with TransMix for Few-shot Hyperspectral Image Classification](https://arxiv.org/abs/2401.11724)
23. [INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning](https://arxiv.org/abs/2401.11667)
24. [Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with Multimodal Loss](https://arxiv.org/abs/2401.11633)
25. [Hierarchical Prompts for Rehearsal-free Continual Learning](https://arxiv.org/abs/2401.11544)
26. [Geometric Prior Guided Feature Representation Learning for Long-Tailed Classification](https://arxiv.org/abs/2401.11436)
27. [A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models](https://arxiv.org/abs/2401.11311)
28. [Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images](https://arxiv.org/abs/2401.11170)
29. [Spatial Structure Constraints for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2401.11122)
30. [One Step Learning, One Step Review](https://arxiv.org/abs/2401.10962)
31. [Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs](https://arxiv.org/abs/2401.11708)
32. [In-Context Language Learning: Arhitectures and Algorithms](https://arxiv.org/abs/2401.12973)
33. [DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer](https://arxiv.org/abs/2401.12820)
34. [ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation](https://arxiv.org/abs/2401.12665)
35. [Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR](https://arxiv.org/abs/2401.12513)
36. [Enhancing In-context Learning via Linear Probe Calibration](https://arxiv.org/abs/2401.12406)
37. [OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection](https://arxiv.org/abs/2401.12344)