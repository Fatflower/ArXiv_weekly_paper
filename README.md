# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)

<!-- | | | | | -->

## Recommended Papers for Week 14, 2024
1. [CosmicMan: A Text-to-Image Foundation Model for Humans](https://arxiv.org/abs/2404.01294)
2. [Evaluating Text-to-Visual Generation with Image-to-Text Generation](https://arxiv.org/abs/2404.01291)
3. [LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action Localization](https://arxiv.org/abs/2404.01282)
4. [Bridging Remote Sensors with Multisensor Geospatial Foundation Models](https://arxiv.org/abs/2404.01260)
5. [Open-Vocabulary Federated Learning with Multimodal Prototyping](https://arxiv.org/abs/2404.01232)
6. [BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2404.01179)
7. [Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation](https://arxiv.org/abs/2404.01127)
8. [Efficient Prompting Methods for Large Language Models: A Survey](https://arxiv.org/abs/2404.01077)
9. [T-Mamba: Frequency-Enhanced Gated Long-Range Dependency for Tooth 3D CBCT Segmentation](https://arxiv.org/abs/2404.01065)
10. [Continual Learning for Smart City: A Survey](https://arxiv.org/abs/2404.00983)
11. [Harnessing The Power of Attention For Patch-Based Biomedical Image Classification](https://arxiv.org/abs/2404.00949)
12. [A Comprehensive Review of Knowledge Distillation in Computer Vision](https://arxiv.org/abs/2404.00936)
13. [Rethinking Saliency-Guided Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2404.00918)
14. [Slightly Shift New Classes to Remember Old Classes for Video Class-Incremental Learning](https://arxiv.org/abs/2404.00901)
15. [Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance](https://arxiv.org/abs/2404.00860)
16. [Rehearsal-Free Modular and Compositional Continual Learning for Language Models](https://arxiv.org/abs/2404.00790)
17. [Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2404.00781)
18. [LLM meets Vision-Language Models for Zero-Shot One-Class Classification](https://arxiv.org/abs/2404.00675)
19. [Deep Instruction Tuning for Segment Anything Model](https://arxiv.org/abs/2404.00650)
20. [SpiralMLP: A Lightweight Vision MLP Architecture](https://arxiv.org/abs/2404.00648)
21. [Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning](https://arxiv.org/abs/2404.00603)
22. [LLMs are Good Action Recognizers](https://arxiv.org/abs/2404.00532)
23. [Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation](https://arxiv.org/abs/2404.00417)
24. [TTD: Text-Tag Self-Distillation Enhancing Image-Text Alignment in CLIP to Alleviate Single Tag Bias](https://arxiv.org/abs/2404.00384)
25. [Small Language Models Learn Enhanced Reasoning Skills from Medical Textbooks](https://arxiv.org/abs/2404.00376)
26. [DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation](https://arxiv.org/abs/2404.00380)
27. [CLIP-driven Outliers Synthesis for few-shot OOD detection](https://arxiv.org/abs/2404.00323)
28. [Bayesian Exploration of Pre-trained Models for Low-shot Image Classification](https://arxiv.org/abs/2404.00312)
29. [DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation](https://arxiv.org/abs/2404.00264)
30. [Image-to-Image Matching via Foundation Models: A New Perspective for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2404.00262)
31. [InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning](https://arxiv.org/abs/2404.00228)
32. [Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark](https://arxiv.org/abs/2404.00216)
33. [AgileFormer: Spatially Agile Transformer UNet for Medical Image Segmentation](https://arxiv.org/abs/2404.00122)
34. [Diffusion based Zero-shot Medical Image-to-Image Translation for Cross Modality Segmentation](https://arxiv.org/abs/2404.01102)
35. [Are We on the Right Way for Evaluating Large Vision-Language Models?](https://arxiv.org/abs/2403.20330)
36. [MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning](https://arxiv.org/abs/2403.20320)
37. [Convolutional Prompting meets Language Models for Continual Learning](https://arxiv.org/abs/2403.20317)
38. [Learn "No" to Say "Yes" Better: Improving Vision-Language Models via Negations](https://arxiv.org/abs/2403.20312)
39. [Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference](https://arxiv.org/abs/2403.20306)
40. [Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain](https://arxiv.org/abs/2403.20288)
41. [LayerNorm: A key component in parameter-efficient fine-tuning](https://arxiv.org/abs/2403.20284)
42. [Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want](https://arxiv.org/abs/2403.20271)
43. [MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation](https://arxiv.org/abs/2403.20253)
44. [ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models](https://arxiv.org/abs/2403.20194)
45. [ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models](https://arxiv.org/abs/2403.20158)
46. [ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning](https://arxiv.org/abs/2403.20126)
47. [FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models](https://arxiv.org/abs/2403.20105)
48. [NLP for Counterspeech against Hate: A Survey and How-To Guide](https://arxiv.org/abs/2403.20103)
49. [Selective Attention-based Modulation for Continual Learning](https://arxiv.org/abs/2403.20086)
50. [Negative Label Guided OOD Detection with Pretrained Vision-Language Models](https://arxiv.org/abs/2403.20078)
51. [Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer](https://arxiv.org/abs/2403.19979)
52. [Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data](https://arxiv.org/abs/2403.19950)
53. [FairCLIP: Harnessing Fairness in Vision-Language Learning](https://arxiv.org/abs/2403.19949)
54. [An Information-Theoretic Framework for Out-of-Distribution Generalization](https://arxiv.org/abs/2403.19895)
55. [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)
56. [Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation](https://arxiv.org/abs/2403.19826)
57. [MAPL: Model Agnostic Peer-to-peer Learning](https://arxiv.org/abs/2403.19792)
58. [CLoRA: A Contrastive Approach to Compose Multiple LoRA Models](https://arxiv.org/abs/2403.19776)
59. [GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation](https://arxiv.org/abs/2403.19754)
60. [Analyzing the Roles of Language and Vision in Learning from Limited Data](https://arxiv.org/abs/2403.19669)
61. [UltraLight VM-UNet: Parallel Vision Mamba Significantly Reduces Parameters for Skin Lesion Segmentation](https://arxiv.org/abs/2403.20035)
62. [Vision-Language Synthetic Data Enhances Echocardiography Downstream Tasks](https://arxiv.org/abs/2403.19880)
63. [Segment Any 3D Object with Language](https://arxiv.org/abs/2404.02157)
64. [Dynamic Pre-training: Towards Efficient and Scalable All-in-One Image Restoration](https://arxiv.org/abs/2404.02154)
65. [Iterated Learning Improves Compositionality in Large Vision-Language Models](https://arxiv.org/abs/2404.02145)
66. [ViTamin: Designing Scalable Vision Models in the Vision-Language Era](https://arxiv.org/abs/2404.02132)
67. [Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners](https://arxiv.org/abs/2404.02117)
68. [Red-Teaming Segment Anything Model](https://arxiv.org/abs/2404.02067)
69. [Improving Bird's Eye View Semantic Segmentation by Task Decomposition](https://arxiv.org/abs/2404.01925)
70. [Class-Incremental Few-Shot Event Detection](https://arxiv.org/abs/2404.01767)
71. [Learning Equi-angular Representations for Online Continual Learning](https://arxiv.org/abs/2404.01628)
72. [Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT Reconstruction](https://arxiv.org/abs/2404.01448)
73. [DeiT-LT Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets](https://arxiv.org/abs/2404.02900)
74. [On the Scalability of Diffusion-based Text-to-Image Generation](https://arxiv.org/abs/2404.02883)
75. [End-To-End Self-tuning Self-supervised Time Series Anomaly Detection](https://arxiv.org/abs/2404.02865)
76. [Toward Inference-optimal Mixture-of-Expert Large Language Models](https://arxiv.org/abs/2404.02852)
77. [Cross-Modal Conditioned Reconstruction for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2404.02845)
78. [Enhancing Interpretability of Vertebrae Fracture Grading using Human-interpretable Prototypes](https://arxiv.org/abs/2404.02830)
79. [BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models](https://arxiv.org/abs/2404.02827)
80. [MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation](https://arxiv.org/abs/2404.02790)
81. [FPT: Feature Prompt Tuning for Few-shot Readability Assessment](https://arxiv.org/abs/2404.02772)
82. [Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models](https://arxiv.org/abs/2404.02747)
83. [InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation](https://arxiv.org/abs/2404.02733)
84. [Harnessing the Power of Large Vision Language Models for Synthetic Image Detection](https://arxiv.org/abs/2404.02726)
85. [Automatic Prompt Selection for Large Language Models](https://arxiv.org/abs/2404.02717)
86. [Model-agnostic Origin Attribution of Generated Images with Few-shot Examples](https://arxiv.org/abs/2404.02697)
87. [Non-negative Subspace Feature Representation for Few-shot Learning in Medical Imaging](https://arxiv.org/abs/2404.02656)
88. [SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation](https://arxiv.org/abs/2404.02638)
89. [Diffexplainer: Towards Cross-modal Global Explanations with Diffusion Models](https://arxiv.org/abs/2404.02618)
90. [PromptRPA: Generating Robotic Process Automation on Smartphones from Textual Prompts](https://arxiv.org/abs/2404.02475)
91. [RS3Mamba: Visual State Space Model for Remote Sensing Images Semantic Segmentation](https://arxiv.org/abs/2404.02457)
92. [Adaptive Cross-lingual Text Classification through In-Context One-Shot Demonstrations](https://arxiv.org/abs/2404.02452)
93. [Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data](https://arxiv.org/abs/2404.02422)
94. [What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases](https://arxiv.org/abs/2404.02415)
95. [Enhancing Human-Computer Interaction in Chest X-ray Analysis using Vision and Language Model with Eye Gaze Patterns](https://arxiv.org/abs/2404.02370)
96. [Semantic Augmentation in Images using Language](https://arxiv.org/abs/2404.02353)
97. [LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP](https://arxiv.org/abs/2404.02285)
98. [Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning](https://arxiv.org/abs/2404.03658)
99. [OW-VISCap: Open-World Video Instance Segmentation and Captioning](https://arxiv.org/abs/2404.03657)
100. [CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept Matching](https://arxiv.org/abs/2404.03653)
101. [OpenNeRF: Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views](https://arxiv.org/abs/2404.03650)
102. [Locating and Editing Factual Associations in Mamba](https://arxiv.org/abs/2404.03646)
103. [Training LLMs over Neurally Compressed Text](https://arxiv.org/abs/2404.03626)
104. [LCM-Lookahead for Encoder-based Text-to-Image Personalization](https://arxiv.org/abs/2404.03620)
105. [DeViDe: Faceted medical knowledge for improved medical vision-language pre-training](https://arxiv.org/abs/2404.03618)
106. [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592)
107. [Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity](https://arxiv.org/abs/2404.03570)
108. [Is CLIP the main roadblock for fine-grained open-world perception?](https://arxiv.org/abs/2404.03539)
109. [How Much Data are Enough? Investigating Dataset Requirements for Patch-Based Brain MRI Segmentation Tasks](https://arxiv.org/abs/2404.03451)
110. [Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?](https://arxiv.org/abs/2404.03411)
111. [Scaling Up Video Summarization Pretraining with Large Language Models](https://arxiv.org/abs/2404.03398)
112. [Two Tricks to Improve Unsupervised Segmentation Learning](https://arxiv.org/abs/2404.03392)
113. [Foundation Model for Advancing Healthcare: Challenges, Opportunities, and Future Directions](https://arxiv.org/abs/2404.03264)
114. [On the Surprising Efficacy of Distillation as an Alternative to Pre-Training Small Models](https://arxiv.org/abs/2404.03263)
115. [Learning Transferable Negative Prompts for Out-of-Distribution Detection](https://arxiv.org/abs/2404.03248)
116. [Future-Proofing Class Incremental Learning](https://arxiv.org/abs/2404.03200)
117. [LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models](https://arxiv.org/abs/2404.03118)
118. [AWOL: Analysis WithOut synthesis using Language](https://arxiv.org/abs/2404.03042)
119. [JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2404.03027)
120. [Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models](https://arxiv.org/abs/2404.02936)