# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)|[25_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/25_week.md)|[26_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/26_week.md)|[27_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/27_week.md)|[28_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/28_week.md)|[29_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/29_week.md)|[30_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/30_week.md)|[31_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/31_week.md)|[32_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/32_week.md)|[33_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/33_week.md)



<!-- | | | | | -->

## Recommended Papers for Week 33, 2024
1. [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/abs/2408.05211)
2. [TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning](https://arxiv.org/abs/2408.05200)
3. [Cautious Calibration in Binary Classification](https://arxiv.org/abs/2408.05120)
4. [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)
5. [UNIC: Universal Classification Models via Multi-teacher Distillation](https://arxiv.org/abs/2408.05088)
6. [In Defense of Lazy Visual Grounding for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2408.04961)
7. [Avoid Wasted Annotation Costs in Open-set Active Learning with Pre-trained Vision-Language Model](https://arxiv.org/abs/2408.04917)
8. [GuidedNet: Semi-Supervised Multi-Organ Segmentation via Labeled Data Guide Unlabeled Data](https://arxiv.org/abs/2408.04914)
9. [ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation](https://arxiv.org/abs/2408.04883)
10. [On the Element-Wise Representation and Reasoning in Zero-Shot Image Recognition: A Systematic Survey](https://arxiv.org/abs/2408.04879)
11. [Your Classifier Can Be Secretly a Likelihood-Based OOD Detector](https://arxiv.org/abs/2408.04851)
12. [Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)](https://arxiv.org/abs/2408.04664)
13. [Beyond the Eye: A Relational Model for Early Dementia Detection Using Retinal OCTA Images](https://arxiv.org/abs/2408.05117)
14. [See It All: Contextualized Late Aggregation for 3D Dense Captioning](https://arxiv.org/abs/2408.07648)
15. [Attention-Guided Perturbation for Unsupervised Image Anomaly Detection](https://arxiv.org/abs/2408.07490)
16. [BAPLe: Backdoor Attacks on Medical Foundational Models using Prompt Learning](https://arxiv.org/abs/2408.07440)
17. [Gradient Alignment Improves Test-Time Adaptation for Medical Image Segmentation](https://arxiv.org/abs/2408.07343)
18. [UniFed: A Universal Federation of a Mixture of Highly Heterogeneous Medical Image Classification Tasks](https://arxiv.org/abs/2408.07075)
19. [Imagen 3](https://arxiv.org/abs/2408.07009)
20. [BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning](https://arxiv.org/abs/2408.06890)
21. [LoRA$^2$ : Multi-Scale Low-Rank Approximations for Fine-Tuning Large Language Models](https://arxiv.org/abs/2408.06854)
22. [Token Compensator: Altering Inference Cost of Vision Transformer without Re-Tuning](https://arxiv.org/abs/2408.06798)
23. [Layerwise Recurrent Router for Mixture-of-Experts](https://arxiv.org/abs/2408.06793)
24. [ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation](https://arxiv.org/abs/2408.06747)
25. [Long-Tailed Out-of-Distribution Detection: Prioritizing Attention to Tail](https://arxiv.org/abs/2408.06742)
26. [DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion](https://arxiv.org/abs/2408.06740)
27. [Towards Cross-Domain Single Blood Cell Image Classification via Large-Scale LoRA-based Segment Anything Model](https://arxiv.org/abs/2408.06716)
28. [Masked Image Modeling: A Survey](https://arxiv.org/abs/2408.06687)
29. [IFShip: A Large Vision-Language Model for Interpretable Fine-grained Ship Classification via Domain Knowledge-Enhanced Instruction Tuning](https://arxiv.org/abs/2408.06631)
30. [CROME: Cross-Modal Adapters for Efficient Multimodal LLM](https://arxiv.org/abs/2408.06610)
31. [S-SAM: SVD-based Fine-Tuning of Segment Anything Model for Medical Image Segmentation](https://arxiv.org/abs/2408.06447)
32. [Attention Based Feature Fusion Network for Monkeypox Skin Lesion Detection](https://arxiv.org/abs/2408.06640)
33. [How good nnU-Net for Segmenting Cardiac MRI: A Comprehensive Evaluation](https://arxiv.org/abs/2408.06358)
34. [VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents](https://arxiv.org/abs/2408.06327)
35. [From SAM to SAM 2: Exploring Improvements in Meta's Segment Anything Model](https://arxiv.org/abs/2408.06305)
36. [FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data](https://arxiv.org/abs/2408.06273)
37. [Context-aware Visual Storytelling with Visual Prefix Tuning and Contrastive Learning](https://arxiv.org/abs/2408.06259)
38. [Correlation Weighted Prototype-based Self-Supervised One-Shot Segmentation of Medical Images](https://arxiv.org/abs/2408.06235)
39. [Med42-v2: A Suite of Clinical LLMs](https://arxiv.org/abs/2408.06142)
40. [HySparK: Hybrid Sparse Masking for Large Scale Medical Image Pre-Training](https://arxiv.org/abs/2408.05815)
41. [Efficient Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2408.05775)
42. [A Novel Momentum-Based Deep Learning Techniques for Medical Image Classification and Segmentation](https://arxiv.org/abs/2408.05692)
43. [PS-TTL: Prototype-based Soft-labels and Test-Time Learning for Few-shot Object Detection](https://arxiv.org/abs/2408.05674)
44. [SAM-FNet: SAM-Guided Fusion Network for Laryngo-Pharyngeal Tumor Detection](https://arxiv.org/abs/2408.05426)
45. [Zero-shot 3D Segmentation of Abdominal Organs in CT Scans Using Segment Anything Model 2: Adapting Video Tracking Capabilities for 3D Medical Imaging](https://arxiv.org/abs/2408.06170)
46. [Polyp SAM 2: Advancing Zero shot Polyp Segmentation in Colorectal Cancer Detection](https://arxiv.org/abs/2408.05892)