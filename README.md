# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)

<!-- | | | | | -->

## Recommended Papers for Week 24, 2024
1. [Advanced Payment Security System:XGBoost, CatBoost and SMOTE Integrated](https://arxiv.org/abs/2406.04658)
2. [CLoG: Benchmarking Continual Learning of Image Generation Models](https://arxiv.org/abs/2406.04584)
3. [Attention Fusion Reverse Distillation for Multi-Lighting Image Anomaly Detection](https://arxiv.org/abs/2406.04573)
4. [OCCAM: Towards Cost-Efficient and Accuracy-Aware Image Classification Inference](https://arxiv.org/abs/2406.04508)
5. [MAIRA-2: Grounded Radiology Report Generation](https://arxiv.org/abs/2406.04449)
6. [Can Language Models Use Forecasting Strategies?](https://arxiv.org/abs/2406.04446)
7. [Can Language Models Serve as Text-Based World Simulators?](https://arxiv.org/abs/2406.06485)
8. [Towards a Personal Health Large Language Model](https://arxiv.org/abs/2406.06474)
9. [Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning](https://arxiv.org/abs/2406.06469)
10. [Towards Lifelong Learning of Large Language Models: A Survey](https://arxiv.org/abs/2406.06391)
11. [Cascading Unknown Detection with Known Classification for Open Set Recognition](https://arxiv.org/abs/2406.06351)
12. [A Statistical Theory of Regularization-Based Continual Learning](https://arxiv.org/abs/2406.06213)
13. [Robust Latent Representation Tuning for Image-text Classification](https://arxiv.org/abs/2406.06048)
14. [Aligning Large Language Models with Representation Editing: A Control Perspective](https://arxiv.org/abs/2406.05954)
15. [Mamba YOLO: SSMs-Based YOLO For Object Detection](https://arxiv.org/abs/2406.05835)
16. [F-LMM: Grounding Frozen Large Multimodal Models](https://arxiv.org/abs/2406.05821)
17. [Visual Prompt Tuning in Null Space for Continual Learning](https://arxiv.org/abs/2406.05658)
18. [CCSI: Continual Class-Specific Impression for Data-free Class Incremental Learning](https://arxiv.org/abs/2406.05631)
19. [A Comprehensive Evaluation of Parameter-Efficient Fine-Tuning on Automated Program Repair](https://arxiv.org/abs/2406.05639)
20. [Domain Generalization Guided by Large-Scale Pre-Trained Priors](https://arxiv.org/abs/2406.05628)
21. [Aligning Human Knowledge with Visual Concepts Towards Explainable Medical Image Classification](https://arxiv.org/abs/2406.05596)
22. [Regularized Training with Generated Datasets for Name-Only Transfer of Vision-Language Models](https://arxiv.org/abs/2406.05432)
23. [LoCoCo: Dropping In Convolutions for Long Context Compression](https://arxiv.org/abs/2406.05317)
24. [SuperPos-Prompt: Enhancing Soft Prompt Tuning of Language Models with Superposition of Multi Token Embeddings](https://arxiv.org/abs/2406.05279)
25. [USE: Universal Segment Embeddings for Open-Vocabulary Image Segmentation](https://arxiv.org/abs/2406.05271)
26. [CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment](https://arxiv.org/abs/2406.05205)
27. [MHS-VM: Multi-Head Scanning in Parallel Subspaces for Vision Mamba](https://arxiv.org/abs/2406.05992)
28. [GCtx-UNet: Efficient Network for Medical Image Segmentation](https://arxiv.org/abs/2406.05891)
29. [MSAGPT: Neural Prompting Protein Structure Prediction via MSA Generative Pre-Training](https://arxiv.org/abs/2406.05347)
30. [An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models](https://arxiv.org/abs/2406.05130)
31. [Prototype Correlation Matching and Class-Relation Reasoning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2406.05054)
32. [AttnDreamBooth: Towards Text-Aligned Personalized Text-to-Image Generation](https://arxiv.org/abs/2406.05000)
33. [ProMotion: Prototypes As Motion Learners](https://arxiv.org/abs/2406.04999)
34. [MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter](https://arxiv.org/abs/2406.04984)
35. [MA-AVT: Modality Alignment for Parameter-Efficient Audio-Visual Transformers](https://arxiv.org/abs/2406.04930)
36. [Revisiting Catastrophic Forgetting in Large Language Model Tuning](https://arxiv.org/abs/2406.04836)
37. [BERTs are Generative In-Context Learners](https://arxiv.org/abs/2406.04823)
38. [REP: Resource-Efficient Prompting for On-device Continual Learning](https://arxiv.org/abs/2406.04772)
39. [MGIMM: Multi-Granularity Instruction Multimodal Model for Attribute-Guided Remote Sensing Image Detailed Description](https://arxiv.org/abs/2406.04716)
40. [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/abs/2406.04692)
41. [An Image is Worth 32 Tokens for Reconstruction and Generation](https://arxiv.org/abs/2406.07550)
42. [Vision Model Pre-training on Interleaved Image-Text Data via Latent Compression Learning](https://arxiv.org/abs/2406.07543)
43. [Autoregressive Pretraining with Mamba in Vision](https://arxiv.org/abs/2406.07537)
44. [Benchmarking Vision-Language Contrastive Methods for Medical Representation Learning](https://arxiv.org/abs/2406.07450)
45. [Bilingual Sexism Classification: Fine-Tuned XLM-RoBERTa and GPT-3.5 Few-Shot Learning](https://arxiv.org/abs/2406.07287)
46. [Teaching Language Models to Self-Improve by Learning from Language Feedback](https://arxiv.org/abs/2406.07168)
47. [CAT: Coordinating Anatomical-Textual Prompts for Multi-Organ and Tumor Segmentation](https://arxiv.org/abs/2406.07085)
48. [Effectively Compress KV Heads for LLM](https://arxiv.org/abs/2406.07056)
49. [Enhancing Text Authenticity: A Novel Hybrid Approach for AI-Generated Text Detection](https://arxiv.org/abs/2406.06558)
50. [Words Worth a Thousand Pictures: Measuring and Understanding Perceptual Variability in Text-to-Image Generation](https://arxiv.org/abs/2406.08482)
51. [Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models](https://arxiv.org/abs/2406.08487)
52. [What If We Recaption Billions of Web Images with LLaMA-3?](https://arxiv.org/abs/2406.08478)
53. [Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens](https://arxiv.org/abs/2406.08477)
54. [Scaling Laws in Linear Regression: Compute, Parameters, and Data](https://arxiv.org/abs/2406.08466)
55. [Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing](https://arxiv.org/abs/2406.08464)
56. [ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery](https://arxiv.org/abs/2406.08457)
57. [Diffusion Soup: Model Merging for Text-to-Image Diffusion Models](https://arxiv.org/abs/2406.08431)
58. [VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks](https://arxiv.org/abs/2406.08394)
59. [Large Language Models Must Be Taught to Know What They Don't Know](https://arxiv.org/abs/2406.08391)
60. [APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentatio](https://arxiv.org/abs/2406.08372)
61. [LaMOT: Language-Guided Multi-Object Tracking](https://arxiv.org/abs/2406.08324)
62. [OpenObj: Open-Vocabulary Object-Level Neural Radiance Fields with Fine-Grained Understanding](https://arxiv.org/abs/2406.08009)
63. [Small Scale Data-Free Knowledge Distillation](https://arxiv.org/abs/2406.07876)
64. [Understanding and Mitigating Compositional Issues in Text-to-Image Generative Models](https://arxiv.org/abs/2406.07844)
65. [AIM: Let Any Multi-modal Large Language Models Embrace Efficient In-Context Learning](https://arxiv.org/abs/2406.07588)