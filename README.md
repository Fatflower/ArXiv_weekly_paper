# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)|[25_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/25_week.md)|[26_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/26_week.md)|[27_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/27_week.md)|[28_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/28_week.md)|[29_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/29_week.md)|[30_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/30_week.md)|[31_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/31_week.md)|[32_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/32_week.md)|[33_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/33_week.md)|[34_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/34_week.md)|[35_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/35_week.md)|[36_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/36_week.md)|[37_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/37_week.md)|[38_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/38_week.md)|[39_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/39_week.md)




<!-- | | | | | -->

## Recommended Papers for Week 39, 2024

1. [The FIX Benchmark: Extracting Features Interpretable to eXperts](https://arxiv.org/abs/2409.13684)
2. [OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition](https://arxiv.org/abs/2409.13652)
3. [Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning](https://arxiv.org/abs/2409.13641)
4. [Exploring Fine-Grained Image-Text Alignment for Referring Remote Sensing Image Segmentation](https://arxiv.org/abs/2409.13637)
5. [YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models](https://arxiv.org/abs/2409.13592)
6. [Graph Similarity Regularized Softmax for Semi-Supervised Node Classification](https://arxiv.org/abs/2409.13544)
7. [Formula-Supervised Visual-Geometric Pre-training](https://arxiv.org/abs/2409.13535)
8. [A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges](https://arxiv.org/abs/2409.13521)
9. [PointSAM: Pointly-Supervised Segment Anything Model for Remote Sensing Images](https://arxiv.org/abs/2409.13401)
10. [Adaptive Margin Global Classifier for Exemplar-Free Class-Incremental Learning](https://arxiv.org/abs/2409.13275)
11. [CFSP: An Efficient Structured Pruning Framework for LLMs with Coarse-to-Fine Activation Information](https://arxiv.org/abs/2409.13199)
12. [Multiscale Encoder and Omni-Dimensional Dynamic Convolution Enrichment in nnU-Net for Brain Tumor Segmentation](https://arxiv.org/abs/2409.13229)
13. [GASA-UNet: Global Axial Self-Attention U-Net for 3D Medical Image Segmentation](https://arxiv.org/abs/2409.13146)
14. [BrainDreamer: Reasoning-Coherent and Controllable Image Generation from EEG Brain Signals via Language Guidance](https://arxiv.org/abs/2409.14021)
15. [Multiple-Exit Tuning: Towards Inference-Efficient Adaptation for Vision Transformer](https://arxiv.org/abs/2409.13999)
16. [Enhanced Semantic Segmentation for Large-Scale and Imbalanced Point Clouds](https://arxiv.org/abs/2409.13983)
17. [Cycle-Consistency Uncertainty Estimation for Visual Prompting based One-Shot Defect Segmentation](https://arxiv.org/abs/2409.13984)
18. [Rule Extrapolation in Language Models: A Study of Compositional Generalization on OOD Prompts](https://arxiv.org/abs/2409.13728)
19. [Towards Ground-truth-free Evaluation of Any Segmentation in Medical Images](https://arxiv.org/abs/2409.14874)
20. [Region Mixup](https://arxiv.org/abs/2409.15028)
21. [Multi-Modal Generative AI: Multi-modal LLM, Diffusion and Beyond](https://arxiv.org/abs/2409.14993)
22. [Dynamic Integration of Task-Specific Adapters for Class Incremental Learning](https://arxiv.org/abs/2409.14983)
23. [A-VL: Adaptive Attention for Large Vision-Language Models](https://arxiv.org/abs/2409.14846)
24. [VLM's Eye Examination: Instruct and Inspect Visual Competency of Vision Language Models](https://arxiv.org/abs/2409.14759)
25. [MemeCLIP: Leveraging CLIP Representations for Multimodal Meme Classification](https://arxiv.org/abs/2409.14703)
26. [Patch Ranking: Efficient CLIP by Learning to Rank Local Patches](https://arxiv.org/abs/2409.14607)
27. [The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends](https://arxiv.org/abs/2409.14195)
28. [PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions](https://arxiv.org/abs/2409.15278)
29. [A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?](https://arxiv.org/abs/2409.15277)
30. [ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models](https://arxiv.org/abs/2409.15250)