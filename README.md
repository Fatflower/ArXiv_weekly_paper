# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)

<!-- | | | | | -->

## Recommended Papers for Week 20, 2024
1. [Multi-Target Unsupervised Domain Adaptation for Semantic Segmentation without External Data](https://arxiv.org/abs/2405.06502)
2. [Pseudo-Prompt Generating in Pre-trained Vision-Language Models for Multi-Label Medical Image Classification](https://arxiv.org/abs/2405.06468)
3. [XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare](https://arxiv.org/abs/2405.06270)
4. [Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark](https://arxiv.org/abs/2405.06634)
5. [Explaining Text Similarity in Transformer Models](https://arxiv.org/abs/2405.06604)
6. [Enhancing Weakly Supervised Semantic Segmentation with Multi-modal Foundation Models: An End-to-End Approach](https://arxiv.org/abs/2405.06586)
7. [MambaOut: Do We Really Need Mamba for Vision?](https://arxiv.org/abs/2405.07992)
8. [Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation](https://arxiv.org/abs/2405.07969)
9. [Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?](https://arxiv.org/abs/2405.07921)
10. [Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis](https://arxiv.org/abs/2405.07481)
11. [Liquid Ensemble Selection for Continual Learning](https://arxiv.org/abs/2405.07327)
12. [Erasing Concepts from Text-to-Image Diffusion Models with Few-shot Unlearning](https://arxiv.org/abs/2405.07288)
13. [Zero Shot Context-Based Object Segmentation using SLIP (SAM+CLIP)](https://arxiv.org/abs/2405.07284)
14. [Cross-Domain Continual Learning via CLAMP](https://arxiv.org/abs/2405.07142)
15. [TAI++: Text as Image for Multi-Label Image Classification by Co-Learning Transferable Prompt](https://arxiv.org/abs/2405.06926)
16. [PLUTO: Pathology-Universal Transformer](https://arxiv.org/abs/2405.07905)
17. [Leveraging Fixed and Dynamic Pseudo-labels for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2405.07256)
18. [Realizing Visual Question Answering for Education: GPT-4V as a Multimodal AI](https://arxiv.org/abs/2405.07163)
19. [EMCAD: Efficient Multi-scale Convolutional Attention Decoding for Medical Image Segmentation](https://arxiv.org/abs/2405.06880)
20. [SAM3D: Zero-Shot Semi-Automatic Segmentation in 3D Medical Images with the Segment Anything Model](https://arxiv.org/abs/2405.06786)
21. [DrugLLM: Open Large Language Model for Few-shot Molecule Generation](https://arxiv.org/abs/2405.06690)
22. [Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model](https://arxiv.org/abs/2405.10316)
23. [On Sample Selection for Continual Learning: a Video Streaming Case Study](https://arxiv.org/abs/2405.10290)
24. [FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models](https://arxiv.org/abs/2405.10286)
25. [GPT Store Mining and Analysis](https://arxiv.org/abs/2405.10210)
26. [Densely Distilling Cumulative Knowledge for Continual Learning](https://arxiv.org/abs/2405.09820)
27. [Many-Shot In-Context Learning in Multimodal Foundation Models](https://arxiv.org/abs/2405.09798)
28. [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)
29. [A Comprehensive Survey on Data Augmentation](https://arxiv.org/abs/2405.09591)
30. [MGSER-SAM: Memory-Guided Soft Experience Replay with Sharpness-Aware Optimization for Enhanced Continual Learning](https://arxiv.org/abs/2405.09492)
31. [Time-Equivariant Contrastive Learning for Degenerative Disease Progression in Retinal OCT](https://arxiv.org/abs/2405.09404)
32. [CTS: A Consistency-Based Medical Image Segmentation Model](https://arxiv.org/abs/2405.09056)
33. [CLIP with Quality Captions: A Strong Pretraining for Vision Tasks](https://arxiv.org/abs/2405.08911)
34. [Energy-based Hopfield Boosting for Out-of-Distribution Detection](https://arxiv.org/abs/2405.08766)
35. [Image to Pseudo-Episode: Boosting Few-Shot Segmentation by Unlabeled Data](https://arxiv.org/abs/2405.08765)
36. [EVDA: Evolving Deepfake Audio Detection Continual Learning Benchmark](https://arxiv.org/abs/2405.08596)
37. [Dynamic Feature Learning and Matching for Class-Incremental Learning](https://arxiv.org/abs/2405.08533)
38. [Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation](https://arxiv.org/abs/2405.08458)
39. [How Alignment Helps Make the Most of Multimodal Data](https://arxiv.org/abs/2405.08454)
40. [Feature Expansion and enhanced Compression for Class Incremental Learning](https://arxiv.org/abs/2405.08038)