# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)|[25_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/25_week.md)|[26_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/26_week.md)|[27_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/27_week.md)|[28_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/28_week.md)|[29_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/29_week.md)|[30_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/30_week.md)|[31_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/31_week.md)|[32_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/32_week.md)|[33_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/33_week.md)|[34_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/34_week.md)|[35_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/35_week.md)|[36_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/36_week.md)|[37_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/37_week.md)|[38_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/38_week.md)|[39_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/39_week.md)|[40_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/40_week.md)|[41_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/41_week.md)|[42_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/42_week.md)




<!-- | | | | | -->

## Recommended Papers for Week 42, 2024

1. [Parameter-Efficient Fine-Tuning of State Space Models](https://arxiv.org/abs/2410.09016)
2. [Semantic Score Distillation Sampling for Compositional Text-to-3D Generation](https://arxiv.org/abs/2410.09009)
3. [UniGlyph: A Seven-Segment Script for Universal Language Representation](https://arxiv.org/abs/2410.08974)
4. [Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2410.08895)
5. [Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies](https://arxiv.org/abs/2410.08860)
6. [VideoSAM: Open-World Video Segmentation](https://arxiv.org/abs/2410.08781)
7. [Conjugated Semantic Pool Improves OOD Detection with Pre-trained Vision-Language Models](https://arxiv.org/abs/2410.08611)
8. [Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP](https://arxiv.org/abs/2410.08469)
9. [Symbolic Music Generation with Fine-grained Interactive Textural Guidance](https://arxiv.org/abs/2410.08435)
10. [Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis](https://arxiv.org/abs/2410.08261)
11. [When Does Perceptual Alignment Benefit Vision Representations?](https://arxiv.org/abs/2410.10817)
12. [Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free](https://arxiv.org/abs/2410.10814)
13. [Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning](https://arxiv.org/abs/2410.10801)
14. [UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2410.10777)
15. [FlexGen: Flexible Multi-View Generation from Text and Image Inputs](https://arxiv.org/abs/2410.10745)
16. [Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs](https://arxiv.org/abs/2410.10739)
17. [Queryable Prototype Multiple Instance Learning with Vision-Language Models for Incremental Whole Slide Image Classification](https://arxiv.org/abs/2410.10573)
18. [RICASSO: Reinforced Imbalance Learning with Class-Aware Self-Supervised Outliers Exposure](https://arxiv.org/abs/2410.10548)
19. [Vision-guided and Mask-enhanced Adaptive Denoising for Prompt-based Image Editing](https://arxiv.org/abs/2410.10496)
20. [Learning to Ground VLMs without Forgetting](https://arxiv.org/abs/2410.10491)
21. [Tighter Risk Bounds for Mixtures of Experts](https://arxiv.org/abs/2410.10397)
22. [V2M: Visual 2-Dimensional Mamba for Image Representation Learning](https://arxiv.org/abs/2410.10382)
23. [GraphCLIP: Enhancing Transferability in Graph Foundation Models for Text-Attributed Graphs](https://arxiv.org/abs/2410.10329)
24. [GlobalMamba: Global Image Serialization for Vision Mamba](https://arxiv.org/abs/2410.10316)
25. [Fine-grained Abnormality Prompt Learning for Zero-shot Anomaly Detection](https://arxiv.org/abs/2410.10289)
26. [LOBG:Less Overfitting for Better Generalization in Vision-Language Model](https://arxiv.org/abs/2410.10247)
27. [KNN Transformer with Pyramid Prompts for Few-Shot Learning](https://arxiv.org/abs/2410.10227)
28. [Is Parameter Collision Hindering Continual Learning in LLMs?](https://arxiv.org/abs/2410.10179)
29. [MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](https://arxiv.org/abs/2410.10139)
30. [Can We Predict Performance of Large Models across Vision-Language Tasks?](https://arxiv.org/abs/2410.10112)
31. [TULIP: Token-length Upgraded CLIP](https://arxiv.org/abs/2410.10034)
32. [Text4Seg: Reimagining Image Segmentation as Text Generation](https://arxiv.org/abs/2410.09855)
33. [Understanding Robustness of Parameter-Efficient Tuning for Image Classification](https://arxiv.org/abs/2410.09845)
34. [Task Adaptive Feature Distribution Based Network for Few-shot Fine-grained Target Classification](https://arxiv.org/abs/2410.09797)
35. [AM-SAM: Automated Prompting and Mask Calibration for Segment Anything Model](https://arxiv.org/abs/2410.09714)
36. [MMCOMPOSITION: Revisiting the Compositionality of Pre-trained Vision-Language Models](https://arxiv.org/abs/2410.09733)
37. [Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings](https://arxiv.org/abs/2410.09649)
38. [AI Model Registries: A Foundational Tool for AI Governance](https://arxiv.org/abs/2410.09645)
39. [MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning](https://arxiv.org/abs/2410.09437)
40. [Debiasing Vison-Language Models with Text-Only Training](https://arxiv.org/abs/2410.09365)
41. [CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation](https://arxiv.org/abs/2410.09400)
42. [Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor](https://arxiv.org/abs/2410.09237)
43. [Continual Learning with Neuromorphic Computing: Theories, Methods, and Applications](https://arxiv.org/abs/2410.09218)
44. [Cross-Domain Evaluation of Few-Shot Classification Models: Natural Images vs. Histopathological Images](https://arxiv.org/abs/2410.09176)
45. [MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding](https://arxiv.org/abs/2410.11829)
46. [Improving Long-Text Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2410.11817)
47. [The Best of Both Worlds: On the Dilemma of Out-of-distribution Detection](https://arxiv.org/abs/2410.11576)
48. [Overcoming Domain Limitations in Open-vocabulary Segmentation](https://arxiv.org/abs/2410.11536)
49. [Augmentation-Driven Metric for Balancing Preservation and Modification in Text-Guided Image Editing](https://arxiv.org/abs/2410.11374)
50. [CONSULT: Contrastive Self-Supervised Learning for Few-shot Tumor Detection](https://arxiv.org/abs/2410.11307)
51. [Tree of Attributes Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2410.11201)
52. [MANet: Fine-Tuning Segment Anything Model for Multimodal Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2410.11160)
53. [LLaCA: Multimodal Large Language Continual Assistant](https://arxiv.org/abs/2410.10868)
54. [STA-Unet: Rethink the semantic redundant for Medical Imaging Segmentation](https://arxiv.org/abs/2410.11578)
55. [Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models](https://arxiv.org/abs/2410.12790)
56. [VividMed: Vision Language Model with Versatile Visual Grounding for Medicine](https://arxiv.org/abs/2410.12694)
57. [3DIS: Depth-Driven Decoupled Instance Synthesis for Text-to-Image Generation](https://arxiv.org/abs/2410.12669)
58. [Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models](https://arxiv.org/abs/2410.12662)
59. [CMAL: A Novel Cross-Modal Associative Learning Framework for Vision-Language Pre-Training](https://arxiv.org/abs/2410.12595)
60. [Prompt Compression for Large Language Models: A Survey](https://arxiv.org/abs/2410.12388)
61. [VisAnatomy: An SVG Chart Corpus with Fine-Grained Semantic Labels](https://arxiv.org/abs/2410.12268)
62. [Model Balancing Helps Low-data Training and Fine-tuning](https://arxiv.org/abs/2410.12178)
63. [CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning](https://arxiv.org/abs/2410.11963)
64. [SeQuiFi: Mitigating Catastrophic Forgetting in Speech Emotion Recognition with Sequential Class-Finetuning](https://arxiv.org/abs/2410.12567)
65. [Explainable AI Methods for Multi-Omics Analysis: A Survey](https://arxiv.org/abs/2410.11910)
66. [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/abs/2410.11842)