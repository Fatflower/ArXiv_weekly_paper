# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|[7_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/7_week.md)|[8_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/8_week.md)|[9_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/9_week.md)|[10_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/10_week.md)|[11_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/11_week.md)|[12_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/12_week.md)|[13_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/13_week.md)|[14_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/14_week.md)|[15_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/15_week.md)|[16_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/16_week.md)|[17_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/17_week.md)|[18_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/18_week.md)|[19_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/19_week.md)|[20_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/20_week.md)|[21_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/21_week.md)|[22_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/22_week.md)|[23_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/23_week.md)|[24_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/24_week.md)|[25_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/25_week.md)|[26_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/26_week.md)|[27_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/27_week.md)|[28_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/28_week.md)|[29_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/29_week.md)|[30_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/30_week.md)|[31_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/31_week.md)|[32_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/32_week.md)|[33_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/33_week.md)|[34_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/34_week.md)|[35_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/35_week.md)|[36_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/36_week.md)|[37_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/37_week.md)|[38_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/38_week.md)|[39_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/39_week.md)|[40_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/40_week.md)|[41_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/41_week.md)




<!-- | | | | | -->

## Recommended Papers for Week 41, 2024
1. [LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts](https://arxiv.org/abs/2410.08211)
2. [Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training](https://arxiv.org/abs/2410.08202)
3. [HybridBooth: Hybrid Prompt Inversion for Efficient Subject-Driven Generation](https://arxiv.org/abs/2410.08192)
4. [Scaling Laws For Diffusion Transformers](https://arxiv.org/abs/2410.08184)
5. [DART: Denoising Autoregressive Transformer for Scalable Text-to-Image Generation](https://arxiv.org/abs/2410.08159)
6. [Q-VLM: Post-training Quantization for Large Vision-Language Models](https://arxiv.org/abs/2410.08119)
7. [Parameter-Efficient Fine-Tuning in Spectral Domain for Point Cloud Learning](https://arxiv.org/abs/2410.08114)
8. [Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation](https://arxiv.org/abs/2410.08091)
9. [Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs](https://arxiv.org/abs/2410.08020)
10. [AHA: Human-Assisted Out-of-Distribution Generalization and Detection](https://arxiv.org/abs/2410.08000)
11. [Executing Arithmetic: Fine-Tuning Large Language Models as Turing Machines](https://arxiv.org/abs/2410.07896)
12. [Generated Bias: Auditing Internal Bias Dynamics of Text-To-Image Generative Models](https://arxiv.org/abs/2410.07884)
13. [HeGraphAdapter: Tuning Multi-Modal Vision-Language Models with Heterogeneous Graph Adapter](https://arxiv.org/abs/2410.07854)
14. [MinorityPrompt: Text to Minority Image Generation via Prompt Optimization](https://arxiv.org/abs/2410.07838)
15. [Exploring Foundation Models in Remote Sensing Image Change Detection: A Comprehensive Survey](https://arxiv.org/abs/2410.07824)
16. [Temporal-Difference Variational Continual Learning](https://arxiv.org/abs/2410.07812)
17. [Prototype-based Optimal Transport for Out-of-Distribution Detection](https://arxiv.org/abs/2410.07617)
18. [A Unified Debiasing Approach for Vision-Language Models across Modalities and Tasks](https://arxiv.org/abs/2410.07593)
19. [Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training](https://arxiv.org/abs/2410.07336)
20. [Margin-bounded Confidence Scores for Out-of-Distribution Detection](https://arxiv.org/abs/2410.07185)
21. [Towards Interpreting Visual Information Processing in Vision-Language Models](https://arxiv.org/abs/2410.07149)
22. [EvolveDirector: Approaching Advanced Text-to-Image Generation with Large Vision-Language Models](https://arxiv.org/abs/2410.07133)
23. [VHELM: A Holistic Evaluation of Vision Language Models](https://arxiv.org/abs/2410.07112)
24. [Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay](https://arxiv.org/abs/2410.07110)
25. [Clean Evaluations on Contaminated Visual Language Models](https://arxiv.org/abs/2410.07030)
26. [Preference Fine-Tuning for Factuality in Chest X-Ray Interpretation Models Without Human Feedback](https://arxiv.org/abs/2410.07025)
27. [Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization](https://arxiv.org/abs/2410.07018)
28. [From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2410.06795)
29. [CoBa: Convergence Balancer for Multitask Finetuning of Large Language Models](https://arxiv.org/abs/2410.06741)
30. [ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time](https://arxiv.org/abs/2410.06625)
31. [3D Representation Methods: A Survey](https://arxiv.org/abs/2410.06475)
32. [From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning](https://arxiv.org/abs/2410.06456)
33. [Fine-grained Hallucination Detection and Mitigation in Language Model Mathematical Reasoning](https://arxiv.org/abs/2410.06304)
34. [Adaptive Label Smoothing for Out-of-Distribution Detection](https://arxiv.org/abs/2410.06134)
35. [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993)
36. [Training-Free Open-Ended Object Detection and Segmentation via Attention as Prompts](https://arxiv.org/abs/2410.05963)
37. [MedUniSeg: 2D and 3D Medical Image Segmentation via a Prompt-driven Universal Model](https://arxiv.org/abs/2410.05905)
38. [CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2410.05627)
39. [Self-rationalization improves LLM as a fine-grained judge](https://arxiv.org/abs/2410.05495)
40. [Recent Advances of Multimodal Continual Learning: A Comprehensive Survey](https://arxiv.org/abs/2410.05352)
41. [MedImageInsight: An Open-Source Embedding Model for General Domain Medical Imaging](https://arxiv.org/abs/2410.06542)
42. [Fine-Tuning CLIP's Last Visual Projector: A Few-Shot Cornucopia](https://arxiv.org/abs/2410.05270)
43. [TextHawk2: A Large Vision-Language Model Excels in Bilingual OCR and Grounding with 16x Fewer Tokens](https://arxiv.org/abs/2410.05261)
44. [LoTLIP: Improving Language-Image Pre-training for Long Text Understanding](https://arxiv.org/abs/2410.05249)
45. [SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe](https://arxiv.org/abs/2410.05248)
46. [TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models](https://arxiv.org/abs/2410.05239)
47. [SimO Loss: Anchor-Free Contrastive Loss for Fine-Grained Supervised Contrastive Learning](https://arxiv.org/abs/2410.05233)
48. [DiffuseReg: Denoising Diffusion Model for Obtaining Deformation Fields in Unsupervised Deformable Image Registration](https://arxiv.org/abs/2410.05234)
49. [Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality](https://arxiv.org/abs/2410.05210)
50. [Enhancing Equity in Large Language Models for Medical Applications](https://arxiv.org/abs/2410.05180)
51. [VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks](https://arxiv.org/abs/2410.05160)
52. [IGroupSS-Mamba: Interval Group Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2410.05100)
53. [Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data](https://arxiv.org/abs/2410.05078)
54. [SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification](https://arxiv.org/abs/2410.05057)
55. [HE-Drive: Human-Like End-to-End Driving with Vision Language Models](https://arxiv.org/abs/2410.05051)
56. [On Efficient Variants of Segment Anything Model: A Survey](https://arxiv.org/abs/2410.04960)
57. [OmniBooth: Learning Latent Control for Image Synthesis with Multi-modal Instruction](https://arxiv.org/abs/2410.04932)
58. [Intent Classification for Bank Chatbots through LLM Fine-Tuning](https://arxiv.org/abs/2410.04925)
59. [Art2Mus: Bridging Visual Arts and Music through Cross-Modal Generation](https://arxiv.org/abs/2410.04906)
60. [Low-Rank Continual Personalization of Diffusion Models](https://arxiv.org/abs/2410.04891)
61. [A Simple Image Segmentation Framework via In-Context Examples](https://arxiv.org/abs/2410.04842)
62. [Diffusion Models in 3D Vision: A Survey](https://arxiv.org/abs/2410.04738)
63. [TLDR: Token-Level Detective Reward Model for Large Vision Language Models](https://arxiv.org/abs/2410.04734)
64. [Low-Rank Continual Pyramid Vision Transformer: Incrementally Segment Whole-Body Organs in CT with Light-Weighted Adaptation](https://arxiv.org/abs/2410.04689)
65. [ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models](https://arxiv.org/abs/2410.04659)
66. [AdaptDiff: Cross-Modality Domain Adaptation via Weak Conditional Semantic Diffusion for Retinal Vessel Segmentation](https://arxiv.org/abs/2410.04648)
67. [Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models](https://arxiv.org/abs/2410.04634)
68. [VISTA: A Visual and Textual Attention Dataset for Interpreting Multimodal Models](https://arxiv.org/abs/2410.04609)
69. [Look Around and Find Out: OOD Detection with Relative Angles](https://arxiv.org/abs/2410.04525)
70. [Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning](https://arxiv.org/abs/2410.04524)
71. [MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network](https://arxiv.org/abs/2410.04507)
72. [Interpret Your Decision: Logical Reasoning Regularization for Generalization in Visual Classification](https://arxiv.org/abs/2410.04492)
73. [A Mathematical Explanation of UNet](https://arxiv.org/abs/2410.04434)
74. [SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417)
75. [MVP-Bench: Can Large Vision--Language Models Conduct Multi-level Visual Perception Like Humans?](https://arxiv.org/abs/2410.04345)
76. [Leveraging Hierarchical Taxonomies in Prompt-based Continual Learning](https://arxiv.org/abs/2410.04327)
77. [LongGenBench: Long-context Generation Benchmark](https://arxiv.org/abs/2410.04199)
78. [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/abs/2410.04055)
79. [AUCSeg: AUC-oriented Pixel-level Long-tail Semantic Segmentation](https://arxiv.org/abs/2409.20398)
80. [DB-SAM: Delving into High Quality Universal Medical Image Segmentation](https://arxiv.org/abs/2410.04172)
81. [WAVE-UNET: Wavelength based Image Reconstruction method using attention UNET for OCT images](https://arxiv.org/abs/2410.04123)
82. [Multiscale Latent Diffusion Model for Enhanced Feature Extraction from Medical Images](https://arxiv.org/abs/2410.04000)