# ArXiv_weekly_paper
This repository records interesting articles on the arXiv website every week (based on my own opinions only).
If you find some interesting articles in this week's arxiv that I've missed, feel free to email me (z1282429194@163.com) or ask a question and I'll add it later.

## Summary of recommended papers in 2024
<!-- | | | | |
|--------|--------|--------|--------| -->
|[1_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/1_week.md)|[2_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/2_week.md)|[3_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/3_week.md)|[4_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/4_week.md)|[5_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/5_week.md)|[6_week](https://github.com/Fatflower/ArXiv_weekly_paper/blob/main/2024/6_week.md)|

<!-- | | | | | -->

## Recommended Papers for Week 6, 2024
1. [Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting](https://arxiv.org/abs/2402.01440)
2. [From Words to Molecules: A Survey of Large Language Models in Chemistry](https://arxiv.org/abs/2402.01439)
3. [XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision](https://arxiv.org/abs/2402.01410)
4. [Continual Learning for Large Language Models: A Survey](https://arxiv.org/abs/2402.01364)
5. [FindingEmo: An Image Dataset for Emotion Recognition in the Wild](https://arxiv.org/abs/2402.01355)
6. [Describing Images $\textit{Fast and Slow}$: Quantifying and Predicting the Variation in Human Signals during Visuo-Linguistic Processes](https://arxiv.org/abs/2402.01352)
7. [CORE: Mitigating Catastrophic Forgetting in Continual Learning through Cognitive Replay](https://arxiv.org/abs/2402.01348)
8. [Can MLLMs Perform Text-to-Image In-Context Learning?](https://arxiv.org/abs/2402.01293)
9. [On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio Classification](https://arxiv.org/abs/2402.01274)
10. [Can Shape-Infused Joint Embeddings Improve Image-Conditioned 3D Diffusion?](https://arxiv.org/abs/2402.01241)
11. [Segment Any Change](https://arxiv.org/abs/2402.01188)
12. [A Comprehensive Survey on 3D Content Generation](https://arxiv.org/abs/2402.01166)
13. [Efficient Prompt Caching via Embedding Similarity](https://arxiv.org/abs/2402.01173)
14. [A Single Simple Patch is All You Need for AI-generated Image Detection](https://arxiv.org/abs/2402.01123)
15. [A Survey for Foundation Models in Autonomous Driving](https://arxiv.org/abs/2402.01105)
16. [Compositional Generative Modeling: A Single Model is Not All You Need](https://arxiv.org/abs/2402.01103)
17. [Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning](https://arxiv.org/abs/2402.00910)
18. [VISION-MAE: A Foundation Model for Medical Image Segmentation and Classification](https://arxiv.org/abs/2402.01034)
19. [V-IRL: Grounding Virtual Intelligence in Real Life](https://arxiv.org/abs/2402.03310)
20. [HASSOD: Hierarchical Adaptive Self-Supervised Object Detection](https://arxiv.org/abs/2402.03311)
21. [Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?](https://arxiv.org/abs/2402.03305)
22. [Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining](https://arxiv.org/abs/2402.03302)
23. [Nevermind: Instruction Override and Moderation in Large Language Models](https://arxiv.org/abs/2402.03303)
24. [Zero-shot Object-Level OOD Detection with Context-Aware Inpainting](https://arxiv.org/abs/2402.03292)
25. [InstanceDiffusion: Instance-level Control for Image Generation](https://arxiv.org/abs/2402.03290)
26. [Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2402.03286)
27. [Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models](https://arxiv.org/abs/2402.03284)
28. [CLIP Can Understand Depth](https://arxiv.org/abs/2402.03251)
29. [FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition](https://arxiv.org/abs/2402.03241)
30. [The Matrix: A Bayesian learning model for LLMs](https://arxiv.org/abs/2402.03175)
31. [RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein Segmentation and Classification](https://arxiv.org/abs/2402.03166)
32. [Is Mamba Capable of In-Context Learning?](https://arxiv.org/abs/2402.03170)
33. [Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector](https://arxiv.org/abs/2402.03094)
34. [Text-Guided Image Clustering](https://arxiv.org/abs/2402.02996)
35. [Retrieval-Augmented Score Distillation for Text-to-3D Generation](https://arxiv.org/abs/2402.02972)
36. [Kernel PCA for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02949)
37. [Exploring the Synergies of Hybrid CNNs and ViTs Architectures for Computer Vision: A survey](https://arxiv.org/abs/2402.02941)
38. [Understanding the planning of LLM agents: A survey](https://arxiv.org/abs/2402.02716)
39. [Causal Feature Selection for Responsible Machine Learning](https://arxiv.org/abs/2402.02696)
40. [Learning with Mixture of Prototypes for Out-of-Distribution Detection](https://arxiv.org/abs/2402.02653)
41. [VM-UNet: Vision Mamba UNet for Medical Image Segmentation](https://arxiv.org/abs/2402.02491)
42. [Why are hyperbolic neural networks effective? A study on hierarchical representation capability](https://arxiv.org/abs/2402.02478)
43. [Deep Spectral Improvement for Unsupervised Image Instance Segmentation](https://arxiv.org/abs/2402.02474)
44. [BECLR: Batch Enhanced Contrastive Few-Shot Learning](https://arxiv.org/abs/2402.02444)
45. [Revisiting the Power of Prompt for Visual Tuning](https://arxiv.org/abs/2402.02382)
46. [Exploring Intrinsic Properties of Medical Images for Self-Supervised Binary Semantic Segmentation](https://arxiv.org/abs/2402.02367)
47. [The Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364)
48. [Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models](https://arxiv.org/abs/2402.02347)
49. [Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning](https://arxiv.org/abs/2402.02340)
50. [Your Diffusion Model is Secretly a Certifiably Robust Classifier](https://arxiv.org/abs/2402.02316)
51. [Selecting Large Language Model to Fine-tune via Rectified Scaling Law](https://arxiv.org/abs/2402.02314)
52. [Polyp-DAM: Polyp segmentation via depth anything model](https://arxiv.org/abs/2402.02298)
53. [Multi-Level Feature Aggregation and Recursive Alignment Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2402.02286)
54. [SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State Tracking](https://arxiv.org/abs/2402.02285)
55. [Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey](https://arxiv.org/abs/2402.02242)
56. [Revisiting Generative Adversarial Networks for Binary Semantic Segmentation on Imbalanced Datasets](https://arxiv.org/abs/2402.02245)
57. [Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models](https://arxiv.org/abs/2402.02207)
58. [GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events](https://arxiv.org/abs/2402.02205)
59. [Prompting Diverse Ideas: Increasing AI Idea Variance](https://arxiv.org/abs/2402.01727)
60. [Are Large Language Models Good Prompt Optimizers?](https://arxiv.org/abs/2402.02101)
61. [Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification Approach](https://arxiv.org/abs/2402.02066)
62. [MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning](https://arxiv.org/abs/2402.02045)
63. [ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation](https://arxiv.org/abs/2402.02029)
64. [Understanding Time Series Anomaly State Detection through One-Class Classification](https://arxiv.org/abs/2402.02007)
65. [Robust Counterfactual Explanations in Machine Learning: A Survey](https://arxiv.org/abs/2402.01928)
66. [A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm](https://arxiv.org/abs/2402.01684)
67. [Sample, estimate, aggregate: A recipe for causal discovery foundation models](https://arxiv.org/abs/2402.01929)
68. [SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?](https://arxiv.org/abs/2402.01832)
69. [Rethinking Interpretability in the Era of Large Language Models](https://arxiv.org/abs/2402.01761)
70. [VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models](https://arxiv.org/abs/2402.01735)